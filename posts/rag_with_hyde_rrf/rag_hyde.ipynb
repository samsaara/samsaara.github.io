{
 "cells": [
  {
   "cell_type": "raw",
   "id": "aa6e7ac9-7a20-4797-a763-5e9dd852f399",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Mastering RAG Pipelines\"\n",
    "description: How HyDE's simulated document representations combined with query rewrites and RRF enhance retrieval quality in RAG systems\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "jupyter: python3\n",
    "date: \"2025-10-25\"\n",
    "categories: [EDA, pytorch, nlp, genai, llms, rag, hyde, rrf, ranking, information retrieval]\n",
    "image: RAG_Robot.png\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82282269-de42-498b-8f36-0fa8419e8b1d",
   "metadata": {},
   "source": [
    "Ever wished your search engine could read your mind? In this post, we‚Äôll dive into how Retrieval-Augmented Generation (RAG) can come close ‚Äî by expanding user queries into hypothetical documents, refining them through query rewriting, and fusing results intelligently using Reciprocal Rank Fusion (RRF). Together, these techniques transform ordinary retrieval into a contextual, knowledge-aware reasoning process.\n",
    "\n",
    "_PS: This post is inspired by these two articles: [RAG Hack](https://medium.com/data-and-beyond/the-ultimate-rag-hack-use-this-customized-hyde-for-better-answers-28e251332a0b), [LLM Based Query Rewriting](https://blog.gopenai.com/part-5-advanced-rag-techniques-llm-based-query-rewriting-and-hyde-dbcadb2f20d1)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f120d-78e5-4726-982c-fad6a2301f52",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation** (RAG) is an architecture that combines two powerful components: _retrieval_ and _generation_. Instead of relying solely on what a language model ‚Äúremembers,‚Äù RAG retrieves relevant documents from an external knowledge base and uses them to generate more accurate, up-to-date, and context-aware responses.\n",
    "\n",
    "It‚Äôs particularly useful when dealing with dynamic or domain-specific information ‚Äî for example, enterprise knowledge bases, customer support documentation, or academic research. By grounding responses in retrieved facts, RAG reduces hallucinations and enhances factual accuracy, making it an ideal solution for any use case where the underlying data changes frequently or is too large to fit into a model‚Äôs internal memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf2e6e-7026-43b5-b761-057f262c761c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Limitations of traditional RAG based systems\n",
    "\n",
    "While RAG sounds elegant in theory, traditional implementations often struggle in practice. The retrieval step depends heavily on keyword or embedding similarity, which can miss relevant documents when the query is vague, abstract, or phrased differently from the source text. This leads to poor recall ‚Äî the model simply doesn‚Äôt find the right context to reason from.\n",
    "\n",
    "Additionally, many RAG systems fail under semantic drift, long-tail queries, or domain-specific jargon. Even when relevant information exists, it may rank too low or be drowned out by irrelevant results. As a result, the generation step produces confident but inaccurate answers ‚Äî defeating the very purpose of retrieval augmentation.\n",
    "\n",
    "Enter üî•**HyDE**üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6103e7d-775a-482f-90dd-0897707c432c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hypothetical Document Embeddings\n",
    "\n",
    "Hypothetical Document Embeddings (HyDE) is a retrieval technique that addresses a key weakness of traditional RAG systems‚Äîpoor performance on vague or under-specified queries. Instead of retrieving directly from the raw query, HyDE first uses a language model to generate a short, plausible ‚Äúhypothetical document‚Äù answering the query. This synthetic passage is embedded and used to retrieve semantically similar real documents from the knowledge base.\n",
    "\n",
    "By transforming queries into document-like representations, HyDE aligns retrieval with document embedding space, allowing contrastive encoders (such as Contriever) to match these hypotheticals to relevant corpus entries. This approach yields strong zero-shot retrieval, especially for open-ended, exploratory, or low-context queries where users may lack precise keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aee103-4745-4959-812d-fa6944681192",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Query Generation/ReWriting\n",
    "\n",
    "Before applying HyDE, it‚Äôs often useful to rewrite user queries to make them clearer and more retrieval-friendly. In real-world scenarios, users tend to ask compound or referential questions such as _What is XXX and how does it impact sales?_. Instead of treating this as one query, we can split it into multiple focused sub-queries ‚Äî for example: `['What is XXX?', 'How does XXX impact sales?']`.\n",
    "\n",
    "During this rewriting step, an LLM can also resolve demonstrative pronouns (like _this, that, it, or they_) in follow-up questions, replacing them with explicit references from earlier parts of the query. This process ensures that each rewritten query is self-contained, semantically clear, and ready for more effective retrieval downstream.\n",
    "\n",
    "This stage is called **query generation or query rewriting**, and it acts as a smart preprocessing layer before the HyDE step. By running HyDE on each rewritten sub-query, we get richer hypothetical contexts and significantly better retrieval accuracy for complex, multi-part user inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252f742-10c0-426b-80af-19c46fbea914",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reciprocal Rank Fusion\n",
    "\n",
    "Even after applying Query Rewriting and HyDE, retrieval isn‚Äôt always perfect. Each rewritten or hypothetical query might surface slightly different sets of relevant documents ‚Äî some overlap, some don‚Äôt. For example, one version of the query might capture documents with broader context, while another highlights highly specific details. Choosing just one retrieval run risks losing valuable information.\n",
    "\n",
    "In real-world RAG systems, no single retrieval method or query formulation is universally optimal. Dense embeddings, keyword search, and HyDE-generated queries each offer unique perspectives on relevance. What we need is a way to combine their strengths ‚Äî to merge the best results from multiple retrieval strategies into a unified, high-quality ranking.\n",
    "\n",
    "That‚Äôs where **Reciprocal Rank Fusion** (RRF) comes in.\n",
    "\n",
    "It is a simple yet powerful ranking aggregation technique designed to fuse multiple retrieval results into one coherent ranked list. Instead of trusting one retrieval run, RRF looks across all of them, giving higher priority to documents that appear near the top across lists.\n",
    "\n",
    "Mathematicaly, it sums the reciprocals of rank positions, rewarding items consistently ranked highly. This makes RRF especially useful in hybrid or multi-source retrieval systems‚Äîsuch as combining embeddings, keyword searches, or reformulations‚Äîwhere it boosts recall, stability, and robustness for more reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9180c18-7f3c-472f-8721-ee4ea0f63a8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Powerful Trio\n",
    "\n",
    "When used together, Query Rewriting, HyDE, and RRF form a highly effective multi-stage RAG pipeline.\n",
    "\n",
    "1. Query Rewriting ensures that each user query (or sub-query) is explicit and context-independent.\n",
    "2. HyDE then generates a hypothetical answer for each rewritten query, enriching the semantic representation used for retrieval.\n",
    "3. Finally, RRF fuses the retrieval results from all rewritten and HyDE-generated queries into a unified, ranked list of documents.\n",
    "\n",
    "This combination addresses several real-world challenges:\n",
    "\n",
    "- Ambiguity and reference resolution ‚Üí handled by Query Rewriting.\n",
    "- Sparse or underspecified queries ‚Üí improved via HyDE‚Äôs hypothetical document generation.\n",
    "- Retrieval diversity and ranking stability ‚Üí enhanced by RRF‚Äôs fusion mechanism.\n",
    "\n",
    "In essence, RRF acts as the final consensus layer, aggregating the best retrieval signals from each query variant. Together, these steps make RAG systems far more resilient, accurate, and context-aware, especially in enterprise or multi-domain settings where queries vary in form and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9617a70-672f-45aa-acb9-b9cbc3aa013e",
   "metadata": {},
   "source": [
    "## Choosing Right Models\n",
    "\n",
    "Even with a powerful retrieval pipeline built on Query Rewriting, HyDE, and RRF, the system‚Äôs overall performance still depends heavily on the embedding model used for retrieval and the LLM used for generation. These are the foundational layers that determine how well our RAG system understands, retrieves, and communicates information.\n",
    "\n",
    "The _embedding_ model is responsible for mapping both queries and documents into a shared semantic space. If this model isn‚Äôt well-aligned with our domain or language, even the best retrieval strategies will struggle ‚Äî relevant documents might sit far apart in the vector space, leading to poor recall. For example, an English-only embedding model will fail when users query in Spanish or Hindi, or when the document corpus is multilingual. Choosing a multilingual or domain-tuned embedding model ensures that semantically similar ideas are close together, regardless of language or phrasing.\n",
    "\n",
    "On the _generation_ side, the LLM plays an equally critical role. It needs to interpret the retrieved context correctly, synthesize information fluently, and handle domain-specific or multilingual outputs gracefully. If our users interact in multiple languages, our generation model should either be natively multilingual or supported by a translation layer that preserves meaning without distorting the original intent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681fec2-f5a8-47c2-9910-a058eace23b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we're armed with this knowledge, let's dive into code. The repo is available on [github](https://github.com/samsaara/rag_hyde/) {{< fa brands github title=\"GitHub\" >}}. Feel free to clone it and run this notebook. It is designed primarily to run on Linux {{< fa brands linux title=\"Linux\" >}} with NVIDIA GPU or on Mac {{< fa brands apple title=\"Apple\" >}} with Silicon processors with integrated GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1180b166-155b-490f-9a49-b64110cb6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import sys\n",
    "import faiss\n",
    "import yaml\n",
    "import textwrap\n",
    "\n",
    "from time import time\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from src.utils import build_corpus, empty_cache\n",
    "from src.main import load_models, generate_text, make_query_variants, transform_query, aggregate_queries_and_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3746231-33a2-4b40-9ad8-c72b07bfe157",
   "metadata": {},
   "source": [
    "## üìö Building a Corpus: Extracting Content from PDFs\n",
    "\n",
    "To test our RAG pipeline, we first need a knowledge corpus built from one or more PDFs. Extracting content from PDFs isn‚Äôt always straightforward ‚Äî layouts, tables, and images can make parsing tricky. Two great tools for this are PyMuPDF (or PyMuPDF4LLM) and Docling, each suited for different needs.\n",
    "\n",
    "### ‚ö° PyMuPDF / PyMuPDF4LLM\n",
    "\n",
    "PyMuPDF is a fast, lightweight library for PDF text extraction ‚Äî perfect for CPU environments and large batches of text-heavy documents. Its LLM-optimized variant, PyMuPDF4LLM, adds support for exporting structured Markdown, preserving headings, lists, and tables for easier downstream use.\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Fast, efficient, and CPU-friendly\n",
    "- Handles simple-to-moderate layouts well\n",
    "- Markdown export with PyMuPDF4LLM fits LLM pipelines\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Limited accuracy for complex layouts or multi-column designs\n",
    "\n",
    "Best for: Quick, scalable extraction of mostly text-based PDFs.\n",
    "\n",
    "### üß† Docling\n",
    "\n",
    "Docling is a deep-learning‚Äìpowered PDF parser that excels at handling complex document structures ‚Äî tables, figures, multi-column text, etc. It can export to plain text or Markdown, producing highly structured, clean outputs. However, it‚Äôs heavier and slower, typically requiring GPU support for optimal performance.\n",
    "\n",
    "Pros:\n",
    "\n",
    "- High-fidelity extraction of structured layouts\n",
    "- Flexible text/Markdown outputs\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Slower and GPU-intensive\n",
    "\n",
    "Best for: Complex or visually rich PDFs when GPU resources are available.\n",
    "\n",
    "### üìù Why Extract in Markdown?\n",
    "\n",
    "Extracting in Markdown strikes the right balance between structure and simplicity. It retains document hierarchy ‚Äî like headings, lists, and especially tables ‚Äî that plain text loses, helping LLMs and embedding models better understand context and meaning. Markdown also makes document chunking more coherent, improving retrieval and generation quality downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb1c7d-3d71-4911-8b29-a03371605b44",
   "metadata": {},
   "source": [
    "Let's take the [docling technical report](https://arxiv.org/pdf/2408.09869) as our PDF to build our corpus. But remember, we can use more than one too."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7efb493c-19c9-4e90-8275-47a3901a3047",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "When run from CLI, we can use gradio where we can directly drop the folder or choose PDF(s) to process.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f67a6-0601-42a2-92f7-c1da967a5ffb",
   "metadata": {},
   "source": [
    "Since our documents are extracted in Markdown rather than plain text, we need a text splitter that can respect Markdown structure. For this, we use the `MarkdownTextSplitter` from LangChain‚Äôs text splitters module. It allows us to define a *chunk_size* to control segment length, along with a small overlap to maintain context between chunks.\n",
    "\n",
    "It‚Äôs important to note that with Markdown, the chunk_size serves as an approximation ‚Äî the splitter avoids cutting through sections or formatting elements to preserve the logical and structural continuity of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c1bca4-a071-449f-a799-51e8b2cce856",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = MarkdownTextSplitter(chunk_size=3000, chunk_overlap=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a47c53-2671-4907-8515-8af9171bf17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['notebooks/data/docling_technical_report.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs = glob(\"notebooks/data/*.pdf\")\n",
    "pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d833d6-04c2-4985-85fc-24e1331e81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 21:29:31,859 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-24 21:29:31,897 - INFO - Going to convert document batch...\n",
      "2025-10-24 21:29:31,897 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n",
      "2025-10-24 21:29:31,904 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-24 21:29:31,905 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-24 21:29:31,911 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-24 21:29:31,913 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-10-24 21:29:31,976 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-10-24 21:29:33,349 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-10-24 21:29:34,413 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-10-24 21:29:34,770 - INFO - Processing document docling_technical_report.pdf\n",
      "2025-10-24 21:29:41,917 - INFO - Finished converting document docling_technical_report.pdf in 10.06 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'file', 'chunk_id', 'chunk'],\n",
       "    num_rows: 14\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = build_corpus([pdfs[0]], text_splitter, fast_extract=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe7661-e93c-4510-81ee-ced8a5fa163e",
   "metadata": {},
   "source": [
    "We loaded our corpus in HuggingFace's _dataset_ format containing the following columns:\n",
    "\n",
    "- `id` - unique ID across documents\n",
    "- `file` - filename\n",
    "- `chunk_id` - ID of the chunk within a given document\n",
    "- `chunk` - raw text extracted as markdown\n",
    "\n",
    "We need to preserve this metadata which would later help in fetching the appopriate chunks after RRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5073ada2-9ecd-4b83-998e-2ebcc684f4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2746,\n",
       " 775,\n",
       " 269,\n",
       " 2579,\n",
       " 2696,\n",
       " 2436,\n",
       " 2948,\n",
       " 2885,\n",
       " 331,\n",
       " 2993,\n",
       " 1783,\n",
       " 2552,\n",
       " 2648,\n",
       " 2332]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, ds[\"chunk\"][:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cf375b2-7e0d-40cd-8db7-3a02718a8538",
   "metadata": {},
   "source": [
    "As noted earlier, the data is chunked unevenly. We will also empty the cache to offload docling from GPU as we no longer need it by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a30fed-0c3e-4da4-b714-52eceeff06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4d3bc-da17-4f7e-8d5a-34f38f041904",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3725552-3bac-4fd1-9727-7817fb3b7811",
   "metadata": {},
   "source": [
    "### Embedding Model\n",
    "\n",
    "For our pipeline, we need a lightweight yet high-quality embedding model ‚Äî one that can generate strong dense vector representations while being efficient enough for local or production use. If our use case mandates it, ideally, the model should be multilingual, capable of producing similar embeddings for semantically similar text across different languages. This is crucial because users might upload documents in one language and ask questions in another, or even provide multilingual documents within the same corpus.\n",
    "\n",
    "After experimenting with several variants, I chose Qwen3-Embedding, which strikes a great balance between performance, efficiency, and multilingual capability. For local deployment, I‚Äôve settled on the [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) model ‚Äî with a maximum embedding dimension of 1024. It performs exceptionally well for most workloads, though you can always switch to a larger variant (e.g., via the Hugging Face Spaces) if you need more capacity or precision.\n",
    "\n",
    "### Text Generation Model\n",
    "\n",
    "For the text generation step, I opted for [Qwen3-4B-AWQ](https://huggingface.co/Qwen/Qwen3-4B-AWQ) due to its strong reasoning abilities, instruction-following skills, agent capabilities, and multilingual support. It also offers a generous context window of 32,768 tokens, which is ideal for handling long documents or multi-section queries. `AWQ` (Attention Aware Quantization) preserves a small fraction of the weights that are important for LLM performance to compress a model to 4-bits with minimal performance degradation. Therefore we can be able to load huge models on limited GPU RAM that are otherwise impossible.\n",
    "\n",
    "Smaller variants proved less effective: the 0.6B model was too weak to generate meaningful answers, and the 1.7B model was only partially reliable. The 4B version, on the other hand, though not perfect, consistently produced accurate results across most queries.\n",
    "\n",
    "A key factor contributing to this performance was the Markdown-format PDF extraction, especially when done via Docling, which preserved document structure and context, allowing the model to reason over the content more effectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc549526-2966-41e3-825c-72c1160973ff",
   "metadata": {},
   "source": [
    "We'll define a few model combinations to use depending on host platform's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14fddae-6b0c-4808-b0d1-63d251bbdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_COMBOS = {\n",
    "    \"linux\": {\n",
    "        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"gen_model\": \"Qwen/Qwen3-4B-AWQ\",\n",
    "    },\n",
    "    # feel free to replace with any ??B-MLX-?bit versions from Qwen3 Collection at:\n",
    "    # https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f\n",
    "    \"mac\": {\n",
    "        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"gen_model\": \"Qwen/Qwen3-4B-MLX-4bit\",\n",
    "    },\n",
    "    \"mac_mid\": {\n",
    "        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"gen_model\": \"Qwen/Qwen3-4B-MLX-6bit\",\n",
    "    },\n",
    "    \"mac_high\": {\n",
    "        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"gen_model\": \"Qwen/Qwen3-4B-MLX-8bit\",\n",
    "    },\n",
    "    # models to load on HF Spaces \n",
    "    # HF-low is same as `linux-local`\n",
    "    \"HF-mid\": {\n",
    "        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"gen_model\": \"Qwen/Qwen3-8B-AWQ\",\n",
    "    },\n",
    "    \"HF-high\": {\n",
    "        \"embed_model\": \"Qwen/Qwen3-Embedding-4B\",\n",
    "        \"gen_model\": \"Qwen/Qwen3-14B-AWQ\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec31464-a861-4ad6-adf8-952786ffa332",
   "metadata": {},
   "source": [
    "Now, we load the embedding & generative models based on one of the combination keys above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d79dd8-8ab7-4d14-8292-c54ac838f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = \"linux\" if sys.platform == \"linux\" else \"mac\"\n",
    "GEN_MODEL_NAME = MODEL_COMBOS[combo][\"gen_model\"]\n",
    "embedder, tok, gen = load_models(\n",
    "    embed_model_name=MODEL_COMBOS[combo][\"embed_model\"],\n",
    "    gen_model_name=MODEL_COMBOS[combo][\"gen_model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aed95b-7137-4394-a33b-d53742f54277",
   "metadata": {},
   "source": [
    "## Embeddings for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf33ff8-1a76-459e-9099-63c0db8ac929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(texts, **kwargs):\n",
    "    return embedder.encode(texts, normalize_embeddings=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f91e9-1ea5-461e-8be9-5b3dbc68a9c1",
   "metadata": {},
   "source": [
    "We can now add embeddings directly as a column to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45049b05-bd84-4272-a471-d6d5751b71d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08075af975b946b59c1f990e48053a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0793a4a5d8e545bf9ded03aea2efa217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2da72f13724384b3559ecc983b572e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 8  # change it based on your system's capabilities\n",
    "ds = ds.map(\n",
    "    lambda x: {\n",
    "        \"embeddings\": get_emb(\n",
    "            x[\"chunk\"],\n",
    "            batch_size=batch_size,\n",
    "            prompt_name=\"query\",\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1fc1d5e-70b4-4085-b016-79e976a82412",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9b988f-c9e3-445b-8be3-df71980a8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.save_to_disk('temp_dataset');\n",
    "# ds = load_from_disk('temp_dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550bc17-70ae-4de6-b2c7-2b7771e6897e",
   "metadata": {},
   "source": [
    "## Build Index\n",
    "\n",
    "Now that we have the input data setup, it's time to index them. We shall use FAISS as it's efficient & fast (esp. if you have a GPU). Since we're using datasets, adding a faiss index over a column is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922e1469-cde5-449c-9cc7-c75c01be7f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996d322646074b5c9aacded8edfcc039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.add_faiss_index(\n",
    "    \"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT\n",
    ");  # , device=0 if (torch.cuda.is_available() or torch.mps.is_available()) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce6f00-e5de-43cb-b5da-9061e3111f11",
   "metadata": {},
   "source": [
    "## Generate queries\n",
    "\n",
    "Now that we have the data and the models setup, time to process user queries. Let's define a function that handles that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da5df2a-4c02-446e-a15f-14acffed10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"tell me about cloud bursts\",\n",
    "    \"how to ride a unicorn\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ee252a-d334-4008-8f01-9f6c6dc2d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = generate_text(\n",
    "    tok, gen, queries, model_name=GEN_MODEL_NAME,\n",
    "    system_prompt=\"You are a funny standup comedian and reply only in one liner jokes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d154cc85-2a21-48ce-bf38-936ca8c38009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A cloud burst is like a thunderstorm that‚Äôs so intense, it could make a cloud cry.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "891be82f-3c42-417a-8fe9-2bc5566b1122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Just follow the rainbow and pretend you're not scared of the hooves.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc3d56-12d3-4f04-8e31-9c88a325cc42",
   "metadata": {},
   "source": [
    "Great! Notice that we can give multiple queries in one go which comes handy during batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffabb2e-d523-4b4f-a0e5-bbfe172179ad",
   "metadata": {},
   "source": [
    "# Retrieve results from corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c413e40-5f0d-4362-aed0-2c1b3fb23c86",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "Because our model understands system and user prompts, we can set the context/role depending on what we want. Let's load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff799e1a-d84f-4776-8556-b2b4389c2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/prompts.yaml\") as fl:\n",
    "    prompts = yaml.safe_load(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e3e87-a118-4dc3-bd55-1abcfbf9985a",
   "metadata": {},
   "source": [
    "First we define a function to make different variants of the user query. We provide a few examples in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cb5ed-f2e4-4d9c-b119-1b1d420d6013",
   "metadata": {},
   "source": [
    "## Query Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b25d68-ed7d-497d-8697-f83562db35f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evolution of solar energy costs in europe',\n",
       " 'how has the cost of solar energy changed in different european countries?',\n",
       " 'how have solar energy prices changed in europe over the years?',\n",
       " 'what factors have influenced the evolution of solar energy costs in europe?',\n",
       " 'what has been the trend in solar energy costs across europe?',\n",
       " 'what is the historical development of solar energy costs in europe?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"evolution of solar energy costs in Europe\"\n",
    "q_variants = make_query_variants(\n",
    "    tok, gen, query, prompts[\"variants\"], n=2, model_name=GEN_MODEL_NAME\n",
    ")\n",
    "q_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63384a60-f9e2-40ca-9999-02905f1d94cd",
   "metadata": {},
   "source": [
    "Nice! LLMs work best by role playing & examples and therefore we have given some through system prompt. Look at the _variants_ section of the `prompts.yaml` file to learn more.\n",
    "\n",
    "we always preserve the original query just in case the model loses (or deviates too far from) the original intent in its generated variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edea89d-4368-4366-8678-0fb8f5054a44",
   "metadata": {},
   "source": [
    "## Query Generation/Rewrite\n",
    "\n",
    "We now transform the query into a json with two parts `search` & `tasks`. _Search_ contains one or more queries based on the original query that are transformed & expanded (if applicable) to be made within the context provided. _Tasks_ lists one or more actions to take _after_ the search is performed. Sometimes there'd be overlap in between them but that'll be taken care during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f012bac7-3f50-48da-a11e-61abcb2ebc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search': ['summary of sales in the last quarter',\n",
       "  'sales summary for the previous quarter'],\n",
       " 'tasks': ['draft an email based on the sales summary',\n",
       "  'send the drafted email to the relevant recipient']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"draft an email on summary of sales in the last quarter\"\n",
    "transform_query(tok, gen, query, prompts[\"rewrite\"], model_name=GEN_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd496288-7577-4aa2-8790-b4466ba9de7c",
   "metadata": {},
   "source": [
    "Now just like we derived many variations of the original user query earlier, we shall do the same for each of the above search queries as well. We append the tasks at the last of the user prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8a78d-fa96-4594-a119-f6b834e421da",
   "metadata": {},
   "source": [
    "## final aggregation of queries & tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "585b32a3-d54e-4324-af62-ca70dc04fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, tasks = aggregate_queries_and_tasks(\n",
    "    tok, gen, query, prompts[\"rewrite\"], prompts[\"variants\"], \n",
    "    gen_model_name=GEN_MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7fb8d4e-69bb-4716-9e09-c3b8626dac18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['draft an email on summary of sales in the last quarter',\n",
       " 'how did sales performance compare to the previous quarter',\n",
       " 'what is the sales overview for the quarter that just ended',\n",
       " 'your name']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da9c5936-6dad-49ee-b193-f366273898ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['draft an email', 'send the email to the relevant team']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad7a3444-f43a-489e-9347-3bdd45d0cf46",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "Because we used a multilingual language model, this should also work for other languages\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "445aadb1-f2f6-4baa-88e1-74eeb2aaf435",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was ist die Geschichte der Autos in Deutschland?\"\n",
    "queries, tasks = aggregate_queries_and_tasks(\n",
    "    tok, gen, query, prompts[\"rewrite\"], prompts[\"variants\"], \n",
    "    temperature=0.7, n_variations=2, gen_model_name=GEN_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e185dd6-7012-439b-8f99-a5bf5061bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was ist die geschichte der autos in deutschland?',\n",
       " 'wie hat sich die autoindustrie in deutschland im laufe der zeit entwickelt',\n",
       " 'wie ver√§ndert sich die produktion von autos in deutschland']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a3cb5fe-15f5-4364-9fed-03dc81fa4f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f3103-9574-46ec-886d-fdaa776e98e2",
   "metadata": {},
   "source": [
    "## HyDE generation\n",
    "\n",
    "Let's now pick a query that has something to do with our corpus to create hypothetical documents and its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be75fb90-cee6-423b-b3cd-382bc57ae88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how good is table recognition in docling?',\n",
       " 'how does docling perform in identifying tables from various document types',\n",
       " 'how effective is table recognition in document processing',\n",
       " 'what is the accuracy of table recognition in document analysis']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How good is table recognition in docling?\"\n",
    "queries, tasks = aggregate_queries_and_tasks(\n",
    "    tok, gen, query, prompts[\"rewrite\"], prompts[\"variants\"], \n",
    "    temperature=0.7, n_variations=3, gen_model_name=GEN_MODEL_NAME\n",
    ")\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90702027-c922-40ce-9001-7d0d6f84618c",
   "metadata": {},
   "source": [
    "Instead of passing each query, we send all in one go as a batch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fbe70f1-6f48-41d2-861f-78747eda34f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 287 ms, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%time hyde_docs = generate_text(\n",
    "    tok, gen, queries, prompts['hyde'], temperature=.7, model_name=GEN_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e774df41-9ef5-4103-8fe1-a1747c0f46f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyde_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f9bd7-3e6c-4d81-95e3-3e333330207e",
   "metadata": {},
   "source": [
    "Let's see some samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be53f454-efa0-43d8-89c9-8fe2eef7ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Docling is a document processing tool that includes features for recognizing\n",
      "tables within documents. According to its documentation and user reviews,\n",
      "Docling's table recognition capability is generally considered adequate for most\n",
      "use cases, including extracting table data into structured formats. The tool\n",
      "uses machine learning models to identify and parse tables, which works well for\n",
      "standard formats such as CSV, TSV, and HTML. However, users have reported that\n",
      "its performance can vary depending on the complexity and formatting of the\n",
      "original document. While it is not as advanced as some specialized tools in\n",
      "handling highly irregular or complex table structures, Docling is suitable for\n",
      "general-purpose table extraction tasks. Its effectiveness is often compared\n",
      "favorably to other document processing tools in terms of accuracy and ease of\n",
      "use.\n",
      "==============================\n",
      "Docling is a document processing tool designed to extract and structure\n",
      "information from various document types, including tables. According to a 2023\n",
      "evaluation by DocumentAI, Docling demonstrated strong performance in identifying\n",
      "and extracting tables from common document formats such as PDFs, Word documents,\n",
      "and Excel files. The tool successfully recognized tabular data in both\n",
      "structured and unstructured formats, with an accuracy rate of over 92% in\n",
      "controlled testing environments. However, its performance varied when processing\n",
      "highly formatted or scanned documents, where table recognition was less\n",
      "consistent. Users have reported that Docling's table detection is reliable for\n",
      "standard business documents but may require additional preprocessing for complex\n",
      "or non-standard layouts. The tool is part of a broader suite of AI-driven\n",
      "document processing solutions aimed at improving data extraction efficiency.\n"
     ]
    }
   ],
   "source": [
    "for doc in hyde_docs[:2]:\n",
    "    print(f\"{'='*30}\\n{textwrap.fill(doc, width=80)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd422e64-b5e3-4f61-bcea-cda05d752d4b",
   "metadata": {},
   "source": [
    "We now split it into chunks and get its embeddings; same as what we did for our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1fa3780-1174-47ec-86f3-dee57f2f872b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1024)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "for hyde_doc in hyde_docs:\n",
    "    chunks.extend(text_splitter.split_text(hyde_doc))\n",
    "q_emb = get_emb(chunks, prompt_name=\"query\", show_progress_bar=False)\n",
    "q_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcf43b-712d-4bb7-9c2c-acd4a71e94e6",
   "metadata": {},
   "source": [
    "## Search in index\n",
    "\n",
    "We can define how many matches to retrieve for a given query through `k_per_variant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfba4b86-221b-479c-8b13-6b413ffcab51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7055741 , 0.6299919 , 0.62796915, 0.5997107 , 0.5608388 ],\n",
       "       [0.7040975 , 0.6866737 , 0.6717943 , 0.6013819 , 0.59106   ],\n",
       "       [0.53217894, 0.5187626 , 0.43610024, 0.42912632, 0.41111925],\n",
       "       [0.52601624, 0.4494145 , 0.446052  , 0.436696  , 0.3895734 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_per_variant = 5\n",
    "matches = ds.get_nearest_examples_batch(\"embeddings\", q_emb, k_per_variant)\n",
    "np.array(matches.total_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35f9bfd2-8086-4851-b076-d514d95f061d",
   "metadata": {},
   "source": [
    "Great! We got all the top `k_per_variant` matching chunks for each of the hyde in one go. The above array shows their corresponding similarity scores with each of the query embedding.\n",
    "We shall also now retrieve the unique ids of the corresponding chunks to rank them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52d2abf1-f16f-498f-9a24-b8438a38e3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 4, 0, 3, 1], [7, 4, 0, 3, 9], [4, 9, 0, 13, 7], [4, 9, 13, 12, 10]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [x[\"id\"] for x in matches.total_examples]\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bc6ef-6af8-4728-bdd9-8a08b9f0b7b4",
   "metadata": {},
   "source": [
    "We notice that ids `0, 4, 7` consistently came out on the top. Let's see their rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035a5ba-9eb2-4d61-93eb-c522751ac493",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d1f77-2fc2-4cd7-9e58-c3ff64296c13",
   "metadata": {},
   "source": [
    "### Reciprocal Rank Fusion\n",
    "\n",
    "We define the RRF function to rank and retrieve most relevant matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e096256c-9b25-4432-b43c-4b2787a39f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(indices, top_k=3, denom=50):\n",
    "    scores = defaultdict(int)\n",
    "    for row in indices:\n",
    "        for rank, idx in enumerate(row):\n",
    "            scores[idx] += 1 / (rank + denom)\n",
    "    results = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [idx for idx, _ in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1110da1-9622-479d-adc2-c7bc6c182d0c",
   "metadata": {},
   "source": [
    "Let's now get the `top_k` chunk ids from all of the above that best match our original `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79adb3d3-c3bd-41eb-854c-0255bcd63cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 2\n",
    "top_idx = reciprocal_rank_fusion(indices, top_k=top_k)\n",
    "top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d60de573-5468-462a-882c-983e30a292b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 3.2 AI models\n",
       "\n",
       "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
       "\n",
       "## Layout Analysis Model\n",
       "\n",
       "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
       "\n",
       "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
       "\n",
       "## Table Structure Recognition\n",
       "\n",
       "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
       "\n",
       "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
       "\n",
       "## OCR"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ds[top_idx[0]]['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019bec14-b309-48d6-af98-b0c852b3b861",
   "metadata": {},
   "source": [
    "_Indeed the top most document actually contains the answer to our query. We now wrap it all up in one function!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fdb5025-22c2-4f7e-ab0a-0dfd6ca7f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def retrieve(\n",
    "    tok, gen, query, n_variants=3, top_k_per_variant=5, top_k_retrieve=2, \n",
    "    model_name=\"\", **llm_kwargs,\n",
    "):\n",
    "    queries, tasks = aggregate_queries_and_tasks(\n",
    "        tok, gen, query.strip(), prompts[\"rewrite\"], prompts[\"variants\"], \n",
    "        n_variants, model_name, **llm_kwargs,\n",
    "    )\n",
    "    hyde_docs = generate_text(\n",
    "        tok, gen, queries, prompts[\"hyde\"], model_name, **llm_kwargs)\n",
    "    chunks = []\n",
    "    for hyde_doc in hyde_docs:\n",
    "        chunks.extend(text_splitter.split_text(hyde_doc))\n",
    "    q_emb = get_emb(chunks)\n",
    "    matches = ds.get_nearest_examples_batch(\"embeddings\", q_emb, top_k_per_variant)\n",
    "    indices = [match[\"id\"] for match in matches.total_examples]\n",
    "    top_idx = reciprocal_rank_fusion(indices, top_k_retrieve)\n",
    "    return top_idx, tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6bfd2b3-72a0-47d1-83d1-0f49010d28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550a46781c7a4fd3bde41297db7c2ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 s, sys: 643 ms, total: 19.7 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%time i,t = retrieve(tok, gen, query, model_name=GEN_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f026427-f5f6-48b1-abd6-6bd81b012f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2781da-554d-4117-ac35-29ddfdd6f56c",
   "metadata": {},
   "source": [
    "# Final Aggregated Response\n",
    "\n",
    "Having all the relevant text pieces to generate a context, we now couple that with the original user query and feed it to our LLM to get a final response. We shall tell what we expect and how, through the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "548f6029-a39a-4234-bda2-f5e0b165b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "def get_filtered_entries(idxs):\n",
    "    # We need to drop the index before filtering/selecting the desired indices and re-add it later\n",
    "    # Since it's FAISS and we index very little data, it's not noticeable\n",
    "    ds.drop_index(\"embeddings\")\n",
    "    entries = ds.select(idxs)\n",
    "    ds.add_faiss_index(\"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "    return entries\n",
    "\n",
    "def answer(tok, gen, query, idxs, tasks, model_name, max_ctx_chars=32768):\n",
    "    total, text, prompt_length = 0, \"\", 10000\n",
    "    sep = \"\\n\\n-----\\n\\n\"\n",
    "    tasks = \", \".join(tasks) if tasks else \"\"\n",
    "    entries = get_filtered_entries(idxs)\n",
    "    for content in entries[\"chunk\"]:\n",
    "        ctx = f\"{sep}\\n\\n{content}\"\n",
    "        if total + len(ctx) + len(tasks) + len(sep) + prompt_length > max_ctx_chars:\n",
    "            print(\"context overflow\")\n",
    "            break\n",
    "        text += ctx\n",
    "        total = len(text)\n",
    "\n",
    "    # add tasks if any, at the last\n",
    "    text += f\"{sep}{tasks}\"\n",
    "\n",
    "    instruction = \"go ahead and answer!\"\n",
    "    user_query = f\"\\nq: {query}\\n\\nctx:{text}\" + f\"\\n\\n{instruction}\\n\\n\"\n",
    "    resp = generate_text(\n",
    "        tok, gen, user_query, prompts[\"final_answer\"], model_name=model_name\n",
    "    )\n",
    "\n",
    "    sources = \"\"\n",
    "    for idx, entry in enumerate(entries):\n",
    "        source = f'<h2 style=\"color: cyan;\">Source {idx + 1} :: {entry[\"file\"]}:{entry[\"chunk_id\"]}</h2>'\n",
    "        sources += f\"{sep}{source}\\n\\n{entry['chunk']}\"\n",
    "\n",
    "    return resp, sources.replace(\"```\", \"`\")\n",
    "\n",
    "# little wrapper function to retrieve and display results\n",
    "def ask(query):\n",
    "    start = time()\n",
    "    top_idxs, tasks = retrieve(tok, gen, query.strip(), model_name=GEN_MODEL_NAME)\n",
    "    retrieve_end = time()\n",
    "    print(f\"Retrieval {(retrieve_end - start):.1f} seconds\")\n",
    "    # we can use `top_idxs` to retrieve the source content if we wish to.\n",
    "    answer_start = time()\n",
    "    resp, sources = answer(tok, gen, query, top_idxs, tasks, GEN_MODEL_NAME)\n",
    "    end = time()\n",
    "    print(f\"answering took {(end - answer_start):.1f} seconds\")\n",
    "    return f\"\\n(_Whole search took {(end - start):.1f} seconds_)\\n\\n# Final Answer:\\n{resp}{sources}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd84623-5716-4c84-9861-7c957c3b0f12",
   "metadata": {},
   "source": [
    "Let's see what the model answers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9be5b7d-cf2b-498d-927c-ae34d93d2b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How good is table recognition in docling?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27f7be88-39c3-42ca-b03b-98fc5f17f0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bb759b44414778953d3b00aa91cad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval 18.8 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a38298a70cb4c628ed9bdceaef49cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answering took 5.3 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "(_Whole search took 24.2 seconds_)\n",
       "\n",
       "# Final Answer:\n",
       "The table recognition in Docling is described as \"state-of-the-art\" with the TableFormer model being a \"state-of-the-the-table structure recognition model\". It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. It handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy, and other complexities. Typical tables require between 2 and 6 seconds to be processed on a standard CPU.\n",
       "\n",
       "-----\n",
       "\n",
       "<h2 style=\"color: cyan;\">Source 1 :: docling_technical_report:4</h2>\n",
       "\n",
       "## 3.2 AI models\n",
       "\n",
       "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
       "\n",
       "## Layout Analysis Model\n",
       "\n",
       "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
       "\n",
       "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
       "\n",
       "## Table Structure Recognition\n",
       "\n",
       "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
       "\n",
       "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
       "\n",
       "## OCR\n",
       "\n",
       "-----\n",
       "\n",
       "<h2 style=\"color: cyan;\">Source 2 :: docling_technical_report:7</h2>\n",
       "\n",
       "## 5 Applications\n",
       "\n",
       "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
       "\n",
       "## 6 Future work and contributions\n",
       "\n",
       "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
       "\n",
       "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
       "\n",
       "## References\n",
       "\n",
       "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
       "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = ask(query)\n",
    "Markdown(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bc149a-296b-4da6-bbad-0a0fc4f6a15b",
   "metadata": {},
   "source": [
    "Let's try another one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b3f1abf-7372-45dc-8b91-972da2c5b721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dd68b60820446a93cd56b67c4160ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval 20.2 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdab723145649a786dddc2cb01cb22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answering took 3.4 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "(_Whole search took 23.6 seconds_)\n",
       "\n",
       "# Final Answer:\n",
       "Docling bietet optionale OCR-Unterst√ºtzung, insbesondere f√ºr scanned PDFs oder content in bitmaps images. In der initialen Version verl√§sst sich Docling auf EasyOCR, eine beliebte dritte Partei OCR-Bibliothek mit Unterst√ºtzung f√ºr viele Sprachen.\n",
       "\n",
       "-----\n",
       "\n",
       "<h2 style=\"color: cyan;\">Source 1 :: docling_technical_report:7</h2>\n",
       "\n",
       "## 5 Applications\n",
       "\n",
       "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
       "\n",
       "## 6 Future work and contributions\n",
       "\n",
       "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
       "\n",
       "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
       "\n",
       "## References\n",
       "\n",
       "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
       "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
       "\n",
       "-----\n",
       "\n",
       "<h2 style=\"color: cyan;\">Source 2 :: docling_technical_report:5</h2>\n",
       "\n",
       "## OCR\n",
       "\n",
       "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
       "\n",
       "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
       "\n",
       "## 3.3 Assembly\n",
       "\n",
       "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
       "\n",
       "## 3.4 Extensibility\n",
       "\n",
       "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
       "\n",
       "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
       "\n",
       "## 4 Performance"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Bietet Docling OCR-Unterst√ºtzung??\"\n",
    "resp = ask(query)\n",
    "Markdown(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eac3dc-184e-4add-932e-8470076b42bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8456de-4920-461c-a454-45e5ad3fa48a",
   "metadata": {},
   "source": [
    "_Sweet! The final answer greatly captures the context and provides factual, grounded answer as seen from the cited documents along with replying in the same language as the user's query._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343bd4f-1069-4bfc-9422-85549e296077",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We now have a complete working solution! You can tweak several parameters to trade off speed or accuracy. Ideally, this pipeline should be able to extract pieces of text even when they are deeply embedded inside a table with a complex layout, or mentioned rarely in the entire corpus or sometimes indirectly even. \n",
    "\n",
    "This entire code is available in scripts to be able to run via CLI or via gradio (local as well as on HF Spaces).\n",
    "\n",
    "Here's how it looks with gradio for example: ![gradio](gradio_UI.png)\n",
    "\n",
    "Notice that the information is correctly extracted from a table and a gist is provided.\n",
    "\n",
    "Checkout the project [README](https://github.com/samsaara/rag_hyde/blob/main/README.md) for more insights/observations and let me know if there're any suggestions or corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c81af8-928c-4ac1-8f07-dbb6affc3d8c",
   "metadata": {},
   "source": [
    "## Tips/Observations:\n",
    "\n",
    "  - Extracting text as a markdown greatly preserved the structure and continuity of the text. This resulted in better logical chunking which in turn led to better embeddings and as a consequence, better search results.\n",
    "  - Reading the document via `docling` extracted more and correct text compared to `pymupdf4llm` but at a bit of an expense of speed. It is enabled by default for prioritising accuracy. This proved esp. useful in extracting data containing lots of tables spread over multiple pages.\n",
    "  - You can pass `--fast-extract` from CLI or tick a box via gradio UI to use pymupdf instead.\n",
    "  - Increasing the model size (coupled with correct text extraction in markdown) greatly improved performance. The Qwen3 models very much adhered to instructions but the smaller variants instead of hallucinating simply fell back to saying _'I don't know'_ (as per instructions). The `4B` variant understood the user intent which sometimes was vague and yet managed to give relevant results. The base variant is huge and it wouldn't have been fit and run fast enough on a consumer grade laptop GPU. Loading the `AWQ` variant of it helped as it occupied substantially less memory compared to the original without much loss in performance.\n",
    "  - This model also showed great multilingual capabilities. User can upload document in one language and ask questions in another. Or they could upload multilingual documents and ask multilingual queries. For the demo, I tested mostly in English & German.\n",
    "  - The data is now stored in datasets format that allows for better storage & scaling (arrow) along with indexing (FAISS) for querying.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations / Known Issues\n",
    "\n",
    "- Even though `docling` with mostly default options proved to be better than `pymupdf4llm` to extract text, it's not perfect everytime. There're instances where _pymupdf_ extracted text from an embedded image inside a PDF better than docling. However, docling is highly configurable and allows for deep customization via 'pipelines'. And it also comes with a very permissive license for commercial use compared to PyMuPDF.\n",
    "  - docling comes with `easyocr` by default for text OCR. It's not powerful enough compared to _tesseract_ or similar models. But since installing the latter and linking it with docling involves touching system config, it's not pursued.\n",
    "\n",
    "- When user uploads multiple PDFs, we can improve load times by reading them asynchronously. Attempts to do that with `docling` sometimes resulted in pages with ordering different than the original. So it's dropped for the demo. More investigation is needed later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568db9b1-ae42-42a6-af7b-30a3c6f6065c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "Even though this retrieval pipeline is fairly modern, it's not bleeding edge. Since techniques like HyDE and query rewriting first appeared, several new ideas have emerged to make retrieval even smarter and faster. Some systems now combine both dense and keyword-based (sparse) search, so the strengths of each can cover the other‚Äôs weaknesses. Others use LLMs to rewrite or expand queries more efficiently before retrieval, or train learned retrievers that better understand context without needing extra generation steps. There‚Äôs also growing interest in reranking and hybrid pipelines that automatically learn how to mix results from different retrieval methods. Exploring these newer approaches could help improve both accuracy and speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
