{
 "cells": [
  {
   "cell_type": "raw",
   "id": "94d723e6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Curate jobs with Agentic AI & Ollama\"\n",
    "description: Get a consolidated report of your favorite jobs \n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "jupyter: python3\n",
    "date: \"2025-09-03\"\n",
    "categories: [Agents, NLP, LLMs, Web Scraping, ollama, crewai]\n",
    "image: images/llama_crew.jpg\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29836912-417c-42c3-9f19-0adbe7868350",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Job searching is hard. Anybody looking for one knows how draining it can be to sift through endless listings, hoping to find something that fits or feels right. <i>Ain't nobody got time for dat...</i>\n",
    "\n",
    "<!-- <img src=\"images/no_time.png\" alt=\"icon\" style=\"height: 1em; vertical-align: middle;\"/>  -->\n",
    "\n",
    "Instead, we use agents ü§ñ to keep track of openings at companies we care about and in departments we want to work in. The challenge is, not every company offers a üîî / üì© / {{< fa rss title=\"rss\" >}}  to notify you when new positions open up. And even when they do, how often do you actually check those newsletters? Thought so! üòâ\n",
    "\n",
    "With ‚ú®***agentic job search***‚ú®, you get a streamlined report of all the current openings at companies you‚Äôre interested in, tailored to the departments you want to work in. This is made possible by web scraping combined with agents‚Äîspecialized programs that can run tasks independently and make decisions without human intervention. For this project, we rely on [CrewAI](https://crewai.com/), one of the leading agentic frameworks out there. It brings together multiple specialized AI agents into a collaborative team, letting them work together smoothly to tackle complex tasks. Think of it as an AI squad with defined roles and workflows, functioning much like a well-oiled {{<fa users title='human'>}} team."
   ]
  },
  {
   "cell_type": "raw",
   "id": "47d2a16f-f190-4dff-8ee5-18d1fdbed8b8",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "The inspiration for this project came from one of the use cases demoed in [crewAI's courses](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) on deeplearning.ai. I highly recommend checking them out. Their approach to job searching is a bit different and more generic but equally powerful.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6bfa3-437a-4ffb-8c94-b3a5daba0251",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving in, you'll need:\n",
    "\n",
    "- **Technical Skills**: Basic Python knowledge, familiarity with HTML/CSS, command-line operations\n",
    "- **System Requirements**: Python 3.10+, 4GB+ RAM (16GB+ recommended for local LLMs along with a supporting GPU)\n",
    "- **API Access**: Account with any cloud LLM provider (optional if using Ollama)\n",
    "- **Time Investment**: 30-45 minutes for initial setup, a few minutes per run thereafter\n",
    "\n",
    "This guide targets intermediate developers comfortable with web scraping concepts and API integrations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e044fd-2f98-42ac-ae4b-9fd15fc71948",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Before we look at what ü§ñ do, we need to look at the good'ol web scraping. The idea is simple. You create a list of your favorite organizations and a department/sector that you'd like to filter for. Then you scrape the {{<fa file-code title=\"webpage\">}} for the latest openings and store it on disk. _(We use [Playwright](https://playwright.dev/) {{<fa brands python title=\"python\">}} package for this)_ After that, the ü§ñ steps in to sift through these roles and handpick the ones that match your interests.\n",
    "\n",
    "There are a few good reasons why we can‚Äôt just tell an agent to hop onto a website and grab the latest job postings directly:\n",
    "\n",
    "- The large language model (LLM) the agent relies on might not be powerful enough to fully get the instructions or might miss parts of the content‚Äîafter all, not everyone‚Äôs got access to OpenAI‚Äôs top-tier models!\n",
    "- Dynamic content on job listing pages can trip up the model. Sometimes the section you want isn‚Äôt even loaded yet when the agent tries to fetch it, especially since many organizations use third-party platforms like _Ashby or Workable_ that embed listings via JavaScript.\n",
    "- Some websites are massive, with an overwhelming number of openings across multiple locations, making it tough to scrape everything efficiently.\n",
    "- Models can hallucinate‚Äîmeaning the job listings they generate might look legit but actually be inaccurate, outdated, or just plain wrong based on their training data or knowledge cut-off.\n",
    "\n",
    "That‚Äôs why the scraping step here is crucial. To make it manageable, we create a YAML file where we define specific attributes that help us target and fetch the right content later on.\n",
    "\n",
    "Here's a sample:\n",
    "\n",
    "```yaml\n",
    "#orgs.yaml sample file\n",
    "\n",
    "browser_company:\n",
    "  url: https://jobs.ashbyhq.com/The%20Browser%20Company\n",
    "  selector: \"div._departments_12ylk_345\"\n",
    "\n",
    "ecosia: \n",
    "  url: https://jobs.ashbyhq.com/ecosia.org\n",
    "  selector: \"#root > div._container_ud4nd_29._section_12ylk_341 > div._content_ud4nd_71 > div._departments_12ylk_345\"\n",
    "\n",
    "mozilla:\n",
    "  url: https://www.mozilla.org/en-US/careers/listings/\n",
    "  selector: '#listings-positions tbody tr.position[data-team=\"Strategy, Operations, Data & Ads\"]'\n",
    "\n",
    "```\n",
    "\n",
    "The key here is the organization name you‚Äôre interested in, and the values hold the URL plus a `selector`. These are CSS patterns that identify specific HTML elements on a webpage, so you can zero in on the exact part you want to scrape.\n",
    "\n",
    "Finding the right selector is usually straightforward but _sometimes can be a bit tricky_. If the webpage uses dynamic content, wait until the job listings or tables have fully loaded, then right-click on the listing area and choose **Inspect**. That opens the _developer tools_ where you can explore the DOM tree. Move up or down through parent and child elements until you spot the container holding the data. As you click on different elements, they‚Äôll highlight corresponding webpage parts, helping you confirm you got the right one. Finally, right click on the element (or on the _..._ dots at the start of the line), and choose `copy -> copy selector` as shown below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee7aa958-c9e7-45f4-bed2-60845901a97e",
   "metadata": {},
   "source": [
    "::: {#fig-}\n",
    "![](images/selectors.jpg)\n",
    "\n",
    "Sample table view of openings at Mozilla\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42354cd-0864-4eed-b268-478085785023",
   "metadata": {},
   "source": [
    "Using the selector approach has several key advantages:\n",
    "\n",
    "- It enables scraping of targeted and precise content‚Äîyou download just the data you need, or at most a manageable superset that can be filtered later by an agent or LLM.\n",
    "- It‚Äôs reliable because website layouts don‚Äôt change that often. When they do, you simply update the selector in the YAML file, and you‚Äôre good to go (well, almost!). Plus, the program logs failures if scraping doesn‚Äôt work, so you can jump in and fix it manually.\n",
    "- It allows for a highly effective, yet optional, use of a powerful model‚Äîsince we feed the LLM only the stripped-down HTML content, even mid-sized models can deliver great results.\n",
    "\n",
    "This method ensures the ü§ñ always work with the freshest and cleanest data, making filtering faster, simpler, and more accurate. They're happy, you're happy! Now, let‚Äôs see how it all comes together.\n",
    "\n",
    "The complete code and installation instructions are [available on <u>{{<fa brands github title=\"Github\">}}</u>](https://github.com/samsaara/agentic_job_search). Head over there to setup the repo, then come back, and we‚Äôll dive into the details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea369cc2",
   "metadata": {},
   "source": [
    "## Run with agents\n",
    "\n",
    "Hopefully, you're now set up. You only needed to do three things:\n",
    "\n",
    "- Make sure the _creds.yaml_ file is filled with the credentials for your favorite LLM.\n",
    "    - While it currently supports three providers out of the box, adding another cloud provider‚Äîlike _Anthropic_‚Äîis as simple as adding the corresponding entries to the file and passing the provider name as an argument to _main.py_.\n",
    "    - Different providers make different env variables available apart from common ones like `API_KEY`. Simply add them under the corresponding provider in the YAML file.\n",
    "- Populate the _orgs.yaml_ file with your favorite organizations.\n",
    "- Set the `JOB_TOPIC` value in <u>_src/config.py_</u> with a topic or department you want to filter roles for. Alternatively, you can pass this as a parameter directly to the _main.py_ script.\n",
    "\n",
    "With these in place, you‚Äôre ready to roll!! ü•Å"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d7e8c4a-1eb6-4af9-b643-14625c991cb2",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "If you want to do LLM inferencing locally or from a remote endpoint, checkout the ü¶ô[ollama](#ollama)ü¶ô section below.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417110eb",
   "metadata": {},
   "source": [
    "To run your setup, simply do:\n",
    "```python\n",
    "crewai run\n",
    "```\n",
    "\n",
    "If you run into any errors, the first place to check is the _logs.log_ file‚Äîit‚Äôs where all the action gets recorded. And if you bump into a **ModuleNotFoundError** or your command won‚Äôt execute properly, try running this command instead:\n",
    "```python\n",
    "PYHONPATH='.' uv run run_crew\n",
    "```\n",
    "\n",
    "The completed jobs report is available under <u>_src/scrape/jobs_</u> directory. Here's a sample: \n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"org\": \"mozilla\",\n",
    "    \"url\": \"https://www.mozilla.org/en-US/careers/listings/\",\n",
    "    \"jobs\": [\n",
    "      {\n",
    "        \"title\": \"Staff Data Scientist - Operations\",\n",
    "        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/7141659/\",\n",
    "        \"location\": \"Remote UK\",\n",
    "        \"workplaceType\": null\n",
    "      },\n",
    "      {\n",
    "        \"title\": \"Manager, Data Science\",\n",
    "        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/6989335/\",\n",
    "        \"location\": \"Remote Canada, Remote US\",\n",
    "        \"workplaceType\": null\n",
    "      },\n",
    "      {\n",
    "        \"title\": \"Senior Data Scientist\",\n",
    "        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/7125910/\",\n",
    "        \"location\": \"Remote Canada, Remote US\",\n",
    "        \"workplaceType\": null\n",
    "      },\n",
    "      {\n",
    "        \"title\": \"Senior Data Scientist\",\n",
    "        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/7137777/\",\n",
    "        \"location\": \"Remote US\",\n",
    "        \"workplaceType\": null\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"org\": \"ecosia\",\n",
    "    \"url\": \"https://jobs.ashbyhq.com/ecosia.org\",\n",
    "    \"jobs\": []\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "920f2073-c1dd-4680-8ec1-56fb05838ec1",
   "metadata": {},
   "source": [
    "Sweet!!! Now, all that‚Äôs left is to browse through the filtered roles at your favorite companies, click on the listings you‚Äôre genuinely interested in, and maybe even apply‚Äîall while comfortably sipping your ‚òïÔ∏è with one hand. üòé\n",
    "\n",
    "That said, this setup works best if you have access to a decent model from a reliable cloud provider‚Äîbut those don‚Äôt come cheap. CrewAI agents make multiple calls to the LLM for each organization (including tool calls), so if you‚Äôre not on a generous plan, the tokens consumed and requests made can quickly add up, especially as you add more companies to your YAML file.\n",
    "\n",
    "It gets trickier if the {{<fa brands python title=\"python\">}} scraper can‚Äôt locate your specified selector and ends up scraping the entire page as a fallback, or if the listings come from a large company and you forgot to include adequate filters in your selector. Feeding huge chunks of HTML content‚Äîbeyond just the system and assistant prompts‚Äîüî• through tokens fast. And even worse, if the model makes a mistake, you often have to feed its response back into the system, which further increases both calls and token usage.\n",
    "\n",
    "For those without a paid subscription, there are alternatives like [openrouter](https://openrouter.ai/) and [AIML](https://aimlapi.com/), though the free tiers are quite restrictive. These options work well for manual testing but aren't suited for the high volume of calls that CrewAI requires. If you need to handle larger workloads, you might want to explore the [Programmatic approach](#programmatic-job-search) section optionally combined with [ollama](#Ollama) for better efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a2c12-aad4-4900-b01f-8e4ed7bd101f",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104b77e-3417-405d-8f97-9b1e6e5a27e2",
   "metadata": {},
   "source": [
    "If you have a (beefy) system at home or access to one, you can then run LLMs locally with [ollama](https://ollama.com/). Their extensive [library](https://ollama.com/library) offers a range of models that can comfortably run on your hardware for inference tasks. We won't cover its installation and pre-requisites but assume that it's already setup on your system, which you can check by running `which ollama`.\n",
    "\n",
    "With my üíª config, I was able to run [gpt-oss:20b](https://ollama.com/library/gpt-oss) after closing all other applications, but the model response time remained quite low. It might be fine for having a (patient) conversation but isn‚Äôt fast enough to run our agents effectively. That said, as mentioned earlier, we don‚Äôt necessarily need a highly advanced (reasoning) model for this use case since the tasks aren‚Äôt that complex.\n",
    "\n",
    "What we really need is a model that runs comfortably on our systems and is preferably trained on code. So, I settled on [Qwen2.5-coder](https://ollama.com/library/qwen2.5-coder), which showed strong performance on various coding tasks and programming languages. Of course, feel free to pick whichever model suits your needs best!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01312ed3-8fa6-4c52-b260-fa9365d850b8",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "With ollama, you don't need an API key to run it locally. So you can skip the `OLLAMA_API_KEY` in the _creds.yaml_ file but make sure you fill up the rest.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb44030-d005-4a1d-9ca5-12782cb94bd4",
   "metadata": {},
   "source": [
    "When running an Ollama model locally, you can use it to power your agents instead of relying on expensive cloud providers. Just change the provider name to `OLLAMA` in the _main.py_ file suggested earlier (assuming you've set up its credentials in _creds.yaml_)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7444f-f5f4-4bc8-be51-46c02d8333ac",
   "metadata": {},
   "source": [
    "## Programmatic Job Search\n",
    "\n",
    "Alternatively, you can bypass the agentic workflow altogether and opt for a programmatic approach that sequentially processes each organization to fetch job listings. This method supports using your preferred cloud provider or a local ollama model. Simply configure the appropriate **provider** along with any optional parameters. To execute the programmatic fetch, run:\n",
    "\n",
    "```bash\n",
    "uv run run_manual\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a16593b5-2e64-4702-80d0-6ee50995862c",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "Unfortunately, I could only test my setup with free cloud providers and with ollama. If in case you face errors, please raise an issue on {{< fa brands github title=\"Github\" >}} so that it might be resolved with community's help.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048f3b8-fa24-4408-85f4-d2e8f7fde299",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "On my rig, scraping only took a couple of seconds per organization and about 30s for inferencing and preparing the job report with _qwen2.5-coder_ & ollama per organization. The tokens per second roughly hovered between 6-10 and the response time is usually under 5 seconds. The input prompt tokens roughly are in a couple of thousands (less than 5K) because we send a blob of HTML but the response tokens relatively are far less - roughly around 500. For orgs with no listings, it's negligible (_<10_). ***YMMV***"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0c1357e-ce60-4fe8-b554-9dc8ab1e586a",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "If doing local LLM inferencing, you can check your metrics in the logs whenever the model makes an inference.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bdacb-4bcb-40a4-8285-6d0fc25b97eb",
   "metadata": {},
   "source": [
    "# Next Steps or Customization\n",
    "\n",
    "Thanks for sticking with me this far! If you only need a bare-bones working version, you might want to stop here. But of course, there‚Äôs plenty of room to ‚öóÔ∏èüß™ and expand. We‚Äôve used just one ü§ñ and one task ‚úÖÔ∏è, but feel free to add more.\n",
    "\n",
    "For example, you could do additional post-processing on the final report‚Äîconvert it to markdown, filter roles further by location or workplace type, or even make another LLM call to summarize the entire report ü§Ø. You could also set up an alert system, like a {{<fa brands slack title=\"Slack\" >}} hook, to get your daily job report delivered automatically. ‚ú®Ô∏è\n",
    "\n",
    "If you want to go really deep, you could filter the returned role ‚õìÔ∏è‚Äçüí•, scrape those pages for content, and then use a powerful LLM to customize cover letters for each application. Or, keep it visual üìä to analyze the scraped data.\n",
    "\n",
    "Go ü•úü•úü•ú!!! <u><i>But remember to use this work responsibly and for personal use only.</i></u>\n",
    "\n",
    "Please feel free to ‚≠ê the [repo](https://github.com/samsaara/agentic_job_search) on {{< fa brands github title=\"Github\" >}}, fork it to tweak further and/or provide feedback.\n",
    "\n",
    "Auf Wiedersehen! üëã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
