[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting my humble abode on the Internet. I am Vivek, a freelance Data Scientist by profession.\nSa·πÉsƒÅra means World in Sanskrit and I welcome you into mine where I blog about programming, among other things.\nI have over 6 years of experience in Python & Machine Learning ecosystems across a broad variety of sectors ranging from Fashion üõçÔ∏è to Mobility üõ¥ to Fintech üí±.\nAt Sezzle, a platform offering BNPL service, I worked on developing and deploying early machine learning based fraud detection models. Prior to that at FreeNow, a mobility solutions provider, I worked on GMV forecasting, churn prediction etc.\nApart from  & , I am also interested in üßë‚Äçüåæ, üöú, üé∂, üßë‚Äçüç≥ and aim to know a little bit of everything I come across.\nPlease feel free to reach me for any business, consulting or workshop opportunities. üòÉ"
  },
  {
    "objectID": "about.html#namaste",
    "href": "about.html#namaste",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting my humble abode on the Internet. I am Vivek, a freelance Data Scientist by profession.\nSa·πÉsƒÅra means World in Sanskrit and I welcome you into mine where I blog about programming, among other things.\nI have over 6 years of experience in Python & Machine Learning ecosystems across a broad variety of sectors ranging from Fashion üõçÔ∏è to Mobility üõ¥ to Fintech üí±.\nAt Sezzle, a platform offering BNPL service, I worked on developing and deploying early machine learning based fraud detection models. Prior to that at FreeNow, a mobility solutions provider, I worked on GMV forecasting, churn prediction etc.\nApart from  & , I am also interested in üßë‚Äçüåæ, üöú, üé∂, üßë‚Äçüç≥ and aim to know a little bit of everything I come across.\nPlease feel free to reach me for any business, consulting or workshop opportunities. üòÉ"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "",
    "text": "Welcome back! In this post, we will dive into Fastai‚Äôs mid-level API and learn how they help us build custom pipelines & dataloaders with the help of a simple computer vision (CV) example.\nThe dataset we‚Äôll use is the hello world equivalent of CV called MNIST. Now, there‚Äôre various ways to get this data and in fact, fastai provides it as a direct download from its URLs.MNIST attribute but I recently took part in a kaggle competition1 that provided data in an unusual way‚Ä¶ one that‚Äôs not common enough to load via standard fastai methods. So I thought of making this post to show how that can be done! Let‚Äôs go!\n# command to print multiple outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#transforms",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#transforms",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Transforms",
    "text": "Transforms\nThe image data is in the form of an array and the labels as a column in a dataframe. I couldn‚Äôt find a way to load that using the standard DataBlock API as shown in my face mask detection post right out of the way we have it (i.e., data not being stored on disk). Perhaps one can convert each row from a 784 array to a 28x28 matrix, store it as an image in a column and then load it using one of the ImageDataLoaders methods as shown above. But that sounds a bit more hacky to me compared to the elegant ways Transforms provides.\nTransform is a class that one can inherit from and it has 3 main methods to implement.\n\nencodes takes an item and transforms our data into the way we want (our custom transformation)\nsetups is an optional method that sets the inner state, if there‚Äôs any\ndecodes which is an optional step too that acts as (near) opposite to encodes. It tries to undo the operations performed in encodes, if and when possible.\n\nHere‚Äôs a simple example from one of the lessons in fastai:\n\nclass NormalizeMean(Transform):\n    def setups(self, items): self.mean = sum(items)/len(items)\n    def encodes(self, x): return x-self.mean\n    def decodes(self, x): return x+self.mean\n\nHere, setups stores the inner state of the passed items i.e., the mean, encodes returns the data with the mean subtracted and decodes returns the data with the mean added (opposite of encodes).\n\n# initialize\nnm = NormalizeMean()\nnm\n\nNormalizeMean:\nencodes: (object,object) -&gt; encodes\ndecodes: (object,object) -&gt; decodes\n\n\n\nnm.setup([1, 2, 3, 4, 5])\n\n\nnm.mean\n\n3.0\n\n\n\nnm(2)\n\n-1.0\n\n\n\nnm.decode(2)\n\n5.0\n\n\nUsing this, let‚Äôs see how an image can be obtained from an array of pixels! The idea is to pass one row of our training data and get an image & a label in return‚Ä¶\n\ntrain.head()\n\n\n\n\n\n\n\n\nlabel\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows √ó 785 columns\n\n\n\n\nxx = train.loc[0][1:].values.reshape(-1, 28).astype(np.uint8)\nxx.shape\nshow_image(xx);\n\n\n\n\n\n\n\n\nLet‚Äôs write the same as a transform that can run on all the datapoints.\n\nclass Convert_to_image(Transform):\n    def encodes(self, x):\n        mat = x[1:].reshape(-1, 28).astype(np.uint8)\n        return PILImage.create(mat)\n\nIn our case, we don‚Äôt need to maintain any inner state, hence the setups method was skipped. Also there‚Äôs no need to revert to the original array state (although one can) and therefore the decodes method too was skipped.\nLet‚Äôs test this by taking a sample row from our training dataset.\n\nrow = train.values[10]\nrow.shape\n\n(785,)\n\n\n\nc2i = Convert_to_image()\nc2i(row)\n\n\n\n\n\n\n\n\nYay! Now we have an image. Let‚Äôs also extract the label out of the same data, which is the first value in the array‚Ä¶\n\nclass Extract_label(Transform):\n    def encodes(self, x):\n        return x[0]\n\n\nel = Extract_label()\nel(row)\n\n8\n\n\nSweet!"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#pipelines",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#pipelines",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Pipelines",
    "text": "Pipelines\nNow in order to construct our dataloaders, we still need to run a few more transformations on our independent & dependent variables such as converting the data to a tensor to take advantage of GPU etc. Pipeline helps us build a list of transformations to run on our data sequentially. Let‚Äôs compose two pipelines: one that acts on our dependent data (i.e., images) and another on our independent data (i.e., labels).\n\nx_pipe = Pipeline([Convert_to_image, ToTensor])\ny_pipe = Pipeline([Extract_label, Categorize(vocab=train.label.unique())])\n\nFor our dependent data, first Convert_to_image was run, which takes a row from the dataframe, extract our pixel array, reshapes, converts that to a matrix and then to an image. Later it was converted to a tensor with a ToTensor built-in transformation.\nFor our independent data, the label was first extracted as defined in the Extract_label transform above and later converted to a Category that we want to predict using Categorize built-in transformation. The total possible labels that can be predicted was passed to the vocab (stands for vocabulary) parameter.\nNow let‚Äôs run the pipeline to see what we get!\n\nshow_image(x_pipe(row));\n\n\n\n\n\n\n\n\n\ny_pipe(row)\n\nTensorCategory(8)\n\n\nNice! We‚Äôre now ready to construct our dataloaders."
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#custom-datasets-dataloaders",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#custom-datasets-dataloaders",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Custom Datasets & DataLoaders",
    "text": "Custom Datasets & DataLoaders\nTo construct dataloaders, a Datasets object needs to be created which takes raw data and can apply multiple pipelines in parallel. This wil be used to run our independent & dependent data piplines together. Optionally the parameter splits can be specified using one of the Splitter transforms, in this case a RandomSplitter which returns training & validation indices extracted from our raw dataset.\nAs we see, most of the functions that‚Äôre used regularly in fastai are actually transformations itself. :)\n\nsplits = RandomSplitter(valid_pct=.2)(np.arange(train.shape[0]))\n\n\ndsets = Datasets(train.values, [x_pipe, y_pipe], splits=splits)\n\nwith the Datasets object now obtained, we construct DataLoaders object by simply calling .dataloaders on it. Since we are not collecting data from disk, we don‚Äôt have to specify the path and can optionally set a batch_size with bs. As these are just 28x28 images, we can set a bigger batch size.\n\ndls = dsets.dataloaders(bs=512)\n\n\nlen(dls.train.items), len(dls.valid.items)\n\n(33600, 8400)\n\n\n\ndls.train.show_batch()\n\n\n\n\n\n\n\n\n\ndls.valid.show_batch()\n\n\n\n\n\n\n\n\nFinally! We have come a long way extracting the raw data from a dataframe to be able to construct a dataloaders object with image as our dependent datablock and category as our indenpendent datablock!"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#train",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#train",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Train",
    "text": "Train\nYou can now use this data to train using fastai‚Äôs vision_learner method!\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nlearner = vision_learner(dls, resnet18, metrics=accuracy).to_fp16()\n\n\nlearner.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0020892962347716093)\n\n\n\n\n\n\n\n\n\n\nlearner.fine_tune(5, 5e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.755852\n0.339413\n0.915119\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.119606\n0.081445\n0.981310\n00:07\n\n\n1\n0.092199\n0.080963\n0.983571\n00:07\n\n\n2\n0.054804\n0.051717\n0.986905\n00:07\n\n\n3\n0.029896\n0.037651\n0.991667\n00:07\n\n\n4\n0.015221\n0.032806\n0.992500\n00:07\n\n\n\n\n\na near 100% accuracy on the validation set üéâ\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\n\n\n\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat‚Äôs it for today! Fastai‚Äôs mid-level API offers much more functionality and we barely scratched the surface. Hope it inspires you to learn further and take advantage of this powerful feature. Head to the docs to learn more. Thanks for reading :)"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#footnotes",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#footnotes",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(AstroDave 2012)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html",
    "href": "posts/geospatial_intro_part_II/GIS_II.html",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "",
    "text": "Welcome back! In our last post, we took a look at the basics of GIS and how we can explore some of the datasets in QGIS with shapefiles, TIFF files to derive interesting insights around air quality. This post covers loading data from CSV files on disk and also directly from online using server connections."
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html#filtering-a-text-file",
    "href": "posts/geospatial_intro_part_II/GIS_II.html#filtering-a-text-file",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "filtering a text file",
    "text": "filtering a text file\nComing back to our satellite names, we can filter only for the data coming from a single satellite by right clicking on the hms_fire20230306 layer, selecting filter and later entering the query like so:\n\n\n\n\n\n\nFigure¬†7: Filtering a CSV/TXT file\n\n\n\n\n1 - selects the field we want to filter\n2 - gets a few sample values from that field\n3 - double click on the field to enter it into the query box below. Alternatively you can type it as well.\n4, 5 - double click/type\n6 - clicking Test gives us how many rows that satisfy the condition. Hit OK\n7 - ignore (don‚Äôt click it)\n8 - click OK to finalize the filtered data\n\nIf you now look at the later, we only see the datapoints from the satellite we filtered.\n\n\n\n\n\n\nFigure¬†8: Map with filtered entries"
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html#conclusion",
    "href": "posts/geospatial_intro_part_II/GIS_II.html#conclusion",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "conclusion",
    "text": "conclusion\nNow we have seen how to load text files and filtering for our desired data. You can also load Well Known Text (WKT) and other text files in the same manner. Just make sure to select the appropriate radio button while loading them."
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html#conclusion-1",
    "href": "posts/geospatial_intro_part_II/GIS_II.html#conclusion-1",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "Conclusion",
    "text": "Conclusion\nJust like WFS, you can also add WMS/WCS services much the same way to carry out analysis without downloading data onto your disk."
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "title": "Setting up your blog with Fastpages",
    "section": "",
    "text": "After going through a bit of an intensive setup, I thought of writing up my journey & difficulties I faced while setting up this blog so that it could help others who might start later."
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "title": "Setting up your blog with Fastpages",
    "section": "Motivation",
    "text": "Motivation\nI was looking for ways to have my tiny little place on the internet & therefore scouted for feasible options. As a Data Scientist, I interact with code regularly and (Jupyter) Notebooks are part of our DNA :stuck_out_tongue:. So the criteria that I had set for myself was that whatever platform I choose, it has to be (a) very code friendly, (b) easy to maintain, and (c) relatively affordable (if possible, even free).\n\nQuest for the right tool\nI‚Äôm a big fan of Notion and use it for almost everything in my private life. So I initially considered setting up my blog with Super as you can create websites straight away from Notion pages. Though it looks great, the pricing is something that I‚Äôm still not comfortable with (yet).\nThen there‚Äôs Medium, which provides a nice writing experience but only as long as you don‚Äôt have much code in your content. Though affordable, just like Super, it has mediocre support for code formatting natively & you have to optimize your content well ahead with a lot of GitHub gists. It also has no out-of-the-box support for mathematical symbols/equations. Read more about its shortcomings from a developer perspective in a great post from Prasanth Rao here.Though, I might still consider using this service to post once in a while to increase my outreach. We‚Äôll see how it goes. \n\nThese first two options are great if you don‚Äôt write code-heavy posts (that often) and are also very beginner friendly. But unfortunately, both of them are not free and also don‚Äôt fit well for my use case. Besides, where‚Äôs the fun if you don‚Äôt build stuff by tweaking stuff? :wink:\n\nI then decided to give GitHub Pages a try since it‚Äôs a free static site generator service from GitHub. One need not know (much) about HTML/CSS and can simply write content in human readable Markdown format which gets rendered as a webpage. Besides, you get a nice revision history of your blog as everything would be version controlled with GitHub. In combination with Jekyll (that powers Github pages), there‚Äôre numerous themes & templates to choose from and so much customization that can be made via styles, CSS, etc. I can easily convert Jupyter notebooks into markdown scripts and have them rendered with Jekyll. Since one can display code snippets, markdown cells, and embed 3rd party content within Jupyter notebooks, I intended to go with this as it fulfilled most of my needs‚Ä¶ until I rediscovered Fastpages.\nFastpages, from fast.ai, turbocharges Jupyter notebooks by automating the process of creating blog posts from notebooks via Github Actions. We can write content in notebooks markdown files, or even Word documents and have them rendered as a web page. It offers so much more functionality on-top like code folding, interactive plots on a static web page via embeddings, comments & emoji support :heart_eyes_cat:, etc. For an excellent introduction, please refer to the official documentation.\nThat has convinced me & so now you‚Äôre reading this site built with Fastpages. üéâ"
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "title": "Setting up your blog with Fastpages",
    "section": "Setup",
    "text": "Setup\nFortunately, Fastpages is very well documented and it is highly recommended that you go through that first. However, you might still encounter some problems because of some outdated documentation, and if in case you want to test it locally on Linux systems, which is what I cover here. So, without further ado, let‚Äôs dive in.\n\nThink of a name for your blog. It can just be blog like mine.\nGo through the setup instructions detailed here.\n\nIt might happen that once after you try to generate a copy as per the instructions above, a PR won‚Äôt be automatically generated. The debug instructions in the README are a bit outdated. In this case, go to the Settings -&gt; Actions -&gt; General section of your newly created blog repository and ensure that you have Read and write permissions enabled and the last checkbox is ‚úîÔ∏è like :point_down:. Hit Save. \nGo to the Actions tab and you might see a failed section. Ignore what it is for now and click that failed one. Most likely it‚Äôd be a Setup workflow failure. On the top right, from the drop-down menu Re-run jobs, select Re-run failed jobs.\nOnce the above steps are all ‚úÖ, go to the Pull Requests tab and wait for some time. Your first PR would soon be automatically created. You can also optionally check the progress under the Actions tab if desired.\nNow follow the instructions in the PR and merge it.\n\nCongratulations :confetti_ball:. Your blog would soon be up & running at {your-username}.github.io/{repo-name}. Now you can make changes directly on GitHub online or create notebook/markdown/Word files locally and simply upload them as per the instructions into the right folder. Your blog would be updated just like that (in a few minutes). ü™Ñ\n\n\nTest Locally\nIn most of the cases, you might want to check how your post looks like before you publish to make sure it looks as you intend it to be, especially when it contains data from different sources or in different formats. This is when having the option of testing it locally comes in handy. With Fastpages, you can run your blog locally to see how it would look like so that you can fix any nitty gritty details before pushing it online.\nFastpages again provides a good documentation on how to test it locally with the help of Docker . It has worked fine on my Mac  but installing/upgrading Docker on Linux  is still nowhere as smooth as on Mac and hence I had to go through a bit of digging into forums to make it work on my Ubuntu  machine especially on the latest version 22.04 LTS. So, going forward I‚Äôd cover only this scenario.\n\nFor Mac/Windows, all you need is Docker installed and simply run make server from the root level of the repo.\n\n\nDocker Desktop for Linux (DD4L) is still in beta and is only available for 21.XX versions of Ubuntu. So if you have that, go ahead with the setup below. If not, skip to the next step.\n\nFollow the Docker installation instructions from the official documentation.\nIf you had installed Docker via Snap or you had a previous preview version of Docker Desktop installed, it‚Äôs recommended to uninstall them completely. See more here for help on how to do that.\n\nSince Ubuntu 22.04 LTS is not yet supported, I ended up installing  (not Docker Desktop) from here.\nCheck if you have docker-compose installed by doing which docker-compose or docker-compose -v. If not, install it as a standalone binary.\n\nI‚Äôm not sure if it also installs Make but if it doesn‚Äôt, please install it too following the instructions here.\n\nRun make server from the top level of your repository.\n\nYour Jupyter server would be available shortly at http://localhost:8080/tree and it would take a bit of time for your blog to be available under http://localhost:4000/{your-repo-name}. Be patient. :relieved:\n\n\n\nif make server doesn‚Äôt work because of permission issues, try sudo make server instead\n\n\nCongratulations once again! :tada: You now have a local version of your blog running. You can create new posts and have them rendered almost instantly :sparkles:. Once you‚Äôre happy with the content & format, you can push it (preferably in a new branch and making a PR so that your main/master branch is unaffected). If you feel a bit adventurous, try customizing your blog more to your liking by changing fonts/styles, etc.\nIf you like this blog‚Äôs customization, checkout its  repo esp.¬†its custom-style.scss. May be I‚Äôll write another post detailing it.\nGood Luck & Happy Blogging! ‚ù§Ô∏è"
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html",
    "href": "posts/face-mask-detection/facemask_detection.html",
    "title": "Face mask detection with fastai",
    "section": "",
    "text": "With COVID-19 mutating and still posing a threat globally, wearing a üò∑ is still mandatory in many countries. In this post, we will see how to train a simple computer vision model to detect whether the person is wearing a facemask or not. Let‚Äôs start by downloading the appropriate dataset1 from kaggle.\nYou can either download it manually from kaggle or use its free API. Let‚Äôs do the latter.\nfrom fastai.imports import *\nLet‚Äôs create a directory called data to store and extract our desired dataset.\ndata_dir = Path('./data')\ndata_dir.mkdir(parents=False, exist_ok=True)\npath = Path('.')\npath.ls()\n\n(#3) [Path('mask.jpg'),Path('facemask_detection.ipynb'),Path('data')]"
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html#verifying-results",
    "href": "posts/face-mask-detection/facemask_detection.html#verifying-results",
    "title": "Face mask detection with fastai",
    "section": "verifying results",
    "text": "verifying results\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\n\n\n\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs see what the model is getting wrong or is most unsure about by plotting its top losses!\n\ninterp.plot_top_losses(4, figsize=(8, 8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe only got one incorrect image & the rest are correct but a bit inconfident (and even that‚Äôs low for the 2nd row of images).\n\nImage Shifting\nLet‚Äôs do a bit of fun! Sometimes, it‚Äôs known that some image recognition models predict completely different classes when even a few pixels are changed‚Ä¶ hence let‚Äôs see how robust our model is by rotating and wrapping an image and then letting our model predict.\n\ndef rotate_and_wrap_image(image, percentage=.4):\n    im = tensor(image)\n    val = int(im.shape[1] * percentage)\n    return torch.cat((im[:, val:], im[:, :val]), dim=1)    \n\n\nim.resize((128, 128))\n\n\n\n\n\n\n\n\n\nshow_image(rotate_and_wrap_image(im));\n\n\n\n\n\n\n\n\n\nlearner.predict(rotate_and_wrap_image(im))\n\n\n\n\n\n\n\n\n('WithoutMask', TensorBase(1), TensorBase([3.2003e-07, 1.0000e+00]))\n\n\ninteresting! our model still predicts correct class and this worked on many other tested images as well. This might mean that the model has actually learnt to identify a üò∑ and not ‚Äòremember‚Äô the image itself."
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html#footnotes",
    "href": "posts/face-mask-detection/facemask_detection.html#footnotes",
    "title": "Face mask detection with fastai",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.kaggle.com/datasets/ashishjangra27/face-mask-12k-images-dataset‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nSimple & Fast Local DL Setup with PyTorch, Pixi & Nvidia\n\n\n\nsetup\n\npixi\n\npytorch\n\nnvidia\n\n\n\nGet pytorch running locally with GPU support\n\n\n\n\n\nAug 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GIS with QGIS & Python - Part II\n\n\n\nEDA\n\nGIS\n\nQGIS\n\n\n\nhow to load data from CSV/Text files, do filtering and connect to data servers\n\n\n\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GIS with QGIS & Python - Part I\n\n\n\nEDA\n\nGIS\n\nQGIS\n\n\n\nExploring the fundamentals of geospatial data analysis\n\n\n\n\n\nMar 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDiving into Fastai‚Äôs mid-level API with MNIST\n\n\n\nEDA\n\ncomputer vision\n\nfastai\n\n\n\nhow to build custom pipelines & dataloaders\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFace mask detection with fastai\n\n\n\nEDA\n\ncomputer vision\n\nfastai\n\n\n\nmake your model COVID-19 aware‚Ä¶\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nfastai env setup\n\n\n\nsetup\n\nfastai\n\n\n\nget fastai running locally with GPU support\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating from Fastpages to Quarto\n\n\n\nmeta\n\n\n\nblogging setup‚Ä¶ revisited‚Ä¶\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge your data processing with DuckDB\n\n\n\nEDA\n\npandas\n\nSQL\n\n\n\nEfficient & blazing fast SQL analytics in Pandas with DuckDB\n\n\n\n\n\nMay 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up your blog with Fastpages\n\n\n\nmeta\n\n\n\nStart your blogging journey in a few easy steps.\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "title": "Supercharge your data processing with DuckDB",
    "section": "",
    "text": "Do you have large datasets that you simply can not load into memory to analyse with Pandas? Or do you feel more comfortable expressing operations in SQL instead of python?\nFret not, for you have DuckDB now! ‚ú®ü¶Ü‚ú®"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Setup",
    "text": "Setup\nDuckDB is very lightweight and has no external dependencies and runs within the host process itself. Simply install it with:\npip install duckdb==0.3.4\nTo initialize it, run:\n\nimport duckdb\ndbcon = duckdb.connect()\n\nThat‚Äôs it! Now you can test it by running:\n\ndbcon.execute('select 1, 2, 3').fetchall()\n\n[(1, 2, 3)]\n\n\nNext step is to run pip install pyarrow to add support for reading/writing parquet data.\n\nJupyter Notebook setup\nIf in case you wish to explore it in Jupyter Notebooks, install a few additional libraries for a better experience:\npip install ipython-sql SQLAlchemy duckdb-engine\nImport them once installed:\n\nimport pandas as pd\nimport sqlalchemy\n\n%load_ext sql\n\nSet a few config options to prettify the output and return it as Pandas DataFrame\n\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\nDuckDB is primarily designed to be an in-memory DB. You can however persist your data to disk.\n\n%sql duckdb:///:memory:\n# %sql duckdb:///path/to/file.db\n\n\n#collapse-output\n\n# we can also access the current config & settings of DuckDB by running the following:\n%sql SELECT * FROM duckdb_settings();\n\n\n\n\n\n\n\n\nname\nvalue\ndescription\ninput_type\n\n\n\n\n0\naccess_mode\nautomatic\nAccess mode of the database (AUTOMATIC, READ_O...\nVARCHAR\n\n\n1\ncheckpoint_threshold\n16.7MB\nThe WAL size threshold at which to automatical...\nVARCHAR\n\n\n2\ndebug_checkpoint_abort\nNULL\nDEBUG SETTING: trigger an abort while checkpoi...\nVARCHAR\n\n\n3\ndebug_force_external\nFalse\nDEBUG SETTING: force out-of-core computation f...\nBOOLEAN\n\n\n4\ndebug_many_free_list_blocks\nFalse\nDEBUG SETTING: add additional blocks to the fr...\nBOOLEAN\n\n\n5\ndebug_window_mode\nNULL\nDEBUG SETTING: switch window mode to use\nVARCHAR\n\n\n6\ndefault_collation\n\nThe collation setting used when none is specified\nVARCHAR\n\n\n7\ndefault_order\nasc\nThe order type used when none is specified (AS...\nVARCHAR\n\n\n8\ndefault_null_order\nnulls_first\nNull ordering used when none is specified (NUL...\nVARCHAR\n\n\n9\ndisabled_optimizers\n\nDEBUG SETTING: disable a specific set of optim...\nVARCHAR\n\n\n10\nenable_external_access\nTrue\nAllow the database to access external state (t...\nBOOLEAN\n\n\n11\nenable_object_cache\nFalse\nWhether or not object cache is used to cache e...\nBOOLEAN\n\n\n12\nenable_profiling\nNULL\nEnables profiling, and sets the output format ...\nVARCHAR\n\n\n13\nenable_progress_bar\nFalse\nEnables the progress bar, printing progress to...\nBOOLEAN\n\n\n14\nexplain_output\nphysical_only\nOutput of EXPLAIN statements (ALL, OPTIMIZED_O...\nVARCHAR\n\n\n15\nforce_compression\nNULL\nDEBUG SETTING: forces a specific compression m...\nVARCHAR\n\n\n16\nlog_query_path\nNULL\nSpecifies the path to which queries should be ...\nVARCHAR\n\n\n17\nmax_memory\n26.9GB\nThe maximum memory of the system (e.g. 1GB)\nVARCHAR\n\n\n18\nmemory_limit\n26.9GB\nThe maximum memory of the system (e.g. 1GB)\nVARCHAR\n\n\n19\nnull_order\nnulls_first\nNull ordering used when none is specified (NUL...\nVARCHAR\n\n\n20\nperfect_ht_threshold\n12\nThreshold in bytes for when to use a perfect h...\nBIGINT\n\n\n21\npreserve_identifier_case\nTrue\nWhether or not to preserve the identifier case...\nBOOLEAN\n\n\n22\nprofiler_history_size\nNULL\nSets the profiler history size\nBIGINT\n\n\n23\nprofile_output\n\nThe file to which profile output should be sav...\nVARCHAR\n\n\n24\nprofiling_mode\nNULL\nThe profiling mode (STANDARD or DETAILED)\nVARCHAR\n\n\n25\nprofiling_output\n\nThe file to which profile output should be sav...\nVARCHAR\n\n\n26\nprogress_bar_time\n2000\nSets the time (in milliseconds) how long a que...\nBIGINT\n\n\n27\nschema\n\nSets the default search schema. Equivalent to ...\nVARCHAR\n\n\n28\nsearch_path\n\nSets the default search search path as a comma...\nVARCHAR\n\n\n29\ntemp_directory\n.tmp\nSet the directory to which to write temp files\nVARCHAR\n\n\n30\nthreads\n12\nThe number of total threads used by the system.\nBIGINT\n\n\n31\nwal_autocheckpoint\n16.7MB\nThe WAL size threshold at which to automatical...\nVARCHAR\n\n\n32\nworker_threads\n12\nThe number of total threads used by the system.\nBIGINT\n\n\n33\nbinary_as_string\n\nIn Parquet files, interpret binary data as a s...\nBOOLEAN\n\n\n34\nCalendar\ngregorian\nThe current calendar\nVARCHAR\n\n\n35\nTimeZone\nEurope/Berlin\nThe current time zone\nVARCHAR\n\n\n\n\n\n\n\nFrom now on, you can run SQL directly by prefixing %sql (or %%sql for multiline statements) to your code cell and get your output returned as pandas dataframe :man_dancing:.\n\n%sql select 1 as a;\n\n\n\n\n\n\n\n\na\n\n\n\n\n0\n1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "title": "Supercharge your data processing with DuckDB",
    "section": "DuckDB vs traditional Databases",
    "text": "DuckDB vs traditional Databases\nWith pandas.read_sql command, one can already run SQL queries on an existing DB connection, read tables and load data as pandas DataFrames in memory for processing in python. While this is fine for lightweight operations, it is not optimized for heavy data processing. Traditional RDBMSs such as Postgres, MySQL, etc. process each row sequentially which apart from taking long time to execute, also induce a lot of overhead on CPU. DuckDB on the other hand is built with OLAP in mind and is Column-Vectorized. This helps massively parallelize disk I/O and query executions.\n\nDuckDB uses the Postgres SQL parser under the hood, and offers many of the same SQL features as Postgres 1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Projection & Filter Pushdowns",
    "text": "Projection & Filter Pushdowns\nNow let‚Äôs do a simple filter operation on our dataset. Let‚Äôs count the total number of rows that satisfy the condition TAXI_OUT &gt; 10. We‚Äôll try with both pandas & duckdb.\n\ndf[df['TAXI_OUT'] &gt; 10].shape\n\n(45209245, 28)\n\n\n\n%%sql\n\nselect count(*) as count\nfrom df\nwhere TAXI_OUT &gt; 10\n\n\n\n\n\n\n\n\ncount\n\n\n\n\n0\n45209245\n\n\n\n\n\n\n\nWhile the earlier operation took ~9.5s, the latter just took ~250ms :zap:. There‚Äôs just no comparison.\nThis is because duckdb automatically optimizes the query by selecting only the required column(s) (aka projection pushdown) and then applies the filtering to get a subset of data (aka filter pushdown). Pandas instead reads through all the columns. We can optimize this in pandas by doing these pushdowns ourselves.\n\nprojection_pushdown_df = df[['TAXI_OUT']]\nfilter_pushdown_df = projection_pushdown_df[projection_pushdown_df['TAXI_OUT'] &gt; 10]\nfilter_pushdown_df.shape\n\n(45209245, 1)\n\n\nWe managed to bring this down from several seconds to almost a second. But using duckdb is still about 70-90% faster than this."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Groupby",
    "text": "Using Groupby\nNow let‚Äôs calculate a few aggregates using groupby with projection & filter pushdowns combined.\nHere, we compute a few simple metrics with a certain airline carrier grouped by two origin & destination airports and finally sort the results by the origin airport.\n\nprojection_df = df[['ORIGIN', 'DEST', 'TAXI_OUT', \n                    'AIR_TIME', 'DISTANCE', 'OP_CARRIER']]\norigin_df = projection_df[\n    (projection_df['ORIGIN'].isin(('DCA', 'EWR'))) &\n    (projection_df['DEST'].isin(('DCA', 'EWR'))) &\n    (projection_df['OP_CARRIER'] == 'XE')]\n(origin_df\n     .groupby(['ORIGIN', 'DEST'])\n     .agg(\n         avg_taxi_out=('TAXI_OUT', 'mean'),\n         max_air_time=('AIR_TIME', 'max'),\n         total_distance=('DISTANCE', 'sum'))\n     .sort_index(level=0)\n)\n\n\n\n\n\n\n\n\n\navg_taxi_out\nmax_air_time\ntotal_distance\n\n\nORIGIN\nDEST\n\n\n\n\n\n\n\nDCA\nEWR\n22.116009\n87.0\n828835.0\n\n\nEWR\nDCA\n23.675481\n93.0\n831024.0\n\n\n\n\n\n\n\nWe can make it a bit more concise by using .query for filtering pushdown.\n\n(df\n .query('OP_CARRIER == \"XE\" and ORIGIN in (\"DCA\", \"EWR\") and DEST in (\"DCA\", \"EWR\")')\n .groupby(['ORIGIN', 'DEST'])\n .agg(\n     avg_taxi_out=('TAXI_OUT', 'mean'),\n     max_air_time=('AIR_TIME', 'max'),\n     total_distance=('DISTANCE', 'sum'))\n)\n\n\n\n\n\n\n\n\n\navg_taxi_out\nmax_air_time\ntotal_distance\n\n\nORIGIN\nDEST\n\n\n\n\n\n\n\nDCA\nEWR\n22.116009\n87.0\n828835.0\n\n\nEWR\nDCA\n23.675481\n93.0\n831024.0\n\n\n\n\n\n\n\nThis approach took only about half the time (~3s) compared to our earlier one because¬†.query uses a modified syntax of python and also indexing thus resulting in more efficient evaluation. We can now compare that to our SQL counterpart‚Ä¶\n\n%%sql\n\nselect\n    ORIGIN,\n    DEST,\n    AVG(TAXI_OUT) as avg_taxi_out,\n    MAX(AIR_TIME) as max_air_time,\n    SUM(DISTANCE) as total_distance\n\nfrom df\n\nwhere\n    OP_CARRIER = 'XE' and\n    ORIGIN in ('DCA', 'EWR') and\n    DEST in ('DCA', 'EWR')\n    \ngroup by ORIGIN, DEST\norder by ORIGIN\n\n\n\n\n\n\n\n\nORIGIN\nDEST\navg_taxi_out\nmax_air_time\ntotal_distance\n\n\n\n\n0\nDCA\nEWR\n22.116009\n87.0\n828835.0\n\n\n1\nEWR\nDCA\n23.675481\n93.0\n831024.0\n\n\n\n\n\n\n\nThis ~400ms execution with duckdb above is around an order of magnitude faster and also a lot cleaner, I‚Äôd say. :wink:\nNotice that the data is already loaded under df and hence we don‚Äôt need to read from the source parquet file.\n\nIn the same way, we can also improve the performance of our queries drastically when using joins across multiple tables. I leave this as an exercise to the reader.\n\nBut why actually load data into memory in the first place when we can process it more efficiently with it being just on disk? Often times, the data is too big to load into memory anyways.\nTo do that, we just need to create a VIEW to our data which lets us query the table directly without loading onto memory and update the source from the dataframe df to the newly created view instead.3"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using approximations",
    "text": "Using approximations\nAt times, it suffices just to get an estimate of certain data rather than a precise answer. Using approximations would help us to just that.\n\n%%sql\n\nselect\n    OP_CARRIER,\n    approx_count_distinct(DEST) as approx_num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n\n\n\nOP_CARRIER\napprox_num_unique_destinations\n\n\n\n\n0\n9E\n186\n\n\n1\nAA\n116\n\n\n2\nAS\n77\n\n\n3\nB6\n73\n\n\n4\nCO\n85\n\n\n5\nDL\n171\n\n\n6\nEV\n205\n\n\n7\nF9\n130\n\n\n8\nFL\n75\n\n\n9\nG4\n126\n\n\n\n\n\n\n\n\n%%sql\n\nselect\n    OP_CARRIER,\n    -- takes more time to compute\n    count(distinct DEST) as num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n\n\n\nOP_CARRIER\nnum_unique_destinations\n\n\n\n\n0\n9E\n185\n\n\n1\nAA\n116\n\n\n2\nAS\n77\n\n\n3\nB6\n73\n\n\n4\nCO\n85\n\n\n5\nDL\n170\n\n\n6\nEV\n205\n\n\n7\nF9\n129\n\n\n8\nFL\n75\n\n\n9\nG4\n126\n\n\n\n\n\n\n\nOur approximation query earlier ran about 3-4 times faster than the precise one in this case. This is crucial when responsiveness is more important than precision (esp.¬†for larger datasets)."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Window functions",
    "text": "Using Window functions\nFinally, let‚Äôs wrap our analysis by showing off a bit more of what duckdb can do using some advanced SQL operations.\nWe create two CTEs (Common Table Expressions) to calculate a couple of features. We do filter & projection pushdowns in one CTE and compute our desired features in another. The first feature is a simple demo to showcase if-else support. The second feature is a bit advanced where we find out the last destination a given air carrier has flown to, sorted by flying date. And when it doesn‚Äôt exist, replace it with NA. We then take a sample from the final resultant set.\n\n%%sql\n\nwith limited_data as (\n    select \n        FL_DATE,\n        ORIGIN, \n        DEST, \n        DISTANCE,\n        OP_CARRIER,\n    from airlinedata\n    where FL_DATE &gt;= '2015-01-01'    \n),\n\nlast_destination_data as (\n    select *,\n        case\n            when DISTANCE*1.60934 &gt; 500 then 'yes'\n            else 'no'\n        end as distance_more_than_500_km,\n\n        coalesce(last_value(DEST) over (\n            partition by OP_CARRIER\n            order by FL_DATE\n            rows between unbounded preceding and 1 preceding\n        ), 'NA') as last_destination_flown_with_this_carrier\n\n    from limited_data\n)\n\nselect *\nfrom last_destination_data\nusing sample 10;\n\n\n\n\n\n\n\n\nFL_DATE\nORIGIN\nDEST\nDISTANCE\nOP_CARRIER\ndistance_more_than_500_km\nlast_destination_flown_with_this_carrier\n\n\n\n\n0\n2018-07-10\nDCA\nLGA\n214.0\nYX\nno\nDCA\n\n\n1\n2015-05-08\nDAL\nBWI\n1209.0\nWN\nyes\nBWI\n\n\n2\n2018-03-30\nLAS\nSJC\n386.0\nWN\nyes\nSJC\n\n\n3\n2015-07-10\nBOS\nMSP\n1124.0\nDL\nyes\nDTW\n\n\n4\n2016-06-01\nDTW\nBWI\n409.0\nDL\nyes\nDTW\n\n\n5\n2017-07-26\nGEG\nMSP\n1175.0\nDL\nyes\nSAN\n\n\n6\n2017-01-10\nDFW\nACT\n89.0\nEV\nno\nDFW\n\n\n7\n2015-01-01\nPHX\nDRO\n351.0\nOO\nyes\nBFL\n\n\n8\n2018-05-06\nDFW\nMCO\n985.0\nAA\nyes\nIAH\n\n\n9\n2018-04-30\nMSY\nLAX\n1670.0\nWN\nyes\nLAX\n\n\n\n\n\n\n\nNice, isn‚Äôt it?! The same operation is unimaginably complex (for me, at least) in pandas. ü§Ø\nWith DuckDB, we can combine one or more of many of such complex operations and execute in one go without worrying much about manual optimizations."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#footnotes",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#footnotes",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSQL on Pandas with duckdb‚Ü©Ô∏é\nduckdb on parquet‚Ü©Ô∏é\nThe exact execution times might vary a bit depending on the load & build of your computer. I also noticed that the operations are cached and the first computation takes a bit of time but running it again or after changing the values of the columns in the WHERE clause would only take a couple of ms later on.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/fastai-env-setup/index.html",
    "href": "posts/fastai-env-setup/index.html",
    "title": "fastai env setup",
    "section": "",
    "text": "It‚Äôs been quite a while since I last dabbled myself in deep learning and therefore decided to revisit from the basics. And what better way than to start doing that than learning from fastai? :D In this post, we will see how to quickly setup your dev environment for running notebooks locally to put your hard earned GPUs to use :p\nOf course, you can run your notebooks on cloud with free GPU support on platforms such as Google Colab, Paperspace Gradient or even kaggle notebooks but sometimes, it feels good to run things locally without worrying too much about quotas or network issues etc. If you‚Äôre starting new in this field, it‚Äôs highly recommended to try the above platforms first.\nFirstly, you need mamba. Use it wherever you use conda because it‚Äôs much faster. Once you install it, run the following script:\n\n# create a conda environment\nmamba create -n fastai python=3.10\n\n# install suitable version of `pytorch-cuda` at your time of installation\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n\n# install fastai\nmamba install -c fastchan fastai\n\nmamba install -c conda-forge jupyterlab ipywidgets\nMake sure you can use GPU with pytorch by running this in your python session:\nimport torch\nassert torch.cuda.is_available()\nThat‚Äôs it. Now you can run fastai with GPU support locally simply by doing mamba activate fastai and launching jupyter ! üíö"
  },
  {
    "objectID": "posts/geospatial_intro_part_I/GIS_I.html",
    "href": "posts/geospatial_intro_part_I/GIS_I.html",
    "title": "Introduction to GIS with QGIS & Python - Part I",
    "section": "",
    "text": "As a data scientist, adding data analysis of geospatial information systems (GIS) to our skill set is a smart move in today‚Äôs data-driven world. The availability of free immense satellite and map data online, combined with the power of open source GIS tools, presents enormous opportunities for analyzing and visualizing geospatial data. With GIS, data scientists can enhance their data analytics and machine learning abilities, resulting in a more comprehensive understanding of complex problems such as climate change.\nBy leveraging GIS, we can monitor and track the effects of climate change on the planet by analyzing data from a wide range of sources, such as temperature sensors, satellite imagery, and ocean currents, to provide a better understanding of its impact on our environment. This information can then be used to inform decision-making processes, such as predicting sea level rise and assessing the impact on coastal cities etc.\nMoreover, this also empowers common people by allowing them to answer questions about their own environment and surroundings. For example, farmers can use GIS to monitor crop health, water availability, and soil quality, while city dwellers can use it to explore the impact of urbanization on the environment. Anyone can access these tools to perform basic analysis, enabling them to become citizen scientists and contribute to the health of our planet.\nIn a series of posts, we will try to explore the basics of GIS, and progress towards addressing some interesting questions through the application of QGIS, Python and data visualization. Although I am also new to this area and currently learning, I invite you to join me on this excursion of discovery.\nTogether, we shall learn, experiment and explore the potential of GIS to transform data analytics by combining it with geospatial information.\nBy the end of this session, you‚Äôll be able to do this:\nGIS, or Geographic Information System, is a tool for mapping and analyzing different types of data related to a specific location on üåç. It allows you to visualize data on a map, such as population density, land use, or weather patterns. By combining data from various sources, we can uncover patterns, relationships, and derive insights that may not be apparent from individual datasets alone. It can be used to answer questions such as: Where are the most vulnerable areas to flooding? How has urbanization changed over time? And, where should we build a new school to ensure accessibility to the largest number of students? etc.\nQGIS is an open source tool to explore this layered GIS. Download it from qgis.org and install it. You may be greeted with the following window. You can create & save the project by clicking Project -&gt; Save As on the top left menu of the application.\nThe UI is mainly divided into the following areas:\nQGIS has much, much more things to offer but these four are good enough to start with.\nThe first thing we do is get some basemaps.\nFor this, download the qgis_basemaps.py (courtesy of Asst. Prof.¬†Qiusheng Wu) and open the python console like so:\nPaste the downloaded script and hit run (green ‚ñ∂Ô∏è). You see all basemaps loaded under XYZ Tiles in browser.\nIn order to view a basemap, simply drag & drop any of them into the Canvas. You will notice that the Layers widget starts getting populated. Any subsequent basemaps that you drop to the canvas will get stacked here. In general, we need one basemap layer and one or more data layers for analysis. Which basemap to choose depends on the analysis you‚Äôre carrying.\nAs our initial use case, let‚Äôs examine the 2020 European Air Quality dataset (head here, hit Direct Download). This dataset provides concentrations for the air pollutants \\(NO_2\\) at 1 km grid.\nIt contains a .tif file along with other metadata.\nTo visualize the tif file (no2_avg_20.tif), simply drag & drop to the canvas. Here‚Äôs what it looks like (after some styling). As you can see, it‚Äôs beautiful & colorful but without context. That‚Äôs where our basemap comes in, as you might have expected. You can settle on any that‚Äôs appropriate for our use case here. I liked Esri National Geographic for this as it displays the borders of the countries more clearly. And remember, basemaps always come at the bottom. So make sure you reorder them in the Layers widget accordingly.\nThe default .tif file shows a single band grayscale image. A band is like a channel, much like RGB of a color image. But that looks dull though it has the potential to show much more visual information. We can convert those values into quantiles and visualize those instead. For that, we will now turn our attention to the styling section (shown below).\nOnce you click it, a new tab opens to the right in place of the processing toolbox area from the figure 3 above.\nMake sure that the .tif file is selected and do the following to enhance the visual information (Fig-6(b)):\nHere, we just binned the values of this grayscale image and assigned a color to each bin.\nIn a modern industrialized world, a good portion of air pollution is caused by human settlements. To find its effects, we can check where are its majot sites. Headover here and download populated places, urban areas, airports, ports datasets and unzip them. These contain data at 1:10 (i.e., 1cm = 100km) scale.\nDrag & drop the ne_10m_urban_areas shapefile layer onto canvas. This shows areas of dense human habitation.\nYou can customize the styling based on your choices and make sure it doesn‚Äôt override any of the data shown from the underlying layers. Here‚Äôre mine.\nNot surprisingly, most parts of urban settlement area is under the region with worse air quality (dark red region)\nLet‚Äôs now also add the populated places data layer but this time from the commandline with the help of python.\nTo do that, let‚Äôs open the python console from the top menu (plugins -&gt; python console or hit Ctrl+Alt+P). Qgis already makes an instance of its interface available under the variable iface.\nTo add this vector layer to the canvas, simply run\nYou can notice that the layer is then added with the name populated_places. Headover to its styling to choose how those individual data points are represented.\nAdding the other layers (i.e., ne_10m_ports & ne_10m_airports) similarly would give us our final result.\nThere‚Äôs so much information to unpack here (open in new tab for higher resolution) that I leave it as an exercise to the reader to derive their own insights.\nI hope this has helped you kickstart your journey into GIS analysis and understand our world a bit better. In the next post, we will see how to handle even more types of data, perform a complex processing pipeline and more.\nBis dann üëã"
  },
  {
    "objectID": "posts/geospatial_intro_part_I/GIS_I.html#footnotes",
    "href": "posts/geospatial_intro_part_I/GIS_I.html#footnotes",
    "title": "Introduction to GIS with QGIS & Python - Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2a and 2b‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/migrate-to-quarto/index.html",
    "href": "posts/migrate-to-quarto/index.html",
    "title": "Migrating from Fastpages to Quarto",
    "section": "",
    "text": "Fastpages, based on which my original blog was setup has been archived and they now recommend using Quarto instead. If you‚Äôre starting new, the latter is the recommended way to go but if you too have a fastpages blog setup initially and want to migrate, there‚Äôs a migration guide available. It‚Äôs not completely perfect as you have to tweak a few bits here & there before you see all the renderings correctly (because of slight syntactic variations amongst other things).\nHowever, I felt quarto to be much more intuitive and easy to setup. I also did the migration but found manually moving posts from my old blog repo to the new quarto repo to be a bit easier. (Migration worked but I was not happy with the directories it created as part of it‚Ä¶ I found it aesthetically less pleasing and hence moved them myself).\nFor simple blogging, fastpages offered more than enough features and quarto offers even more on top of that. It‚Äôs easy to get started with quarto. Head over to the start guide to learn more.\nI do miss the advanced styling options I setup in fastpages though‚Ä¶ will have to dig into Quarto to see how much I can reuse. Until then, have fun with my frugalistic looking blog! :D"
  },
  {
    "objectID": "posts/quick-pytorch-setup-with-pixi-Nvidia/index.html",
    "href": "posts/quick-pytorch-setup-with-pixi-Nvidia/index.html",
    "title": "Simple & Fast Local DL Setup with PyTorch, Pixi & Nvidia",
    "section": "",
    "text": "I recently distro-hopped and this time settled onto BlueFin, an atomic OS from Universal Blue based on  Silverblue. I had to setup my local deep learning environment again and this provided a nice opportunity to test a new setup afresh.\nI had previously setup fastai with mamba but this time I wanted to test pixi. It‚Äôs a lot faster and overall, a better alternative to mamba/conda.\nInstalling deep learning libraries locally is always a daunting task, dealing with system level dependencies and potentially corrupting them being one of the main reasons. Let‚Äôs see how easy and safe it is with pixi.\nFirst, make sure you have NVIDIA drivers setup correctly matching your system. You can check that out by running nvidia-smi on your cmdline which gives an output something similar to this.\n$ nvidia-smi\n\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 575.64.05              Driver Version: 575.64.05      CUDA Version: 12.9     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4060 ...    Off |   00000000:01:00.0 Off |                  N/A |\n| N/A   47C    P0             15W /   75W |      12MiB /   8188MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A            3550      G   /usr/bin/gnome-shell                      2MiB |\n+-----------------------------------------------------------------------------------------+\nAs you can see, I have nvidia driver v575 with CUDA v12.9. As of this writing, the latest CUDA version which pytorch supports is 12.8. So, we‚Äôre good to go.\nNow go ahead and install pixi by your preferred method and later run pixi init dl_setup to create a folder named dl_setup and initialize it. This creates a default pixi.toml configuration file which is similar to pyproject.toml but better.\nThe first section workspace of the TOML file should look like this:\n[workspace]\nchannels = [\"conda-forge\"]\nname = \"dl_setup\"\nplatforms = [\"linux-64\"]\nversion = \"0.1.0\"\n\n\n\n\n\n\nNote\n\n\n\nIf in case, you already use pyproject.toml or you prefer to have that instead, just pass the flag --format=pyproject to the init cmd earlier. You also need to do some extra steps if in case you go this way. (E.g., by prefixing each section with tool.prefix and including build-system section. Refer to pixi‚Äôs documentation)\n\n\nFirst run pixi add \"python&gt;=3.12\" to add python itself as a dependency.\nPixi supports installing PyPI dependencies alongside Conda packages, and you can typically run pixi add &lt;pkg&gt; --pypi to install a PyPi package. For example, PyTorch‚Äîwhich is now officially available only via PyPi‚Äîcould be installed this way. However, currently, it is not possible to specify a custom index-url (the URL from which to download wheels) via the command line. Therefore, you need to manually edit the pixi.toml file to set the appropriate index URL.\nBefore that, quickly note the latest versions (minus the patch versions) of torch, torchaudio & torchvision from PyPi so that we can manually add them in the config. Also, since this setup involves system dependencies (e.g., CUDA), we need to specify that as well so that pixi can take advantage of that during dependency resolution.\n[system-requirements]                                         \ncuda = \"12.0\"    # just major version suffices\n\n[feature.gpu.pypi-dependencies]\ntorch = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchaudio = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchvision = { version = \"&gt;=0.22.0\", index = \"https://download.pytorch.org/whl/cu128\" }\nWe have added a feature section named gpu and since these are PyPi dependencies, you notice that appended at the last. Pixi already defines a default environment named default. We just need to include this feature to that environment before creating it by running the following on the cmdline:\n$ pixi workspace environment add default --feature=gpu --force \nIt creates the following section in the toml file which makes sure that the gpu dependencies are included in that environment when run.\n[environments]\ndefault = [\"gpu\"]\nLet‚Äôs install Jupyter as well so that we can explore interactively. Since pixi handles conda and pip dependencies, we can safely run pixi add jupyterlab that fetches this from conda-forge channel by default.\nFinally, if we want to run a jupyter notebook session with a shortcut, we can add a task in pixi by running:\n$ pixi task add jupyter \"jupyter lab\"\n\nThe whole pixi.toml might look something like this:\n[workspace]\nchannels = [\"conda-forge\"]\nname = \"dl_setup\"\nplatforms = [\"linux-64\"]\nversion = \"0.1.0\"\n\n[system-requirements]\ncuda = \"12.0\"\n\n[dependencies]\npython = \"&gt;=3.12\"\njupyterlab = \"&gt;=4.4.5,&lt;5\"\nnumpy = \"&gt;=2.3.2,&lt;3\"\npandas = \"&gt;=2.3.1,&lt;3\"\nseaborn = \"&gt;=0.13.2,&lt;0.14\"\n\n[feature.gpu.pypi-dependencies]\ntorch = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchaudio = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchvision = { version = \"&gt;=0.22.0\", index = \"https://download.pytorch.org/whl/cu128\" }\n\n[environments]\ndefault = [\"gpu\"]\n\n[tasks]\njupyter = \"jupyter lab\"\n\nThe versions might differ for you depending on the platform or when you run this.\nWe can now enjoy a full fledged jupyter lab session by simply running pixi run jupyter at the commandline.\nThat‚Äôs it! Happy coding!!! ‚ö°Ô∏è‚ú®"
  }
]