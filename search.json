[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting my humble abode on the Internet. I am Vivek, a freelance Data Scientist & AI Engineer by profession.\nSa·πÉsƒÅra means World in Sanskrit and I welcome you into mine where I blog about programming, among other things.\nI‚Äôve been building ML solutions for the past 6+ years or so ‚Äî from spotting anomalies in energy data to fraud detection for BNPL, churn prediction and forecasting for mobility, and compliance backends for a Swiss climate tech. I‚Äôve dug into satellite imagery to assess vegetation health, helped classify product carbon footprints with GenAI & LLMs, and worked on optimizing smart-heating systems with IoT data. I love tinkering in Python, NLP, GenAI while keeping things practical, a little playful, and always aim to leverage AI for social & environmental good.\nWhen not working, I take interest in üè∏, üé∂, üßë‚Äçüç≥ and aim to know a little bit of everything I come across.\nPlease feel free to reach me for any business, consulting or workshop opportunities. üòÉ"
  },
  {
    "objectID": "about.html#namaste",
    "href": "about.html#namaste",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting my humble abode on the Internet. I am Vivek, a freelance Data Scientist & AI Engineer by profession.\nSa·πÉsƒÅra means World in Sanskrit and I welcome you into mine where I blog about programming, among other things.\nI‚Äôve been building ML solutions for the past 6+ years or so ‚Äî from spotting anomalies in energy data to fraud detection for BNPL, churn prediction and forecasting for mobility, and compliance backends for a Swiss climate tech. I‚Äôve dug into satellite imagery to assess vegetation health, helped classify product carbon footprints with GenAI & LLMs, and worked on optimizing smart-heating systems with IoT data. I love tinkering in Python, NLP, GenAI while keeping things practical, a little playful, and always aim to leverage AI for social & environmental good.\nWhen not working, I take interest in üè∏, üé∂, üßë‚Äçüç≥ and aim to know a little bit of everything I come across.\nPlease feel free to reach me for any business, consulting or workshop opportunities. üòÉ"
  },
  {
    "objectID": "posts/layout-sim/layout-sim.html",
    "href": "posts/layout-sim/layout-sim.html",
    "title": "Document Layout Similarity Detection",
    "section": "",
    "text": "When you‚Äôre processing hundreds or thousands of structured image documents‚Äîlike tax forms, Energy Performance Certificates (EPCs), or insurance statements etc.‚Äîextracting text from every single page can be slow and costly. Often, you don‚Äôt need to know what the text says right away; you just need to know whether the layout matches a known template before investing in expensive OCR or NLP pipelines.\nBy comparing documents purely based on structure and layout, you can quickly filter, categorize, or flag anomalies. This approach is invaluable for any domain where documents follow consistent formats: mortgage applications, legal contracts, invoices, or medical records. In this post, we‚Äôll explore a lightweight method to perform this, letting you decide where to dig deeper‚Äîonly when it matters.\nThe following code is available on github. Feel free to clone and run the notebook to follow."
  },
  {
    "objectID": "posts/layout-sim/layout-sim.html#merge-masks",
    "href": "posts/layout-sim/layout-sim.html#merge-masks",
    "title": "Document Layout Similarity Detection",
    "section": "Merge Masks",
    "text": "Merge Masks\n\nfrom src.doclayout import get_merged_mask\nmerged_mask = get_merged_mask(\"data_files/reference.png\")\nmerged_mask\n\n\n\n\n\n\n\n\nWe merged the colored & grayscale variations of the reference image and standardized its dimensions to 512x512 to compare with other sample images later on. The darker a portion is, the stronger the overlap. Sometimes, the labels would also be different for a given layout detected, resulting in a mixed color in the final merged mask which would allow us to see disagreements between different masks."
  },
  {
    "objectID": "posts/layout-sim/layout-sim.html#embeddings",
    "href": "posts/layout-sim/layout-sim.html#embeddings",
    "title": "Document Layout Similarity Detection",
    "section": "Embeddings",
    "text": "Embeddings\nWe now derive image embeddings for these merged masks. By projecting bitmap masks into an embedding space, we turn the visual layout into a compact, numerical representation. This lets you quickly measure similarity between documents or images without expensive pixel-by-pixel comparisons or OCR. Besides, it scales well. ;)\n\nImage\nFor this, we use dino-vits16 model.\nHere we load a reference image along with a matching and non-matching training images to explore.\n\n\nCode\n# load the model and processor\n# ckpt = \"google/siglip2-base-patch16-224\"\nckpt = \"facebook/dino-vits16\"\n\nemb_model = AutoModel.from_pretrained(ckpt).eval()\nemb_processor = AutoProcessor.from_pretrained(ckpt, use_fast=True)\n\nmask_ref = get_merged_mask(\"data_files/reference.png\")\nmask_samp = get_merged_mask(\"data_files/train_images/m_0.png\")\nmask_samp_nomatch = get_merged_mask(\"data_files//train_images/nm_1.jpg\")\n\nimg1 = Image.open(\"data_files/train_images/m_0.png\").resize((512, 512))\nimg2 = Image.open(\"data_files/train_images/nm_1.jpg\").resize((512, 512))\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns\n\naxes[0].imshow(img1)\naxes[0].axis(\"off\")  # hide axes\naxes[0].set_title(\"Match\")\n\naxes[1].imshow(img2)\naxes[1].axis(\"off\")\naxes[1].set_title(\"No Match\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nget_hf_cosine_similarity(\n    emb_model, emb_processor, [mask_ref, mask_samp, mask_samp_nomatch]\n)\n\n[0.9231094121932983, 0.8350296020507812]\n\n\nWe see that our reference image is closer to the sample image than to the non-matching sample. That‚Äôs great news!\nLet‚Äôs compute few other statisical embeddings.\n\n\nManual\n\nmask_emb_scores = get_mask_embedding_scores([mask_ref, *[mask_samp, mask_samp_nomatch]])\nmask_emb_scores\n\ndefaultdict(list,\n            {'cos_sim_hu_moments': [-0.3719817103657597, 0.10362230071894155],\n             'cos_sim_zernike': [0.9552975209574077, 0.8994658287209426],\n             'cos_sim_fourier': [0.9995830287092948, 0.6172465282448778],\n             'cos_sim_regionprops': [0.9999954558167454, 0.9999837891488262],\n             'cos_sim_hog': [0.589646804245924, 0.3039637953469709]})\n\n\nWe already see that many metrics above lean towards identifying correct match. Ofc, we need test it on more data before we generalise."
  },
  {
    "objectID": "posts/layout-sim/layout-sim.html#distance-similarity-metrics",
    "href": "posts/layout-sim/layout-sim.html#distance-similarity-metrics",
    "title": "Document Layout Similarity Detection",
    "section": "Distance & Similarity Metrics",
    "text": "Distance & Similarity Metrics\nWe now compute some Intersection and Correlation metrics of pixel distribution histrograms, metrics based on image hashes etc.\n\nmetrics = get_common_metrics(mask_ref, [mask_samp, mask_samp_nomatch])\nmetrics\n\ndefaultdict(list,\n            {'metric_avg_hash': [np.int64(13), np.int64(19)],\n             'metric_phash': [np.int64(20), np.int64(26)],\n             'metric_phash_simple': [np.int64(24), np.int64(20)],\n             'metric_dhash': [np.int64(22), np.int64(24)],\n             'metric_dhash_vertical': [np.int64(14), np.int64(30)],\n             'metric_whash': [np.int64(16), np.int64(12)],\n             'metric_crop_resistant_hash': [1.020833333333333,\n              0.03348214285714324],\n             'metric_ssim': [0.716413475931163, 0.5832392822458005],\n             'metric_dice_coeff': [0.741792643470003, 0.7053555824309493],\n             'metric_iou': [0.5895631110565024, 0.5448257242366155],\n             'metric_hist_sim': [0.9863640115685008, 0.909850095867875]})"
  },
  {
    "objectID": "posts/layout-sim/layout-sim.html#gather-features",
    "href": "posts/layout-sim/layout-sim.html#gather-features",
    "title": "Document Layout Similarity Detection",
    "section": "Gather Features",
    "text": "Gather Features\nget_features computes all the features we‚Äôve computed so far and wraps them in a dataframe.\n\nfeatures = get_features(mask_ref, [mask_samp, mask_samp_nomatch])\nfeatures.head().T\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\ncos_sim_hf\n0.923109\n0.835030\n\n\ncos_sim_hu_moments\n-0.371982\n0.103622\n\n\ncos_sim_zernike\n0.955298\n0.899466\n\n\ncos_sim_fourier\n0.999583\n0.617247\n\n\ncos_sim_regionprops\n0.999995\n0.999984\n\n\ncos_sim_hog\n0.589647\n0.303964\n\n\nmetric_avg_hash\n13.000000\n19.000000\n\n\nmetric_phash\n20.000000\n26.000000\n\n\nmetric_phash_simple\n24.000000\n20.000000\n\n\nmetric_dhash\n22.000000\n24.000000\n\n\nmetric_dhash_vertical\n14.000000\n30.000000\n\n\nmetric_whash\n16.000000\n12.000000\n\n\nmetric_crop_resistant_hash\n1.020833\n0.033482\n\n\nmetric_ssim\n0.716413\n0.583239\n\n\nmetric_dice_coeff\n0.741793\n0.705356\n\n\nmetric_iou\n0.589563\n0.544826\n\n\nmetric_hist_sim\n0.986364\n0.909850\n\n\n\n\n\n\n\nThis is now a simple binary classification problem with tabular data. We assign the label 1 if the training image is similar to the reference image, 0 otherwise. We‚Äôll skip the training part here in this post for brevity and just take a look at the inference done with gradio. For complete training, feature importances, model explainability, refer to the code in the repo or its notebook. Checkout the project‚Äôs README to know more.\nHere‚Äôs a short demo:\n\n\nCode\nfrom IPython.display import Video\nVideo(\"demo.mp4\", embed=True, width=750, height=500)\n\n\n\n \n Your browser does not support the video tag."
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html",
    "href": "posts/agentic-job-search/agentic-job-search.html",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "",
    "text": "Job searching is hard. Anybody looking for one knows how draining it can be to sift through endless listings, hoping to find something that fits or feels right. Ain‚Äôt nobody got time for dat‚Ä¶\n\nInstead, we use agents ü§ñ to keep track of openings at companies we care about and in departments we want to work in. The challenge is, not every company offers a üîî / üì© /  to notify you when new positions open up. And even when they do, how often do you actually check those newsletters? Thought so! üòâ\nWith ‚ú®agentic job search‚ú®, you get a streamlined report of all the current openings at companies you‚Äôre interested in, tailored to the departments you want to work in. This is made possible by web scraping combined with agents‚Äîspecialized programs that can run tasks independently and make decisions without human intervention. For this project, we rely on CrewAI, one of the leading agentic frameworks out there. It brings together multiple specialized AI agents into a collaborative team, letting them work together smoothly to tackle complex tasks. Think of it as an AI squad with defined roles and workflows, functioning much like a well-oiled  team.\n\n\n\n\n\n\nNote\n\n\n\nThe inspiration for this project came from one of the use cases demoed in crewAI‚Äôs courses on deeplearning.ai. I highly recommend checking them out. Their approach to job searching is a bit different and more generic but equally powerful."
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html#intro",
    "href": "posts/agentic-job-search/agentic-job-search.html#intro",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "",
    "text": "Job searching is hard. Anybody looking for one knows how draining it can be to sift through endless listings, hoping to find something that fits or feels right. Ain‚Äôt nobody got time for dat‚Ä¶\n\nInstead, we use agents ü§ñ to keep track of openings at companies we care about and in departments we want to work in. The challenge is, not every company offers a üîî / üì© /  to notify you when new positions open up. And even when they do, how often do you actually check those newsletters? Thought so! üòâ\nWith ‚ú®agentic job search‚ú®, you get a streamlined report of all the current openings at companies you‚Äôre interested in, tailored to the departments you want to work in. This is made possible by web scraping combined with agents‚Äîspecialized programs that can run tasks independently and make decisions without human intervention. For this project, we rely on CrewAI, one of the leading agentic frameworks out there. It brings together multiple specialized AI agents into a collaborative team, letting them work together smoothly to tackle complex tasks. Think of it as an AI squad with defined roles and workflows, functioning much like a well-oiled  team.\n\n\n\n\n\n\nNote\n\n\n\nThe inspiration for this project came from one of the use cases demoed in crewAI‚Äôs courses on deeplearning.ai. I highly recommend checking them out. Their approach to job searching is a bit different and more generic but equally powerful."
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html#prerequisites",
    "href": "posts/agentic-job-search/agentic-job-search.html#prerequisites",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore diving in, you‚Äôll need:\n\nTechnical Skills: Basic Python knowledge, familiarity with HTML/CSS, command-line operations\nSystem Requirements: Python 3.10+, 4GB+ RAM (16GB+ recommended for local LLMs along with a supporting GPU)\nAPI Access: Account with any cloud LLM provider (optional if using Ollama)\nTime Investment: 30-45 minutes for initial setup, a few minutes per run thereafter\n\nThis guide targets intermediate developers comfortable with web scraping concepts and API integrations."
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html#web-scraping",
    "href": "posts/agentic-job-search/agentic-job-search.html#web-scraping",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "Web Scraping",
    "text": "Web Scraping\nBefore we look at what ü§ñ do, we need to look at the good‚Äôol web scraping. The idea is simple. You create a list of your favorite organizations and a department/sector that you‚Äôd like to filter for. Then you scrape the  for the latest openings and store it on disk. (We use Playwright  package for this) After that, the ü§ñ steps in to sift through these roles and handpick the ones that match your interests.\nThere are a few good reasons why we can‚Äôt just tell an agent to hop onto a website and grab the latest job postings directly:\n\nThe large language model (LLM) the agent relies on might not be powerful enough to fully get the instructions or might miss parts of the content‚Äîafter all, not everyone‚Äôs got access to OpenAI‚Äôs top-tier models!\nDynamic content on job listing pages can trip up the model. Sometimes the section you want isn‚Äôt even loaded yet when the agent tries to fetch it, especially since many organizations use third-party platforms like Ashby or Workable that embed listings via JavaScript.\nSome websites are massive, with an overwhelming number of openings across multiple locations, making it tough to scrape everything efficiently.\nModels can hallucinate‚Äîmeaning the job listings they generate might look legit but actually be inaccurate, outdated, or just plain wrong based on their training data or knowledge cut-off.\n\nThat‚Äôs why the scraping step here is crucial. To make it manageable, we create a YAML file where we define specific attributes that help us target and fetch the right content later on.\nHere‚Äôs a sample:\n#orgs.yaml sample file\n\nbrowser_company:\n  url: https://jobs.ashbyhq.com/The%20Browser%20Company\n  selector: \"div._departments_12ylk_345\"\n\necosia: \n  url: https://jobs.ashbyhq.com/ecosia.org\n  selector: \"#root &gt; div._container_ud4nd_29._section_12ylk_341 &gt; div._content_ud4nd_71 &gt; div._departments_12ylk_345\"\n\nmozilla:\n  url: https://www.mozilla.org/en-US/careers/listings/\n  selector: '#listings-positions tbody tr.position[data-team=\"Strategy, Operations, Data & Ads\"]'\nThe key here is the organization name you‚Äôre interested in, and the values hold the URL plus a selector. These are CSS patterns that identify specific HTML elements on a webpage, so you can zero in on the exact part you want to scrape.\nFinding the right selector is usually straightforward but sometimes can be a bit tricky. If the webpage uses dynamic content, wait until the job listings or tables have fully loaded, then right-click on the listing area and choose Inspect. That opens the developer tools where you can explore the DOM tree. Move up or down through parent and child elements until you spot the container holding the data. As you click on different elements, they‚Äôll highlight corresponding webpage parts, helping you confirm you got the right one. Finally, right click on the element (or on the ‚Ä¶ dots at the start of the line), and choose copy -&gt; copy selector as shown below.\n\n\n\n\n\n\nFigure¬†1: Sample table view of openings at Mozilla\n\n\n\nUsing the selector approach has several key advantages:\n\nIt enables scraping of targeted and precise content‚Äîyou download just the data you need, or at most a manageable superset that can be filtered later by an agent or LLM.\nIt‚Äôs reliable because website layouts don‚Äôt change that often. When they do, you simply update the selector in the YAML file, and you‚Äôre good to go (well, almost!). Plus, the program logs failures if scraping doesn‚Äôt work, so you can jump in and fix it manually.\nIt allows for a highly effective, yet optional, use of a powerful model‚Äîsince we feed the LLM only the stripped-down HTML content, even mid-sized models can deliver great results.\n\nThis method ensures the ü§ñ always work with the freshest and cleanest data, making filtering faster, simpler, and more accurate. They‚Äôre happy, you‚Äôre happy! Now, let‚Äôs see how it all comes together.\nThe complete code and installation instructions are available on . Head over there to setup the repo, then come back, and we‚Äôll dive into the details!"
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html#run-with-agents",
    "href": "posts/agentic-job-search/agentic-job-search.html#run-with-agents",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "Run with agents",
    "text": "Run with agents\nHopefully, you‚Äôre now set up. You only needed to do three things:\n\nMake sure the creds.yaml file is filled with the credentials for your favorite LLM.\n\nWhile it currently supports three providers out of the box, adding another cloud provider‚Äîlike Anthropic‚Äîis as simple as adding the corresponding entries to the file and passing the provider name as an argument to main.py.\nDifferent providers make different env variables available apart from common ones like API_KEY. Simply add them under the corresponding provider in the YAML file.\n\nPopulate the orgs.yaml file with your favorite organizations.\nSet the JOB_TOPIC value in src/config.py with a topic or department you want to filter roles for. Alternatively, you can pass this as a parameter directly to the main.py script.\n\nWith these in place, you‚Äôre ready to roll!! ü•Å\n\n\n\n\n\n\nNote\n\n\n\nIf you want to do LLM inferencing locally or from a remote endpoint, checkout the ü¶ôollamaü¶ô section below.\n\n\nTo run your setup, simply do:\ncrewai run\nIf you run into any errors, the first place to check is the logs.log file‚Äîit‚Äôs where all the action gets recorded. And if you bump into a ModuleNotFoundError or your command won‚Äôt execute properly, try running this command instead:\nPYHONPATH='.' uv run run_crew\nThe completed jobs report is available under src/scrape/jobs directory. Here‚Äôs a sample:\n[\n  {\n    \"org\": \"mozilla\",\n    \"url\": \"https://www.mozilla.org/en-US/careers/listings/\",\n    \"jobs\": [\n      {\n        \"title\": \"Staff Data Scientist - Operations\",\n        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/7141659/\",\n        \"location\": \"Remote UK\",\n        \"workplaceType\": null\n      },\n      {\n        \"title\": \"Manager, Data Science\",\n        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/6989335/\",\n        \"location\": \"Remote Canada, Remote US\",\n        \"workplaceType\": null\n      },\n      {\n        \"title\": \"Senior Data Scientist\",\n        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/7125910/\",\n        \"location\": \"Remote Canada, Remote US\",\n        \"workplaceType\": null\n      },\n      {\n        \"title\": \"Senior Data Scientist\",\n        \"href\": \"https://www.mozilla.org/en-US/careers/position/gh/7137777/\",\n        \"location\": \"Remote US\",\n        \"workplaceType\": null\n      }\n    ]\n  },\n  {\n    \"org\": \"ecosia\",\n    \"url\": \"https://jobs.ashbyhq.com/ecosia.org\",\n    \"jobs\": []\n  },\n  ...\n]\nSweet!!! Now, all that‚Äôs left is to browse through the filtered roles at your favorite companies, click on the listings you‚Äôre genuinely interested in, and maybe even apply‚Äîall while comfortably sipping your ‚òïÔ∏è with one hand. üòé\nThat said, this setup works best if you have access to a decent model from a reliable cloud provider‚Äîbut those don‚Äôt come cheap. CrewAI agents make multiple calls to the LLM for each organization (including tool calls), so if you‚Äôre not on a generous plan, the tokens consumed and requests made can quickly add up, especially as you add more companies to your YAML file.\nIt gets trickier if the  scraper can‚Äôt locate your specified selector and ends up scraping the entire page as a fallback, or if the listings come from a large company and you forgot to include adequate filters in your selector. Feeding huge chunks of HTML content‚Äîbeyond just the system and assistant prompts‚Äîüî• through tokens fast. And even worse, if the model makes a mistake, you often have to feed its response back into the system, which further increases both calls and token usage.\nFor those without a paid subscription, there are alternatives like openrouter and AIML, though the free tiers are quite restrictive. These options work well for manual testing but aren‚Äôt suited for the high volume of calls that CrewAI requires. If you need to handle larger workloads, you might want to explore the Programmatic approach section optionally combined with ollama for better efficiency."
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html#ollama",
    "href": "posts/agentic-job-search/agentic-job-search.html#ollama",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "Ollama",
    "text": "Ollama\nIf you have a (beefy) system at home or access to one, you can then run LLMs locally with ollama. Their extensive library offers a range of models that can comfortably run on your hardware for inference tasks. We won‚Äôt cover its installation and pre-requisites but assume that it‚Äôs already setup on your system, which you can check by running which ollama.\nWith my üíª config, I was able to run gpt-oss:20b after closing all other applications, but the model response time remained quite low. It might be fine for having a (patient) conversation but isn‚Äôt fast enough to run our agents effectively. That said, as mentioned earlier, we don‚Äôt necessarily need a highly advanced (reasoning) model for this use case since the tasks aren‚Äôt that complex.\nWhat we really need is a model that runs comfortably on our systems and is preferably trained on code. So, I settled on Qwen2.5-coder, which showed strong performance on various coding tasks and programming languages. Of course, feel free to pick whichever model suits your needs best!\n\n\n\n\n\n\nNote\n\n\n\nWith ollama, you don‚Äôt need an API key to run it locally. So you can skip the OLLAMA_API_KEY in the creds.yaml file but make sure you fill up the rest.\n\n\nWhen running an Ollama model locally, you can use it to power your agents instead of relying on expensive cloud providers. Just change the provider name to OLLAMA in the main.py file suggested earlier (assuming you‚Äôve set up its credentials in creds.yaml)."
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html#programmatic-job-search",
    "href": "posts/agentic-job-search/agentic-job-search.html#programmatic-job-search",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "Programmatic Job Search",
    "text": "Programmatic Job Search\nAlternatively, you can bypass the agentic workflow altogether and opt for a programmatic approach that sequentially processes each organization to fetch job listings. This method supports using your preferred cloud provider or a local ollama model. Simply configure the appropriate provider along with any optional parameters. To execute the programmatic fetch, run:\nuv run run_manual\n\n\n\n\n\n\nNote\n\n\n\nUnfortunately, I could only test my setup with free cloud providers and with ollama. If in case you face errors, please raise an issue on  so that it might be resolved with community‚Äôs help."
  },
  {
    "objectID": "posts/agentic-job-search/agentic-job-search.html#metrics",
    "href": "posts/agentic-job-search/agentic-job-search.html#metrics",
    "title": "Curate jobs with Agentic AI & Ollama",
    "section": "Metrics",
    "text": "Metrics\nOn my rig, scraping only took a couple of seconds per organization and about 30s for inferencing and preparing the job report with qwen2.5-coder & ollama per organization. The tokens per second roughly hovered between 6-10 and the response time is usually under 5 seconds. The input prompt tokens roughly are in a couple of thousands (less than 5K) because we send a blob of HTML but the response tokens relatively are far less - roughly around 500. For orgs with no listings, it‚Äôs negligible (&lt;10). YMMV\n\n\n\n\n\n\nNote\n\n\n\nIf doing local LLM inferencing, you can check your metrics in the logs whenever the model makes an inference."
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "",
    "text": "Welcome back! In this post, we will dive into Fastai‚Äôs mid-level API and learn how they help us build custom pipelines & dataloaders with the help of a simple computer vision (CV) example.\nThe dataset we‚Äôll use is the hello world equivalent of CV called MNIST. Now, there‚Äôre various ways to get this data and in fact, fastai provides it as a direct download from its URLs.MNIST attribute but I recently took part in a kaggle competition1 that provided data in an unusual way‚Ä¶ one that‚Äôs not common enough to load via standard fastai methods. So I thought of making this post to show how that can be done! Let‚Äôs go!\n# command to print multiple outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#transforms",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#transforms",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Transforms",
    "text": "Transforms\nThe image data is in the form of an array and the labels as a column in a dataframe. I couldn‚Äôt find a way to load that using the standard DataBlock API as shown in my face mask detection post right out of the way we have it (i.e., data not being stored on disk). Perhaps one can convert each row from a 784 array to a 28x28 matrix, store it as an image in a column and then load it using one of the ImageDataLoaders methods as shown above. But that sounds a bit more hacky to me compared to the elegant ways Transforms provides.\nTransform is a class that one can inherit from and it has 3 main methods to implement.\n\nencodes takes an item and transforms our data into the way we want (our custom transformation)\nsetups is an optional method that sets the inner state, if there‚Äôs any\ndecodes which is an optional step too that acts as (near) opposite to encodes. It tries to undo the operations performed in encodes, if and when possible.\n\nHere‚Äôs a simple example from one of the lessons in fastai:\n\nclass NormalizeMean(Transform):\n    def setups(self, items): self.mean = sum(items)/len(items)\n    def encodes(self, x): return x-self.mean\n    def decodes(self, x): return x+self.mean\n\nHere, setups stores the inner state of the passed items i.e., the mean, encodes returns the data with the mean subtracted and decodes returns the data with the mean added (opposite of encodes).\n\n# initialize\nnm = NormalizeMean()\nnm\n\nNormalizeMean:\nencodes: (object,object) -&gt; encodes\ndecodes: (object,object) -&gt; decodes\n\n\n\nnm.setup([1, 2, 3, 4, 5])\n\n\nnm.mean\n\n3.0\n\n\n\nnm(2)\n\n-1.0\n\n\n\nnm.decode(2)\n\n5.0\n\n\nUsing this, let‚Äôs see how an image can be obtained from an array of pixels! The idea is to pass one row of our training data and get an image & a label in return‚Ä¶\n\ntrain.head()\n\n\n\n\n\n\n\n\nlabel\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows √ó 785 columns\n\n\n\n\nxx = train.loc[0][1:].values.reshape(-1, 28).astype(np.uint8)\nxx.shape\nshow_image(xx);\n\n\n\n\n\n\n\n\nLet‚Äôs write the same as a transform that can run on all the datapoints.\n\nclass Convert_to_image(Transform):\n    def encodes(self, x):\n        mat = x[1:].reshape(-1, 28).astype(np.uint8)\n        return PILImage.create(mat)\n\nIn our case, we don‚Äôt need to maintain any inner state, hence the setups method was skipped. Also there‚Äôs no need to revert to the original array state (although one can) and therefore the decodes method too was skipped.\nLet‚Äôs test this by taking a sample row from our training dataset.\n\nrow = train.values[10]\nrow.shape\n\n(785,)\n\n\n\nc2i = Convert_to_image()\nc2i(row)\n\n\n\n\n\n\n\n\nYay! Now we have an image. Let‚Äôs also extract the label out of the same data, which is the first value in the array‚Ä¶\n\nclass Extract_label(Transform):\n    def encodes(self, x):\n        return x[0]\n\n\nel = Extract_label()\nel(row)\n\n8\n\n\nSweet!"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#pipelines",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#pipelines",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Pipelines",
    "text": "Pipelines\nNow in order to construct our dataloaders, we still need to run a few more transformations on our independent & dependent variables such as converting the data to a tensor to take advantage of GPU etc. Pipeline helps us build a list of transformations to run on our data sequentially. Let‚Äôs compose two pipelines: one that acts on our dependent data (i.e., images) and another on our independent data (i.e., labels).\n\nx_pipe = Pipeline([Convert_to_image, ToTensor])\ny_pipe = Pipeline([Extract_label, Categorize(vocab=train.label.unique())])\n\nFor our dependent data, first Convert_to_image was run, which takes a row from the dataframe, extract our pixel array, reshapes, converts that to a matrix and then to an image. Later it was converted to a tensor with a ToTensor built-in transformation.\nFor our independent data, the label was first extracted as defined in the Extract_label transform above and later converted to a Category that we want to predict using Categorize built-in transformation. The total possible labels that can be predicted was passed to the vocab (stands for vocabulary) parameter.\nNow let‚Äôs run the pipeline to see what we get!\n\nshow_image(x_pipe(row));\n\n\n\n\n\n\n\n\n\ny_pipe(row)\n\nTensorCategory(8)\n\n\nNice! We‚Äôre now ready to construct our dataloaders."
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#custom-datasets-dataloaders",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#custom-datasets-dataloaders",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Custom Datasets & DataLoaders",
    "text": "Custom Datasets & DataLoaders\nTo construct dataloaders, a Datasets object needs to be created which takes raw data and can apply multiple pipelines in parallel. This wil be used to run our independent & dependent data piplines together. Optionally the parameter splits can be specified using one of the Splitter transforms, in this case a RandomSplitter which returns training & validation indices extracted from our raw dataset.\nAs we see, most of the functions that‚Äôre used regularly in fastai are actually transformations itself. :)\n\nsplits = RandomSplitter(valid_pct=.2)(np.arange(train.shape[0]))\n\n\ndsets = Datasets(train.values, [x_pipe, y_pipe], splits=splits)\n\nwith the Datasets object now obtained, we construct DataLoaders object by simply calling .dataloaders on it. Since we are not collecting data from disk, we don‚Äôt have to specify the path and can optionally set a batch_size with bs. As these are just 28x28 images, we can set a bigger batch size.\n\ndls = dsets.dataloaders(bs=512)\n\n\nlen(dls.train.items), len(dls.valid.items)\n\n(33600, 8400)\n\n\n\ndls.train.show_batch()\n\n\n\n\n\n\n\n\n\ndls.valid.show_batch()\n\n\n\n\n\n\n\n\nFinally! We have come a long way extracting the raw data from a dataframe to be able to construct a dataloaders object with image as our dependent datablock and category as our indenpendent datablock!"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#train",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#train",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Train",
    "text": "Train\nYou can now use this data to train using fastai‚Äôs vision_learner method!\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nlearner = vision_learner(dls, resnet18, metrics=accuracy).to_fp16()\n\n\nlearner.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0020892962347716093)\n\n\n\n\n\n\n\n\n\n\nlearner.fine_tune(5, 5e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.755852\n0.339413\n0.915119\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.119606\n0.081445\n0.981310\n00:07\n\n\n1\n0.092199\n0.080963\n0.983571\n00:07\n\n\n2\n0.054804\n0.051717\n0.986905\n00:07\n\n\n3\n0.029896\n0.037651\n0.991667\n00:07\n\n\n4\n0.015221\n0.032806\n0.992500\n00:07\n\n\n\n\n\na near 100% accuracy on the validation set üéâ\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\n\n\n\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat‚Äôs it for today! Fastai‚Äôs mid-level API offers much more functionality and we barely scratched the surface. Hope it inspires you to learn further and take advantage of this powerful feature. Head to the docs to learn more. Thanks for reading :)"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#footnotes",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#footnotes",
    "title": "Diving into Fastai‚Äôs mid-level API with MNIST",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(AstroDave 2012)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html",
    "href": "posts/geospatial_intro_part_II/GIS_II.html",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "",
    "text": "Welcome back! In our last post, we took a look at the basics of GIS and how we can explore some of the datasets in QGIS with shapefiles, TIFF files to derive interesting insights around air quality. This post covers loading data from CSV files on disk and also directly from online using server connections."
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html#filtering-a-text-file",
    "href": "posts/geospatial_intro_part_II/GIS_II.html#filtering-a-text-file",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "filtering a text file",
    "text": "filtering a text file\nComing back to our satellite names, we can filter only for the data coming from a single satellite by right clicking on the hms_fire20230306 layer, selecting filter and later entering the query like so:\n\n\n\n\n\n\nFigure¬†7: Filtering a CSV/TXT file\n\n\n\n\n1 - selects the field we want to filter\n2 - gets a few sample values from that field\n3 - double click on the field to enter it into the query box below. Alternatively you can type it as well.\n4, 5 - double click/type\n6 - clicking Test gives us how many rows that satisfy the condition. Hit OK\n7 - ignore (don‚Äôt click it)\n8 - click OK to finalize the filtered data\n\nIf you now look at the later, we only see the datapoints from the satellite we filtered.\n\n\n\n\n\n\nFigure¬†8: Map with filtered entries"
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html#conclusion",
    "href": "posts/geospatial_intro_part_II/GIS_II.html#conclusion",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "conclusion",
    "text": "conclusion\nNow we have seen how to load text files and filtering for our desired data. You can also load Well Known Text (WKT) and other text files in the same manner. Just make sure to select the appropriate radio button while loading them."
  },
  {
    "objectID": "posts/geospatial_intro_part_II/GIS_II.html#conclusion-1",
    "href": "posts/geospatial_intro_part_II/GIS_II.html#conclusion-1",
    "title": "Introduction to GIS with QGIS & Python - Part II",
    "section": "Conclusion",
    "text": "Conclusion\nJust like WFS, you can also add WMS/WCS services much the same way to carry out analysis without downloading data onto your disk."
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "title": "Setting up your blog with Fastpages",
    "section": "",
    "text": "After going through a bit of an intensive setup, I thought of writing up my journey & difficulties I faced while setting up this blog so that it could help others who might start later."
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "title": "Setting up your blog with Fastpages",
    "section": "Motivation",
    "text": "Motivation\nI was looking for ways to have my tiny little place on the internet & therefore scouted for feasible options. As a Data Scientist, I interact with code regularly and (Jupyter) Notebooks are part of our DNA :stuck_out_tongue:. So the criteria that I had set for myself was that whatever platform I choose, it has to be (a) very code friendly, (b) easy to maintain, and (c) relatively affordable (if possible, even free).\n\nQuest for the right tool\nI‚Äôm a big fan of Notion and use it for almost everything in my private life. So I initially considered setting up my blog with Super as you can create websites straight away from Notion pages. Though it looks great, the pricing is something that I‚Äôm still not comfortable with (yet).\nThen there‚Äôs Medium, which provides a nice writing experience but only as long as you don‚Äôt have much code in your content. Though affordable, just like Super, it has mediocre support for code formatting natively & you have to optimize your content well ahead with a lot of GitHub gists. It also has no out-of-the-box support for mathematical symbols/equations. Read more about its shortcomings from a developer perspective in a great post from Prasanth Rao here.Though, I might still consider using this service to post once in a while to increase my outreach. We‚Äôll see how it goes. \n\nThese first two options are great if you don‚Äôt write code-heavy posts (that often) and are also very beginner friendly. But unfortunately, both of them are not free and also don‚Äôt fit well for my use case. Besides, where‚Äôs the fun if you don‚Äôt build stuff by tweaking stuff? :wink:\n\nI then decided to give GitHub Pages a try since it‚Äôs a free static site generator service from GitHub. One need not know (much) about HTML/CSS and can simply write content in human readable Markdown format which gets rendered as a webpage. Besides, you get a nice revision history of your blog as everything would be version controlled with GitHub. In combination with Jekyll (that powers Github pages), there‚Äôre numerous themes & templates to choose from and so much customization that can be made via styles, CSS, etc. I can easily convert Jupyter notebooks into markdown scripts and have them rendered with Jekyll. Since one can display code snippets, markdown cells, and embed 3rd party content within Jupyter notebooks, I intended to go with this as it fulfilled most of my needs‚Ä¶ until I rediscovered Fastpages.\nFastpages, from fast.ai, turbocharges Jupyter notebooks by automating the process of creating blog posts from notebooks via Github Actions. We can write content in notebooks markdown files, or even Word documents and have them rendered as a web page. It offers so much more functionality on-top like code folding, interactive plots on a static web page via embeddings, comments & emoji support :heart_eyes_cat:, etc. For an excellent introduction, please refer to the official documentation.\nThat has convinced me & so now you‚Äôre reading this site built with Fastpages. üéâ"
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "title": "Setting up your blog with Fastpages",
    "section": "Setup",
    "text": "Setup\nFortunately, Fastpages is very well documented and it is highly recommended that you go through that first. However, you might still encounter some problems because of some outdated documentation, and if in case you want to test it locally on Linux systems, which is what I cover here. So, without further ado, let‚Äôs dive in.\n\nThink of a name for your blog. It can just be blog like mine.\nGo through the setup instructions detailed here.\n\nIt might happen that once after you try to generate a copy as per the instructions above, a PR won‚Äôt be automatically generated. The debug instructions in the README are a bit outdated. In this case, go to the Settings -&gt; Actions -&gt; General section of your newly created blog repository and ensure that you have Read and write permissions enabled and the last checkbox is ‚úîÔ∏è like :point_down:. Hit Save. \nGo to the Actions tab and you might see a failed section. Ignore what it is for now and click that failed one. Most likely it‚Äôd be a Setup workflow failure. On the top right, from the drop-down menu Re-run jobs, select Re-run failed jobs.\nOnce the above steps are all ‚úÖ, go to the Pull Requests tab and wait for some time. Your first PR would soon be automatically created. You can also optionally check the progress under the Actions tab if desired.\nNow follow the instructions in the PR and merge it.\n\nCongratulations :confetti_ball:. Your blog would soon be up & running at {your-username}.github.io/{repo-name}. Now you can make changes directly on GitHub online or create notebook/markdown/Word files locally and simply upload them as per the instructions into the right folder. Your blog would be updated just like that (in a few minutes). ü™Ñ\n\n\nTest Locally\nIn most of the cases, you might want to check how your post looks like before you publish to make sure it looks as you intend it to be, especially when it contains data from different sources or in different formats. This is when having the option of testing it locally comes in handy. With Fastpages, you can run your blog locally to see how it would look like so that you can fix any nitty gritty details before pushing it online.\nFastpages again provides a good documentation on how to test it locally with the help of Docker . It has worked fine on my Mac  but installing/upgrading Docker on Linux  is still nowhere as smooth as on Mac and hence I had to go through a bit of digging into forums to make it work on my Ubuntu  machine especially on the latest version 22.04 LTS. So, going forward I‚Äôd cover only this scenario.\n\nFor Mac/Windows, all you need is Docker installed and simply run make server from the root level of the repo.\n\n\nDocker Desktop for Linux (DD4L) is still in beta and is only available for 21.XX versions of Ubuntu. So if you have that, go ahead with the setup below. If not, skip to the next step.\n\nFollow the Docker installation instructions from the official documentation.\nIf you had installed Docker via Snap or you had a previous preview version of Docker Desktop installed, it‚Äôs recommended to uninstall them completely. See more here for help on how to do that.\n\nSince Ubuntu 22.04 LTS is not yet supported, I ended up installing  (not Docker Desktop) from here.\nCheck if you have docker-compose installed by doing which docker-compose or docker-compose -v. If not, install it as a standalone binary.\n\nI‚Äôm not sure if it also installs Make but if it doesn‚Äôt, please install it too following the instructions here.\n\nRun make server from the top level of your repository.\n\nYour Jupyter server would be available shortly at http://localhost:8080/tree and it would take a bit of time for your blog to be available under http://localhost:4000/{your-repo-name}. Be patient. :relieved:\n\n\n\nif make server doesn‚Äôt work because of permission issues, try sudo make server instead\n\n\nCongratulations once again! :tada: You now have a local version of your blog running. You can create new posts and have them rendered almost instantly :sparkles:. Once you‚Äôre happy with the content & format, you can push it (preferably in a new branch and making a PR so that your main/master branch is unaffected). If you feel a bit adventurous, try customizing your blog more to your liking by changing fonts/styles, etc.\nIf you like this blog‚Äôs customization, checkout its  repo esp.¬†its custom-style.scss. May be I‚Äôll write another post detailing it.\nGood Luck & Happy Blogging! ‚ù§Ô∏è"
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html",
    "href": "posts/face-mask-detection/facemask_detection.html",
    "title": "Face mask detection with fastai",
    "section": "",
    "text": "With COVID-19 mutating and still posing a threat globally, wearing a üò∑ is still mandatory in many countries. In this post, we will see how to train a simple computer vision model to detect whether the person is wearing a facemask or not. Let‚Äôs start by downloading the appropriate dataset1 from kaggle.\nYou can either download it manually from kaggle or use its free API. Let‚Äôs do the latter.\nfrom fastai.imports import *\nLet‚Äôs create a directory called data to store and extract our desired dataset.\ndata_dir = Path('./data')\ndata_dir.mkdir(parents=False, exist_ok=True)\npath = Path('.')\npath.ls()\n\n(#3) [Path('mask.jpg'),Path('facemask_detection.ipynb'),Path('data')]"
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html#verifying-results",
    "href": "posts/face-mask-detection/facemask_detection.html#verifying-results",
    "title": "Face mask detection with fastai",
    "section": "verifying results",
    "text": "verifying results\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\n\n\n\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs see what the model is getting wrong or is most unsure about by plotting its top losses!\n\ninterp.plot_top_losses(4, figsize=(8, 8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe only got one incorrect image & the rest are correct but a bit inconfident (and even that‚Äôs low for the 2nd row of images).\n\nImage Shifting\nLet‚Äôs do a bit of fun! Sometimes, it‚Äôs known that some image recognition models predict completely different classes when even a few pixels are changed‚Ä¶ hence let‚Äôs see how robust our model is by rotating and wrapping an image and then letting our model predict.\n\ndef rotate_and_wrap_image(image, percentage=.4):\n    im = tensor(image)\n    val = int(im.shape[1] * percentage)\n    return torch.cat((im[:, val:], im[:, :val]), dim=1)    \n\n\nim.resize((128, 128))\n\n\n\n\n\n\n\n\n\nshow_image(rotate_and_wrap_image(im));\n\n\n\n\n\n\n\n\n\nlearner.predict(rotate_and_wrap_image(im))\n\n\n\n\n\n\n\n\n('WithoutMask', TensorBase(1), TensorBase([3.2003e-07, 1.0000e+00]))\n\n\ninteresting! our model still predicts correct class and this worked on many other tested images as well. This might mean that the model has actually learnt to identify a üò∑ and not ‚Äòremember‚Äô the image itself."
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html#footnotes",
    "href": "posts/face-mask-detection/facemask_detection.html#footnotes",
    "title": "Face mask detection with fastai",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.kaggle.com/datasets/ashishjangra27/face-mask-12k-images-dataset‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "title": "Supercharge your data processing with DuckDB",
    "section": "",
    "text": "Do you have large datasets that you simply can not load into memory to analyse with Pandas? Or do you feel more comfortable expressing operations in SQL instead of python?\nFret not, for you have DuckDB now! ‚ú®ü¶Ü‚ú®"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Setup",
    "text": "Setup\nDuckDB is very lightweight and has no external dependencies and runs within the host process itself. Simply install it with:\npip install duckdb==0.3.4\nTo initialize it, run:\n\nimport duckdb\ndbcon = duckdb.connect()\n\nThat‚Äôs it! Now you can test it by running:\n\ndbcon.execute('select 1, 2, 3').fetchall()\n\n[(1, 2, 3)]\n\n\nNext step is to run pip install pyarrow to add support for reading/writing parquet data.\n\nJupyter Notebook setup\nIf in case you wish to explore it in Jupyter Notebooks, install a few additional libraries for a better experience:\npip install ipython-sql SQLAlchemy duckdb-engine\nImport them once installed:\n\nimport pandas as pd\nimport sqlalchemy\n\n%load_ext sql\n\nSet a few config options to prettify the output and return it as Pandas DataFrame\n\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\nDuckDB is primarily designed to be an in-memory DB. You can however persist your data to disk.\n\n%sql duckdb:///:memory:\n# %sql duckdb:///path/to/file.db\n\n\n#collapse-output\n\n# we can also access the current config & settings of DuckDB by running the following:\n%sql SELECT * FROM duckdb_settings();\n\n\n\n\n\n\n\n\nname\nvalue\ndescription\ninput_type\n\n\n\n\n0\naccess_mode\nautomatic\nAccess mode of the database (AUTOMATIC, READ_O...\nVARCHAR\n\n\n1\ncheckpoint_threshold\n16.7MB\nThe WAL size threshold at which to automatical...\nVARCHAR\n\n\n2\ndebug_checkpoint_abort\nNULL\nDEBUG SETTING: trigger an abort while checkpoi...\nVARCHAR\n\n\n3\ndebug_force_external\nFalse\nDEBUG SETTING: force out-of-core computation f...\nBOOLEAN\n\n\n4\ndebug_many_free_list_blocks\nFalse\nDEBUG SETTING: add additional blocks to the fr...\nBOOLEAN\n\n\n5\ndebug_window_mode\nNULL\nDEBUG SETTING: switch window mode to use\nVARCHAR\n\n\n6\ndefault_collation\n\nThe collation setting used when none is specified\nVARCHAR\n\n\n7\ndefault_order\nasc\nThe order type used when none is specified (AS...\nVARCHAR\n\n\n8\ndefault_null_order\nnulls_first\nNull ordering used when none is specified (NUL...\nVARCHAR\n\n\n9\ndisabled_optimizers\n\nDEBUG SETTING: disable a specific set of optim...\nVARCHAR\n\n\n10\nenable_external_access\nTrue\nAllow the database to access external state (t...\nBOOLEAN\n\n\n11\nenable_object_cache\nFalse\nWhether or not object cache is used to cache e...\nBOOLEAN\n\n\n12\nenable_profiling\nNULL\nEnables profiling, and sets the output format ...\nVARCHAR\n\n\n13\nenable_progress_bar\nFalse\nEnables the progress bar, printing progress to...\nBOOLEAN\n\n\n14\nexplain_output\nphysical_only\nOutput of EXPLAIN statements (ALL, OPTIMIZED_O...\nVARCHAR\n\n\n15\nforce_compression\nNULL\nDEBUG SETTING: forces a specific compression m...\nVARCHAR\n\n\n16\nlog_query_path\nNULL\nSpecifies the path to which queries should be ...\nVARCHAR\n\n\n17\nmax_memory\n26.9GB\nThe maximum memory of the system (e.g. 1GB)\nVARCHAR\n\n\n18\nmemory_limit\n26.9GB\nThe maximum memory of the system (e.g. 1GB)\nVARCHAR\n\n\n19\nnull_order\nnulls_first\nNull ordering used when none is specified (NUL...\nVARCHAR\n\n\n20\nperfect_ht_threshold\n12\nThreshold in bytes for when to use a perfect h...\nBIGINT\n\n\n21\npreserve_identifier_case\nTrue\nWhether or not to preserve the identifier case...\nBOOLEAN\n\n\n22\nprofiler_history_size\nNULL\nSets the profiler history size\nBIGINT\n\n\n23\nprofile_output\n\nThe file to which profile output should be sav...\nVARCHAR\n\n\n24\nprofiling_mode\nNULL\nThe profiling mode (STANDARD or DETAILED)\nVARCHAR\n\n\n25\nprofiling_output\n\nThe file to which profile output should be sav...\nVARCHAR\n\n\n26\nprogress_bar_time\n2000\nSets the time (in milliseconds) how long a que...\nBIGINT\n\n\n27\nschema\n\nSets the default search schema. Equivalent to ...\nVARCHAR\n\n\n28\nsearch_path\n\nSets the default search search path as a comma...\nVARCHAR\n\n\n29\ntemp_directory\n.tmp\nSet the directory to which to write temp files\nVARCHAR\n\n\n30\nthreads\n12\nThe number of total threads used by the system.\nBIGINT\n\n\n31\nwal_autocheckpoint\n16.7MB\nThe WAL size threshold at which to automatical...\nVARCHAR\n\n\n32\nworker_threads\n12\nThe number of total threads used by the system.\nBIGINT\n\n\n33\nbinary_as_string\n\nIn Parquet files, interpret binary data as a s...\nBOOLEAN\n\n\n34\nCalendar\ngregorian\nThe current calendar\nVARCHAR\n\n\n35\nTimeZone\nEurope/Berlin\nThe current time zone\nVARCHAR\n\n\n\n\n\n\n\nFrom now on, you can run SQL directly by prefixing %sql (or %%sql for multiline statements) to your code cell and get your output returned as pandas dataframe :man_dancing:.\n\n%sql select 1 as a;\n\n\n\n\n\n\n\n\na\n\n\n\n\n0\n1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "title": "Supercharge your data processing with DuckDB",
    "section": "DuckDB vs traditional Databases",
    "text": "DuckDB vs traditional Databases\nWith pandas.read_sql command, one can already run SQL queries on an existing DB connection, read tables and load data as pandas DataFrames in memory for processing in python. While this is fine for lightweight operations, it is not optimized for heavy data processing. Traditional RDBMSs such as Postgres, MySQL, etc. process each row sequentially which apart from taking long time to execute, also induce a lot of overhead on CPU. DuckDB on the other hand is built with OLAP in mind and is Column-Vectorized. This helps massively parallelize disk I/O and query executions.\n\nDuckDB uses the Postgres SQL parser under the hood, and offers many of the same SQL features as Postgres 1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Projection & Filter Pushdowns",
    "text": "Projection & Filter Pushdowns\nNow let‚Äôs do a simple filter operation on our dataset. Let‚Äôs count the total number of rows that satisfy the condition TAXI_OUT &gt; 10. We‚Äôll try with both pandas & duckdb.\n\ndf[df['TAXI_OUT'] &gt; 10].shape\n\n(45209245, 28)\n\n\n\n%%sql\n\nselect count(*) as count\nfrom df\nwhere TAXI_OUT &gt; 10\n\n\n\n\n\n\n\n\ncount\n\n\n\n\n0\n45209245\n\n\n\n\n\n\n\nWhile the earlier operation took ~9.5s, the latter just took ~250ms :zap:. There‚Äôs just no comparison.\nThis is because duckdb automatically optimizes the query by selecting only the required column(s) (aka projection pushdown) and then applies the filtering to get a subset of data (aka filter pushdown). Pandas instead reads through all the columns. We can optimize this in pandas by doing these pushdowns ourselves.\n\nprojection_pushdown_df = df[['TAXI_OUT']]\nfilter_pushdown_df = projection_pushdown_df[projection_pushdown_df['TAXI_OUT'] &gt; 10]\nfilter_pushdown_df.shape\n\n(45209245, 1)\n\n\nWe managed to bring this down from several seconds to almost a second. But using duckdb is still about 70-90% faster than this."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Groupby",
    "text": "Using Groupby\nNow let‚Äôs calculate a few aggregates using groupby with projection & filter pushdowns combined.\nHere, we compute a few simple metrics with a certain airline carrier grouped by two origin & destination airports and finally sort the results by the origin airport.\n\nprojection_df = df[['ORIGIN', 'DEST', 'TAXI_OUT', \n                    'AIR_TIME', 'DISTANCE', 'OP_CARRIER']]\norigin_df = projection_df[\n    (projection_df['ORIGIN'].isin(('DCA', 'EWR'))) &\n    (projection_df['DEST'].isin(('DCA', 'EWR'))) &\n    (projection_df['OP_CARRIER'] == 'XE')]\n(origin_df\n     .groupby(['ORIGIN', 'DEST'])\n     .agg(\n         avg_taxi_out=('TAXI_OUT', 'mean'),\n         max_air_time=('AIR_TIME', 'max'),\n         total_distance=('DISTANCE', 'sum'))\n     .sort_index(level=0)\n)\n\n\n\n\n\n\n\n\n\navg_taxi_out\nmax_air_time\ntotal_distance\n\n\nORIGIN\nDEST\n\n\n\n\n\n\n\nDCA\nEWR\n22.116009\n87.0\n828835.0\n\n\nEWR\nDCA\n23.675481\n93.0\n831024.0\n\n\n\n\n\n\n\nWe can make it a bit more concise by using .query for filtering pushdown.\n\n(df\n .query('OP_CARRIER == \"XE\" and ORIGIN in (\"DCA\", \"EWR\") and DEST in (\"DCA\", \"EWR\")')\n .groupby(['ORIGIN', 'DEST'])\n .agg(\n     avg_taxi_out=('TAXI_OUT', 'mean'),\n     max_air_time=('AIR_TIME', 'max'),\n     total_distance=('DISTANCE', 'sum'))\n)\n\n\n\n\n\n\n\n\n\navg_taxi_out\nmax_air_time\ntotal_distance\n\n\nORIGIN\nDEST\n\n\n\n\n\n\n\nDCA\nEWR\n22.116009\n87.0\n828835.0\n\n\nEWR\nDCA\n23.675481\n93.0\n831024.0\n\n\n\n\n\n\n\nThis approach took only about half the time (~3s) compared to our earlier one because¬†.query uses a modified syntax of python and also indexing thus resulting in more efficient evaluation. We can now compare that to our SQL counterpart‚Ä¶\n\n%%sql\n\nselect\n    ORIGIN,\n    DEST,\n    AVG(TAXI_OUT) as avg_taxi_out,\n    MAX(AIR_TIME) as max_air_time,\n    SUM(DISTANCE) as total_distance\n\nfrom df\n\nwhere\n    OP_CARRIER = 'XE' and\n    ORIGIN in ('DCA', 'EWR') and\n    DEST in ('DCA', 'EWR')\n    \ngroup by ORIGIN, DEST\norder by ORIGIN\n\n\n\n\n\n\n\n\nORIGIN\nDEST\navg_taxi_out\nmax_air_time\ntotal_distance\n\n\n\n\n0\nDCA\nEWR\n22.116009\n87.0\n828835.0\n\n\n1\nEWR\nDCA\n23.675481\n93.0\n831024.0\n\n\n\n\n\n\n\nThis ~400ms execution with duckdb above is around an order of magnitude faster and also a lot cleaner, I‚Äôd say. :wink:\nNotice that the data is already loaded under df and hence we don‚Äôt need to read from the source parquet file.\n\nIn the same way, we can also improve the performance of our queries drastically when using joins across multiple tables. I leave this as an exercise to the reader.\n\nBut why actually load data into memory in the first place when we can process it more efficiently with it being just on disk? Often times, the data is too big to load into memory anyways.\nTo do that, we just need to create a VIEW to our data which lets us query the table directly without loading onto memory and update the source from the dataframe df to the newly created view instead.3"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using approximations",
    "text": "Using approximations\nAt times, it suffices just to get an estimate of certain data rather than a precise answer. Using approximations would help us to just that.\n\n%%sql\n\nselect\n    OP_CARRIER,\n    approx_count_distinct(DEST) as approx_num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n\n\n\nOP_CARRIER\napprox_num_unique_destinations\n\n\n\n\n0\n9E\n186\n\n\n1\nAA\n116\n\n\n2\nAS\n77\n\n\n3\nB6\n73\n\n\n4\nCO\n85\n\n\n5\nDL\n171\n\n\n6\nEV\n205\n\n\n7\nF9\n130\n\n\n8\nFL\n75\n\n\n9\nG4\n126\n\n\n\n\n\n\n\n\n%%sql\n\nselect\n    OP_CARRIER,\n    -- takes more time to compute\n    count(distinct DEST) as num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n\n\n\nOP_CARRIER\nnum_unique_destinations\n\n\n\n\n0\n9E\n185\n\n\n1\nAA\n116\n\n\n2\nAS\n77\n\n\n3\nB6\n73\n\n\n4\nCO\n85\n\n\n5\nDL\n170\n\n\n6\nEV\n205\n\n\n7\nF9\n129\n\n\n8\nFL\n75\n\n\n9\nG4\n126\n\n\n\n\n\n\n\nOur approximation query earlier ran about 3-4 times faster than the precise one in this case. This is crucial when responsiveness is more important than precision (esp.¬†for larger datasets)."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Window functions",
    "text": "Using Window functions\nFinally, let‚Äôs wrap our analysis by showing off a bit more of what duckdb can do using some advanced SQL operations.\nWe create two CTEs (Common Table Expressions) to calculate a couple of features. We do filter & projection pushdowns in one CTE and compute our desired features in another. The first feature is a simple demo to showcase if-else support. The second feature is a bit advanced where we find out the last destination a given air carrier has flown to, sorted by flying date. And when it doesn‚Äôt exist, replace it with NA. We then take a sample from the final resultant set.\n\n%%sql\n\nwith limited_data as (\n    select \n        FL_DATE,\n        ORIGIN, \n        DEST, \n        DISTANCE,\n        OP_CARRIER,\n    from airlinedata\n    where FL_DATE &gt;= '2015-01-01'    \n),\n\nlast_destination_data as (\n    select *,\n        case\n            when DISTANCE*1.60934 &gt; 500 then 'yes'\n            else 'no'\n        end as distance_more_than_500_km,\n\n        coalesce(last_value(DEST) over (\n            partition by OP_CARRIER\n            order by FL_DATE\n            rows between unbounded preceding and 1 preceding\n        ), 'NA') as last_destination_flown_with_this_carrier\n\n    from limited_data\n)\n\nselect *\nfrom last_destination_data\nusing sample 10;\n\n\n\n\n\n\n\n\nFL_DATE\nORIGIN\nDEST\nDISTANCE\nOP_CARRIER\ndistance_more_than_500_km\nlast_destination_flown_with_this_carrier\n\n\n\n\n0\n2018-07-10\nDCA\nLGA\n214.0\nYX\nno\nDCA\n\n\n1\n2015-05-08\nDAL\nBWI\n1209.0\nWN\nyes\nBWI\n\n\n2\n2018-03-30\nLAS\nSJC\n386.0\nWN\nyes\nSJC\n\n\n3\n2015-07-10\nBOS\nMSP\n1124.0\nDL\nyes\nDTW\n\n\n4\n2016-06-01\nDTW\nBWI\n409.0\nDL\nyes\nDTW\n\n\n5\n2017-07-26\nGEG\nMSP\n1175.0\nDL\nyes\nSAN\n\n\n6\n2017-01-10\nDFW\nACT\n89.0\nEV\nno\nDFW\n\n\n7\n2015-01-01\nPHX\nDRO\n351.0\nOO\nyes\nBFL\n\n\n8\n2018-05-06\nDFW\nMCO\n985.0\nAA\nyes\nIAH\n\n\n9\n2018-04-30\nMSY\nLAX\n1670.0\nWN\nyes\nLAX\n\n\n\n\n\n\n\nNice, isn‚Äôt it?! The same operation is unimaginably complex (for me, at least) in pandas. ü§Ø\nWith DuckDB, we can combine one or more of many of such complex operations and execute in one go without worrying much about manual optimizations."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#footnotes",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#footnotes",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSQL on Pandas with duckdb‚Ü©Ô∏é\nduckdb on parquet‚Ü©Ô∏é\nThe exact execution times might vary a bit depending on the load & build of your computer. I also noticed that the operations are cached and the first computation takes a bit of time but running it again or after changing the values of the columns in the WHERE clause would only take a couple of ms later on.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/fastai-env-setup/index.html",
    "href": "posts/fastai-env-setup/index.html",
    "title": "fastai env setup",
    "section": "",
    "text": "It‚Äôs been quite a while since I last dabbled myself in deep learning and therefore decided to revisit from the basics. And what better way than to start doing that than learning from fastai? :D In this post, we will see how to quickly setup your dev environment for running notebooks locally to put your hard earned GPUs to use :p\nOf course, you can run your notebooks on cloud with free GPU support on platforms such as Google Colab, Paperspace Gradient or even kaggle notebooks but sometimes, it feels good to run things locally without worrying too much about quotas or network issues etc. If you‚Äôre starting new in this field, it‚Äôs highly recommended to try the above platforms first.\nFirstly, you need mamba. Use it wherever you use conda because it‚Äôs much faster. Once you install it, run the following script:\n\n# create a conda environment\nmamba create -n fastai python=3.10\n\n# install suitable version of `pytorch-cuda` at your time of installation\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n\n# install fastai\nmamba install -c fastchan fastai\n\nmamba install -c conda-forge jupyterlab ipywidgets\nMake sure you can use GPU with pytorch by running this in your python session:\nimport torch\nassert torch.cuda.is_available()\nThat‚Äôs it. Now you can run fastai with GPU support locally simply by doing mamba activate fastai and launching jupyter ! üíö"
  },
  {
    "objectID": "posts/geospatial_intro_part_I/GIS_I.html",
    "href": "posts/geospatial_intro_part_I/GIS_I.html",
    "title": "Introduction to GIS with QGIS & Python - Part I",
    "section": "",
    "text": "As a data scientist, adding data analysis of geospatial information systems (GIS) to our skill set is a smart move in today‚Äôs data-driven world. The availability of free immense satellite and map data online, combined with the power of open source GIS tools, presents enormous opportunities for analyzing and visualizing geospatial data. With GIS, data scientists can enhance their data analytics and machine learning abilities, resulting in a more comprehensive understanding of complex problems such as climate change.\nBy leveraging GIS, we can monitor and track the effects of climate change on the planet by analyzing data from a wide range of sources, such as temperature sensors, satellite imagery, and ocean currents, to provide a better understanding of its impact on our environment. This information can then be used to inform decision-making processes, such as predicting sea level rise and assessing the impact on coastal cities etc.\nMoreover, this also empowers common people by allowing them to answer questions about their own environment and surroundings. For example, farmers can use GIS to monitor crop health, water availability, and soil quality, while city dwellers can use it to explore the impact of urbanization on the environment. Anyone can access these tools to perform basic analysis, enabling them to become citizen scientists and contribute to the health of our planet.\nIn a series of posts, we will try to explore the basics of GIS, and progress towards addressing some interesting questions through the application of QGIS, Python and data visualization. Although I am also new to this area and currently learning, I invite you to join me on this excursion of discovery.\nTogether, we shall learn, experiment and explore the potential of GIS to transform data analytics by combining it with geospatial information.\nBy the end of this session, you‚Äôll be able to do this:\nGIS, or Geographic Information System, is a tool for mapping and analyzing different types of data related to a specific location on üåç. It allows you to visualize data on a map, such as population density, land use, or weather patterns. By combining data from various sources, we can uncover patterns, relationships, and derive insights that may not be apparent from individual datasets alone. It can be used to answer questions such as: Where are the most vulnerable areas to flooding? How has urbanization changed over time? And, where should we build a new school to ensure accessibility to the largest number of students? etc.\nQGIS is an open source tool to explore this layered GIS. Download it from qgis.org and install it. You may be greeted with the following window. You can create & save the project by clicking Project -&gt; Save As on the top left menu of the application.\nThe UI is mainly divided into the following areas:\nQGIS has much, much more things to offer but these four are good enough to start with.\nThe first thing we do is get some basemaps.\nFor this, download the qgis_basemaps.py (courtesy of Asst. Prof.¬†Qiusheng Wu) and open the python console like so:\nPaste the downloaded script and hit run (green ‚ñ∂Ô∏è). You see all basemaps loaded under XYZ Tiles in browser.\nIn order to view a basemap, simply drag & drop any of them into the Canvas. You will notice that the Layers widget starts getting populated. Any subsequent basemaps that you drop to the canvas will get stacked here. In general, we need one basemap layer and one or more data layers for analysis. Which basemap to choose depends on the analysis you‚Äôre carrying.\nAs our initial use case, let‚Äôs examine the 2020 European Air Quality dataset (head here, hit Direct Download). This dataset provides concentrations for the air pollutants \\(NO_2\\) at 1 km grid.\nIt contains a .tif file along with other metadata.\nTo visualize the tif file (no2_avg_20.tif), simply drag & drop to the canvas. Here‚Äôs what it looks like (after some styling). As you can see, it‚Äôs beautiful & colorful but without context. That‚Äôs where our basemap comes in, as you might have expected. You can settle on any that‚Äôs appropriate for our use case here. I liked Esri National Geographic for this as it displays the borders of the countries more clearly. And remember, basemaps always come at the bottom. So make sure you reorder them in the Layers widget accordingly.\nThe default .tif file shows a single band grayscale image. A band is like a channel, much like RGB of a color image. But that looks dull though it has the potential to show much more visual information. We can convert those values into quantiles and visualize those instead. For that, we will now turn our attention to the styling section (shown below).\nOnce you click it, a new tab opens to the right in place of the processing toolbox area from the figure 3 above.\nMake sure that the .tif file is selected and do the following to enhance the visual information (Fig-6(b)):\nHere, we just binned the values of this grayscale image and assigned a color to each bin.\nIn a modern industrialized world, a good portion of air pollution is caused by human settlements. To find its effects, we can check where are its majot sites. Headover here and download populated places, urban areas, airports, ports datasets and unzip them. These contain data at 1:10 (i.e., 1cm = 100km) scale.\nDrag & drop the ne_10m_urban_areas shapefile layer onto canvas. This shows areas of dense human habitation.\nYou can customize the styling based on your choices and make sure it doesn‚Äôt override any of the data shown from the underlying layers. Here‚Äôre mine.\nNot surprisingly, most parts of urban settlement area is under the region with worse air quality (dark red region)\nLet‚Äôs now also add the populated places data layer but this time from the commandline with the help of python.\nTo do that, let‚Äôs open the python console from the top menu (plugins -&gt; python console or hit Ctrl+Alt+P). Qgis already makes an instance of its interface available under the variable iface.\nTo add this vector layer to the canvas, simply run\nYou can notice that the layer is then added with the name populated_places. Headover to its styling to choose how those individual data points are represented.\nAdding the other layers (i.e., ne_10m_ports & ne_10m_airports) similarly would give us our final result.\nThere‚Äôs so much information to unpack here (open in new tab for higher resolution) that I leave it as an exercise to the reader to derive their own insights.\nI hope this has helped you kickstart your journey into GIS analysis and understand our world a bit better. In the next post, we will see how to handle even more types of data, perform a complex processing pipeline and more.\nBis dann üëã"
  },
  {
    "objectID": "posts/geospatial_intro_part_I/GIS_I.html#footnotes",
    "href": "posts/geospatial_intro_part_I/GIS_I.html#footnotes",
    "title": "Introduction to GIS with QGIS & Python - Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2a and 2b‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/migrate-to-quarto/index.html",
    "href": "posts/migrate-to-quarto/index.html",
    "title": "Migrating from Fastpages to Quarto",
    "section": "",
    "text": "Fastpages, based on which my original blog was setup has been archived and they now recommend using Quarto instead. If you‚Äôre starting new, the latter is the recommended way to go but if you too have a fastpages blog setup initially and want to migrate, there‚Äôs a migration guide available. It‚Äôs not completely perfect as you have to tweak a few bits here & there before you see all the renderings correctly (because of slight syntactic variations amongst other things).\nHowever, I felt quarto to be much more intuitive and easy to setup. I also did the migration but found manually moving posts from my old blog repo to the new quarto repo to be a bit easier. (Migration worked but I was not happy with the directories it created as part of it‚Ä¶ I found it aesthetically less pleasing and hence moved them myself).\nFor simple blogging, fastpages offered more than enough features and quarto offers even more on top of that. It‚Äôs easy to get started with quarto. Head over to the start guide to learn more.\nI do miss the advanced styling options I setup in fastpages though‚Ä¶ will have to dig into Quarto to see how much I can reuse. Until then, have fun with my frugalistic looking blog! :D"
  },
  {
    "objectID": "posts/quick-pytorch-setup-with-pixi-Nvidia/index.html",
    "href": "posts/quick-pytorch-setup-with-pixi-Nvidia/index.html",
    "title": "Simple & Fast Local DL Setup with PyTorch, Pixi & Nvidia",
    "section": "",
    "text": "I recently distro-hopped and this time settled onto  BlueFin, from Universal Blue. I had to setup my local deep learning environment again and this provided a nice opportunity to test a new setup afresh.\nI had previously setup fastai with mamba but this time I wanted to test pixi. It‚Äôs a lot faster and overall, a better alternative to mamba/conda.\nInstalling deep learning libraries locally is always a daunting task, dealing with system level dependencies and potentially corrupting them being one of the main reasons. Let‚Äôs see how easy and safe it is with pixi.\nFirst, make sure you have NVIDIA drivers setup correctly matching your system. You can check that out by running nvidia-smi on your cmdline which gives an output something similar to this.\n$ nvidia-smi\n\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 575.64.05              Driver Version: 575.64.05      CUDA Version: 12.9     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4060 ...    Off |   00000000:01:00.0 Off |                  N/A |\n| N/A   47C    P0             15W /   75W |      12MiB /   8188MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A            3550      G   /usr/bin/gnome-shell                      2MiB |\n+-----------------------------------------------------------------------------------------+\nAs you can see, I have nvidia driver v575 with CUDA v12.9. As of this writing, the latest CUDA version which pytorch supports is 12.8. So, we‚Äôre good to go.\nNow go ahead and install pixi by your preferred method and later run pixi init dl_setup to create a folder named dl_setup and initialize it. This creates a default pixi.toml configuration file which is similar to pyproject.toml but better.\nThe first section workspace of the TOML file should look like this:\n[workspace]\nchannels = [\"conda-forge\"]\nname = \"dl_setup\"\nplatforms = [\"linux-64\"]\nversion = \"0.1.0\"\n\n\n\n\n\n\nNote\n\n\n\nIf in case, you already use pyproject.toml or you prefer to have that instead, just pass the flag --format=pyproject to the init cmd earlier. You also need to do some extra steps if in case you go this way. (E.g., by prefixing each section with tool.pixi and including build-system section. Refer to pixi‚Äôs documentation)\n\n\nFirst run pixi add \"python&gt;=3.12\" to add python itself as a dependency.\nPixi supports installing PyPI dependencies alongside Conda packages, and you can typically run pixi add &lt;pkg&gt; --pypi to install a PyPi package. For example, PyTorch‚Äîwhich is now officially available only via PyPi‚Äîcould be installed this way. However, currently, it is not possible to specify a custom index-url (the URL from which to download wheels) via the command line. Therefore, you need to manually edit the pixi.toml file to set the appropriate index URL.\nBefore that, quickly note the latest versions (minus the patch versions) of torch, torchaudio & torchvision from PyPi so that we can manually add them in the config. Also, since this setup involves system dependencies (e.g., CUDA), we need to specify that as well so that pixi can take advantage of that during dependency resolution.\n[system-requirements]                                         \ncuda = \"12.0\"    # just major version suffices\n\n[feature.gpu.pypi-dependencies]\ntorch = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchaudio = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchvision = { version = \"&gt;=0.22.0\", index = \"https://download.pytorch.org/whl/cu128\" }\nWe have added a feature section named gpu and since these are PyPi dependencies, you notice that appended at the last. Pixi already defines a default environment named default. We just need to include this feature to that environment before creating it by running the following on the cmdline:\n$ pixi workspace environment add default --feature=gpu --force \nIt creates the following section in the toml file which makes sure that the gpu dependencies are included in that environment when run.\n[environments]\ndefault = [\"gpu\"]\nWe pass the --force flag so as to update the default environment which already exists.\nLet‚Äôs install Jupyter as well so that we can explore interactively. Since pixi handles conda and pip dependencies, we can safely run pixi add jupyterlab that fetches this from conda-forge channel by default.\nFinally, if we want to run a jupyter notebook session with a shortcut, we can add a task by running:\n$ pixi task add jupyter \"jupyter lab\"\n\nThe whole pixi.toml might look something like this:\n[workspace]\nchannels = [\"conda-forge\"]\nname = \"dl_setup\"\nplatforms = [\"linux-64\"]\nversion = \"0.1.0\"\n\n[system-requirements]\ncuda = \"12.0\"\n\n[dependencies]\npython = \"&gt;=3.12\"\njupyterlab = \"&gt;=4.4.5,&lt;5\"\nnumpy = \"&gt;=2.3.2,&lt;3\"\npandas = \"&gt;=2.3.1,&lt;3\"\nseaborn = \"&gt;=0.13.2,&lt;0.14\"\n\n[feature.gpu.pypi-dependencies]\ntorch = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchaudio = { version = \"&gt;=2.7.0\", index = \"https://download.pytorch.org/whl/cu128\" }\ntorchvision = { version = \"&gt;=0.22.0\", index = \"https://download.pytorch.org/whl/cu128\" }\n\n[environments]\ndefault = [\"gpu\"]\n\n[tasks]\njupyter = \"jupyter lab\"\n\nThe versions might differ for you depending on the platform or when you run this.\nWe can now enjoy a full fledged jupyter lab session by simply running pixi run jupyter at the commandline.\nThat‚Äôs it! Happy coding!!! ‚ö°Ô∏è‚ú®"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html",
    "title": "Mastering RAG Pipelines",
    "section": "",
    "text": "Ever wished your search engine could read your mind? In this post, we‚Äôll dive into how Retrieval-Augmented Generation (RAG) can come close ‚Äî by expanding user queries into hypothetical documents, refining them through query rewriting, and fusing results intelligently using Reciprocal Rank Fusion (RRF). Together, these techniques transform ordinary retrieval into a contextual, knowledge-aware reasoning process.\nPS: This post is inspired by these two articles: RAG Hack, LLM Based Query Rewriting"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#what-is-rag",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#what-is-rag",
    "title": "Mastering RAG Pipelines",
    "section": "What is RAG?",
    "text": "What is RAG?\nRetrieval-Augmented Generation (RAG) is an architecture that combines two powerful components: retrieval and generation. Instead of relying solely on what a language model ‚Äúremembers,‚Äù RAG retrieves relevant documents from an external knowledge base and uses them to generate more accurate, up-to-date, and context-aware responses.\nIt‚Äôs particularly useful when dealing with dynamic or domain-specific information ‚Äî for example, enterprise knowledge bases, customer support documentation, or academic research. By grounding responses in retrieved facts, RAG reduces hallucinations and enhances factual accuracy, making it an ideal solution for any use case where the underlying data changes frequently or is too large to fit into a model‚Äôs internal memory.\n\nLimitations of traditional RAG based systems\nWhile RAG sounds elegant in theory, traditional implementations often struggle in practice. The retrieval step depends heavily on keyword or embedding similarity, which can miss relevant documents when the query is vague, abstract, or phrased differently from the source text. This leads to poor recall ‚Äî the model simply doesn‚Äôt find the right context to reason from.\nAdditionally, many RAG systems fail under semantic drift, long-tail queries, or domain-specific jargon. Even when relevant information exists, it may rank too low or be drowned out by irrelevant results. As a result, the generation step produces confident but inaccurate answers ‚Äî defeating the very purpose of retrieval augmentation.\nEnter üî•HyDEüî•"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#hypothetical-document-embeddings",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#hypothetical-document-embeddings",
    "title": "Mastering RAG Pipelines",
    "section": "Hypothetical Document Embeddings",
    "text": "Hypothetical Document Embeddings\nHypothetical Document Embeddings (HyDE) is a retrieval technique that addresses a key weakness of traditional RAG systems‚Äîpoor performance on vague or under-specified queries. Instead of retrieving directly from the raw query, HyDE first uses a language model to generate a short, plausible ‚Äúhypothetical document‚Äù answering the query. This synthetic passage is embedded and used to retrieve semantically similar real documents from the knowledge base.\nBy transforming queries into document-like representations, HyDE aligns retrieval with document embedding space, allowing contrastive encoders (such as Contriever) to match these hypotheticals to relevant corpus entries. This approach yields strong zero-shot retrieval, especially for open-ended, exploratory, or low-context queries where users may lack precise keywords."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#query-generationrewriting",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#query-generationrewriting",
    "title": "Mastering RAG Pipelines",
    "section": "Query Generation/ReWriting",
    "text": "Query Generation/ReWriting\nBefore applying HyDE, it‚Äôs often useful to rewrite user queries to make them clearer and more retrieval-friendly. In real-world scenarios, users tend to ask compound or referential questions such as What is XXX and how does it impact sales?. Instead of treating this as one query, we can split it into multiple focused sub-queries ‚Äî for example: ['What is XXX?', 'How does XXX impact sales?'].\nDuring this rewriting step, an LLM can also resolve demonstrative pronouns (like this, that, it, or they) in follow-up questions, replacing them with explicit references from earlier parts of the query. This process ensures that each rewritten query is self-contained, semantically clear, and ready for more effective retrieval downstream.\nThis stage is called query generation or query rewriting, and it acts as a smart preprocessing layer before the HyDE step. By running HyDE on each rewritten sub-query, we get richer hypothetical contexts and significantly better retrieval accuracy for complex, multi-part user inputs."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#reciprocal-rank-fusion",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#reciprocal-rank-fusion",
    "title": "Mastering RAG Pipelines",
    "section": "Reciprocal Rank Fusion",
    "text": "Reciprocal Rank Fusion\nEven after applying Query Rewriting and HyDE, retrieval isn‚Äôt always perfect. Each rewritten or hypothetical query might surface slightly different sets of relevant documents ‚Äî some overlap, some don‚Äôt. For example, one version of the query might capture documents with broader context, while another highlights highly specific details. Choosing just one retrieval run risks losing valuable information.\nIn real-world RAG systems, no single retrieval method or query formulation is universally optimal. Dense embeddings, keyword search, and HyDE-generated queries each offer unique perspectives on relevance. What we need is a way to combine their strengths ‚Äî to merge the best results from multiple retrieval strategies into a unified, high-quality ranking.\nThat‚Äôs where Reciprocal Rank Fusion (RRF) comes in.\nIt is a simple yet powerful ranking aggregation technique designed to fuse multiple retrieval results into one coherent ranked list. Instead of trusting one retrieval run, RRF looks across all of them, giving higher priority to documents that appear near the top across lists.\nMathematicaly, it sums the reciprocals of rank positions, rewarding items consistently ranked highly. This makes RRF especially useful in hybrid or multi-source retrieval systems‚Äîsuch as combining embeddings, keyword searches, or reformulations‚Äîwhere it boosts recall, stability, and robustness for more reliable results."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#powerful-trio",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#powerful-trio",
    "title": "Mastering RAG Pipelines",
    "section": "Powerful Trio",
    "text": "Powerful Trio\nWhen used together, Query Rewriting, HyDE, and RRF form a highly effective multi-stage RAG pipeline.\n\nQuery Rewriting ensures that each user query (or sub-query) is explicit and context-independent.\nHyDE then generates a hypothetical answer for each rewritten query, enriching the semantic representation used for retrieval.\nFinally, RRF fuses the retrieval results from all rewritten and HyDE-generated queries into a unified, ranked list of documents.\n\nThis combination addresses several real-world challenges:\n\nAmbiguity and reference resolution ‚Üí handled by Query Rewriting.\nSparse or underspecified queries ‚Üí improved via HyDE‚Äôs hypothetical document generation.\nRetrieval diversity and ranking stability ‚Üí enhanced by RRF‚Äôs fusion mechanism.\n\nIn essence, RRF acts as the final consensus layer, aggregating the best retrieval signals from each query variant. Together, these steps make RAG systems far more resilient, accurate, and context-aware, especially in enterprise or multi-domain settings where queries vary in form and complexity."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#choosing-right-models",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#choosing-right-models",
    "title": "Mastering RAG Pipelines",
    "section": "Choosing Right Models",
    "text": "Choosing Right Models\nEven with a powerful retrieval pipeline built on Query Rewriting, HyDE, and RRF, the system‚Äôs overall performance still depends heavily on the embedding model used for retrieval and the LLM used for generation. These are the foundational layers that determine how well our RAG system understands, retrieves, and communicates information.\nThe embedding model is responsible for mapping both queries and documents into a shared semantic space. If this model isn‚Äôt well-aligned with our domain or language, even the best retrieval strategies will struggle ‚Äî relevant documents might sit far apart in the vector space, leading to poor recall. For example, an English-only embedding model will fail when users query in Spanish or Hindi, or when the document corpus is multilingual. Choosing a multilingual or domain-tuned embedding model ensures that semantically similar ideas are close together, regardless of language or phrasing.\nOn the generation side, the LLM plays an equally critical role. It needs to interpret the retrieved context correctly, synthesize information fluently, and handle domain-specific or multilingual outputs gracefully. If our users interact in multiple languages, our generation model should either be natively multilingual or supported by a translation layer that preserves meaning without distorting the original intent.\n\nNow that we‚Äôre armed with this knowledge, let‚Äôs dive into code. The repo is available on github . Feel free to clone it and run this notebook. It is designed primarily to run on Linux  with NVIDIA GPU or on Mac  with Silicon processors with integrated GPU.\n\n\nCode\nimport os\nos.chdir('..')\n\nimport sys\nimport faiss\nimport yaml\nimport textwrap\n\nfrom time import time\nfrom glob import glob\nfrom collections import defaultdict\nfrom langchain_text_splitters import MarkdownTextSplitter\n\nfrom IPython.display import Markdown\n\nfrom src.utils import build_corpus, empty_cache\nfrom src.main import load_models, generate_text, make_query_variants, transform_query, aggregate_queries_and_tasks"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#building-a-corpus-extracting-content-from-pdfs",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#building-a-corpus-extracting-content-from-pdfs",
    "title": "Mastering RAG Pipelines",
    "section": "üìö Building a Corpus: Extracting Content from PDFs",
    "text": "üìö Building a Corpus: Extracting Content from PDFs\nTo test our RAG pipeline, we first need a knowledge corpus built from one or more PDFs. Extracting content from PDFs isn‚Äôt always straightforward ‚Äî layouts, tables, and images can make parsing tricky. Two great tools for this are PyMuPDF (or PyMuPDF4LLM) and Docling, each suited for different needs.\n\n‚ö° PyMuPDF / PyMuPDF4LLM\nPyMuPDF is a fast, lightweight library for PDF text extraction ‚Äî perfect for CPU environments and large batches of text-heavy documents. Its LLM-optimized variant, PyMuPDF4LLM, adds support for exporting structured Markdown, preserving headings, lists, and tables for easier downstream use.\nPros:\n\nFast, efficient, and CPU-friendly\nHandles simple-to-moderate layouts well\nMarkdown export with PyMuPDF4LLM fits LLM pipelines\n\nCons:\n\nLimited accuracy for complex layouts or multi-column designs\n\nBest for: Quick, scalable extraction of mostly text-based PDFs.\n\n\nüß† Docling\nDocling is a deep-learning‚Äìpowered PDF parser that excels at handling complex document structures ‚Äî tables, figures, multi-column text, etc. It can export to plain text or Markdown, producing highly structured, clean outputs. However, it‚Äôs heavier and slower, typically requiring GPU support for optimal performance.\nPros:\n\nHigh-fidelity extraction of structured layouts\nFlexible text/Markdown outputs\n\nCons:\n\nSlower and GPU-intensive\n\nBest for: Complex or visually rich PDFs when GPU resources are available.\n\n\nüìù Why Extract in Markdown?\nExtracting in Markdown strikes the right balance between structure and simplicity. It retains document hierarchy ‚Äî like headings, lists, and especially tables ‚Äî that plain text loses, helping LLMs and embedding models better understand context and meaning. Markdown also makes document chunking more coherent, improving retrieval and generation quality downstream.\nLet‚Äôs take the docling technical report as our PDF to build our corpus. But remember, we can use more than one too.\n\n\n\n\n\n\nNote\n\n\n\nWhen run from CLI, we can use gradio where we can directly drop the folder or choose PDF(s) to process.\n\n\nSince our documents are extracted in Markdown rather than plain text, we need a text splitter that can respect Markdown structure. For this, we use the MarkdownTextSplitter from LangChain‚Äôs text splitters module. It allows us to define a chunk_size to control segment length, along with a small overlap to maintain context between chunks.\nIt‚Äôs important to note that with Markdown, the chunk_size serves as an approximation ‚Äî the splitter avoids cutting through sections or formatting elements to preserve the logical and structural continuity of the text.\n\ntext_splitter = MarkdownTextSplitter(chunk_size=3000, chunk_overlap=450)\n\n\npdfs = glob(\"notebooks/data/*.pdf\")\npdfs\n\n['notebooks/data/docling_technical_report.pdf']\n\n\n\nds = build_corpus([pdfs[0]], text_splitter, fast_extract=False)\nds\n\n2025-10-24 21:29:31,859 - INFO - detected formats: [&lt;InputFormat.PDF: 'pdf'&gt;]\n2025-10-24 21:29:31,897 - INFO - Going to convert document batch...\n2025-10-24 21:29:31,897 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n2025-10-24 21:29:31,904 - INFO - Loading plugin 'docling_defaults'\n2025-10-24 21:29:31,905 - INFO - Registered picture descriptions: ['vlm', 'api']\n2025-10-24 21:29:31,911 - INFO - Loading plugin 'docling_defaults'\n2025-10-24 21:29:31,913 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n2025-10-24 21:29:31,976 - INFO - Accelerator device: 'cuda:0'\n2025-10-24 21:29:33,349 - INFO - Accelerator device: 'cuda:0'\n2025-10-24 21:29:34,413 - INFO - Accelerator device: 'cuda:0'\n2025-10-24 21:29:34,770 - INFO - Processing document docling_technical_report.pdf\n2025-10-24 21:29:41,917 - INFO - Finished converting document docling_technical_report.pdf in 10.06 sec.\n\n\nDataset({\n    features: ['id', 'file', 'chunk_id', 'chunk'],\n    num_rows: 14\n})\n\n\nWe loaded our corpus in HuggingFace‚Äôs dataset format containing the following columns:\n\nid - unique ID across documents\nfile - filename\nchunk_id - ID of the chunk within a given document\nchunk - raw text extracted as markdown\n\nWe need to preserve this metadata which would later help in fetching the appopriate chunks after RRF.\n\nlist(map(len, ds[\"chunk\"][:]))\n\n[2746,\n 775,\n 269,\n 2579,\n 2696,\n 2436,\n 2948,\n 2885,\n 331,\n 2993,\n 1783,\n 2552,\n 2648,\n 2332]\n\n\nAs noted earlier, the data is chunked unevenly. We will also empty the cache to offload docling from GPU as we no longer need it by running:\n\nempty_cache()"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#load-models",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#load-models",
    "title": "Mastering RAG Pipelines",
    "section": "Load Models",
    "text": "Load Models\n\nEmbedding Model\nFor our pipeline, we need a lightweight yet high-quality embedding model ‚Äî one that can generate strong dense vector representations while being efficient enough for local or production use. If our use case mandates it, ideally, the model should be multilingual, capable of producing similar embeddings for semantically similar text across different languages. This is crucial because users might upload documents in one language and ask questions in another, or even provide multilingual documents within the same corpus.\nAfter experimenting with several variants, I chose Qwen3-Embedding, which strikes a great balance between performance, efficiency, and multilingual capability. For local deployment, I‚Äôve settled on the Qwen3-Embedding-0.6B model ‚Äî with a maximum embedding dimension of 1024. It performs exceptionally well for most workloads, though you can always switch to a larger variant (e.g., via the Hugging Face Spaces) if you need more capacity or precision.\n\n\nText Generation Model\nFor the text generation step, I opted for Qwen3-4B-AWQ due to its strong reasoning abilities, instruction-following skills, agent capabilities, and multilingual support. It also offers a generous context window of 32,768 tokens, which is ideal for handling long documents or multi-section queries. AWQ (Attention Aware Quantization) preserves a small fraction of the weights that are important for LLM performance to compress a model to 4-bits with minimal performance degradation. Therefore we can be able to load huge models on limited GPU RAM that are otherwise impossible.\nSmaller variants proved less effective: the 0.6B model was too weak to generate meaningful answers, and the 1.7B model was only partially reliable. The 4B version, on the other hand, though not perfect, consistently produced accurate results across most queries.\nA key factor contributing to this performance was the Markdown-format PDF extraction, especially when done via Docling, which preserved document structure and context, allowing the model to reason over the content more effectively.\nWe‚Äôll define a few model combinations to use depending on host platform‚Äôs capabilities.\n\nMODEL_COMBOS = {\n    \"linux\": {\n        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n        \"gen_model\": \"Qwen/Qwen3-4B-AWQ\",\n    },\n    # feel free to replace with any ??B-MLX-?bit versions from Qwen3 Collection at:\n    # https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f\n    \"mac\": {\n        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n        \"gen_model\": \"Qwen/Qwen3-4B-MLX-4bit\",\n    },\n    \"mac_mid\": {\n        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n        \"gen_model\": \"Qwen/Qwen3-4B-MLX-6bit\",\n    },\n    \"mac_high\": {\n        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n        \"gen_model\": \"Qwen/Qwen3-4B-MLX-8bit\",\n    },\n    # models to load on HF Spaces \n    # HF-low is same as `linux-local`\n    \"HF-mid\": {\n        \"embed_model\": \"Qwen/Qwen3-Embedding-0.6B\",\n        \"gen_model\": \"Qwen/Qwen3-8B-AWQ\",\n    },\n    \"HF-high\": {\n        \"embed_model\": \"Qwen/Qwen3-Embedding-4B\",\n        \"gen_model\": \"Qwen/Qwen3-14B-AWQ\",\n    },\n}\n\nNow, we load the embedding & generative models based on one of the combination keys above.\n\ncombo = \"linux\" if sys.platform == \"linux\" else \"mac\"\nGEN_MODEL_NAME = MODEL_COMBOS[combo][\"gen_model\"]\nembedder, tok, gen = load_models(\n    embed_model_name=MODEL_COMBOS[combo][\"embed_model\"],\n    gen_model_name=MODEL_COMBOS[combo][\"gen_model\"],\n)"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#embeddings-for-corpus",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#embeddings-for-corpus",
    "title": "Mastering RAG Pipelines",
    "section": "Embeddings for corpus",
    "text": "Embeddings for corpus\n\ndef get_emb(texts, **kwargs):\n    return embedder.encode(texts, normalize_embeddings=True, **kwargs)\n\nWe can now add embeddings directly as a column to the dataset\n\nbatch_size = 8  # change it based on your system's capabilities\nds = ds.map(\n    lambda x: {\n        \"embeddings\": get_emb(\n            x[\"chunk\"],\n            batch_size=batch_size,\n            prompt_name=\"query\",\n            show_progress_bar=True,\n        )\n    },\n    batched=True,\n    batch_size=batch_size,\n)\n\n\n\n\n\n\n\n\n\n\n\nempty_cache()\n\n\n# ds.save_to_disk('temp_dataset');\n# ds = load_from_disk('temp_dataset/')"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#build-index",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#build-index",
    "title": "Mastering RAG Pipelines",
    "section": "Build Index",
    "text": "Build Index\nNow that we have the input data setup, it‚Äôs time to index them. We shall use FAISS as it‚Äôs efficient & fast (esp.¬†if you have a GPU). Since we‚Äôre using datasets, adding a faiss index over a column is pretty simple.\n\nds.add_faiss_index(\n    \"embeddings\", metric_type=faiss.METRIC_INNER_PRODUCT\n);  # , device=0 if (torch.cuda.is_available() or torch.mps.is_available()) else None)"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#generate-queries",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#generate-queries",
    "title": "Mastering RAG Pipelines",
    "section": "Generate queries",
    "text": "Generate queries\nNow that we have the data and the models setup, time to process user queries. Let‚Äôs define a function that handles that.\n\nqueries = [\n    \"tell me about cloud bursts\",\n    \"how to ride a unicorn\",\n]\n\n\nresp = generate_text(\n    tok, gen, queries, model_name=GEN_MODEL_NAME,\n    system_prompt=\"You are a funny standup comedian and reply only in one liner jokes\",\n)\n\n\nresp[0]\n\n'A cloud burst is like a thunderstorm that‚Äôs so intense, it could make a cloud cry.'\n\n\n\nresp[1]\n\n\"Just follow the rainbow and pretend you're not scared of the hooves.\"\n\n\nGreat! Notice that we can give multiple queries in one go which comes handy during batch processing."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#prompts",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#prompts",
    "title": "Mastering RAG Pipelines",
    "section": "Prompts",
    "text": "Prompts\nBecause our model understands system and user prompts, we can set the context/role depending on what we want. Let‚Äôs load them.\n\nwith open(\"src/prompts.yaml\") as fl:\n    prompts = yaml.safe_load(fl)\n\nFirst we define a function to make different variants of the user query. We provide a few examples in the prompt."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#query-variations",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#query-variations",
    "title": "Mastering RAG Pipelines",
    "section": "Query Variations",
    "text": "Query Variations\n\nquery = \"evolution of solar energy costs in Europe\"\nq_variants = make_query_variants(\n    tok, gen, query, prompts[\"variants\"], n=2, model_name=GEN_MODEL_NAME\n)\nq_variants\n\n['evolution of solar energy costs in europe',\n 'how has the cost of solar energy changed in different european countries?',\n 'how have solar energy prices changed in europe over the years?',\n 'what factors have influenced the evolution of solar energy costs in europe?',\n 'what has been the trend in solar energy costs across europe?',\n 'what is the historical development of solar energy costs in europe?']\n\n\nNice! LLMs work best by role playing & examples and therefore we have given some through system prompt. Look at the variants section of the prompts.yaml file to learn more.\nwe always preserve the original query just in case the model loses (or deviates too far from) the original intent in its generated variants."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#query-generationrewrite",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#query-generationrewrite",
    "title": "Mastering RAG Pipelines",
    "section": "Query Generation/Rewrite",
    "text": "Query Generation/Rewrite\nWe now transform the query into a json with two parts search & tasks. Search contains one or more queries based on the original query that are transformed & expanded (if applicable) to be made within the context provided. Tasks lists one or more actions to take after the search is performed. Sometimes there‚Äôd be overlap in between them but that‚Äôll be taken care during preprocessing.\n\nquery = \"draft an email on summary of sales in the last quarter\"\ntransform_query(tok, gen, query, prompts[\"rewrite\"], model_name=GEN_MODEL_NAME)\n\n{'search': ['summary of sales in the last quarter',\n  'sales summary for the previous quarter'],\n 'tasks': ['draft an email based on the sales summary',\n  'send the drafted email to the relevant recipient']}\n\n\nNow just like we derived many variations of the original user query earlier, we shall do the same for each of the above search queries as well. We append the tasks at the last of the user prompt."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#final-aggregation-of-queries-tasks",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#final-aggregation-of-queries-tasks",
    "title": "Mastering RAG Pipelines",
    "section": "final aggregation of queries & tasks",
    "text": "final aggregation of queries & tasks\n\nqueries, tasks = aggregate_queries_and_tasks(\n    tok, gen, query, prompts[\"rewrite\"], prompts[\"variants\"], \n    gen_model_name=GEN_MODEL_NAME,\n)\n\n\nqueries\n\n['draft an email on summary of sales in the last quarter',\n 'how did sales performance compare to the previous quarter',\n 'what is the sales overview for the quarter that just ended',\n 'your name']\n\n\n\ntasks\n\n['draft an email', 'send the email to the relevant team']\n\n\n\n\n\n\n\n\nTip\n\n\n\nBecause we used a multilingual language model, this should also work for other languages\n\n\n\nquery = \"Was ist die Geschichte der Autos in Deutschland?\"\nqueries, tasks = aggregate_queries_and_tasks(\n    tok, gen, query, prompts[\"rewrite\"], prompts[\"variants\"], \n    temperature=0.7, n_variations=2, gen_model_name=GEN_MODEL_NAME\n)\n\n\nqueries\n\n['was ist die geschichte der autos in deutschland?',\n 'wie hat sich die autoindustrie in deutschland im laufe der zeit entwickelt',\n 'wie ver√§ndert sich die produktion von autos in deutschland']\n\n\n\ntasks\n\n[]"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#hyde-generation",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#hyde-generation",
    "title": "Mastering RAG Pipelines",
    "section": "HyDE generation",
    "text": "HyDE generation\nLet‚Äôs now pick a query that has something to do with our corpus to create hypothetical documents and its embeddings.\n\nquery = \"How good is table recognition in docling?\"\nqueries, tasks = aggregate_queries_and_tasks(\n    tok, gen, query, prompts[\"rewrite\"], prompts[\"variants\"], \n    temperature=0.7, n_variations=3, gen_model_name=GEN_MODEL_NAME\n)\nqueries\n\n['how good is table recognition in docling?',\n 'how does docling perform in identifying tables from various document types',\n 'how effective is table recognition in document processing',\n 'what is the accuracy of table recognition in document analysis']\n\n\nInstead of passing each query, we send all in one go as a batch‚Ä¶\n\n%time hyde_docs = generate_text(\n    tok, gen, queries, prompts['hyde'], temperature=.7, model_name=GEN_MODEL_NAME\n)\n\nCPU times: user 11.1 s, sys: 287 ms, total: 11.4 s\nWall time: 11.4 s\n\n\n\nlen(hyde_docs)\n\n4\n\n\nLet‚Äôs see some samples‚Ä¶\n\nfor doc in hyde_docs[:2]:\n    print(f\"{'='*30}\\n{textwrap.fill(doc, width=80)}\")\n\n==============================\nDocling is a document processing tool that includes features for recognizing\ntables within documents. According to its documentation and user reviews,\nDocling's table recognition capability is generally considered adequate for most\nuse cases, including extracting table data into structured formats. The tool\nuses machine learning models to identify and parse tables, which works well for\nstandard formats such as CSV, TSV, and HTML. However, users have reported that\nits performance can vary depending on the complexity and formatting of the\noriginal document. While it is not as advanced as some specialized tools in\nhandling highly irregular or complex table structures, Docling is suitable for\ngeneral-purpose table extraction tasks. Its effectiveness is often compared\nfavorably to other document processing tools in terms of accuracy and ease of\nuse.\n==============================\nDocling is a document processing tool designed to extract and structure\ninformation from various document types, including tables. According to a 2023\nevaluation by DocumentAI, Docling demonstrated strong performance in identifying\nand extracting tables from common document formats such as PDFs, Word documents,\nand Excel files. The tool successfully recognized tabular data in both\nstructured and unstructured formats, with an accuracy rate of over 92% in\ncontrolled testing environments. However, its performance varied when processing\nhighly formatted or scanned documents, where table recognition was less\nconsistent. Users have reported that Docling's table detection is reliable for\nstandard business documents but may require additional preprocessing for complex\nor non-standard layouts. The tool is part of a broader suite of AI-driven\ndocument processing solutions aimed at improving data extraction efficiency.\n\n\nWe now split it into chunks and get its embeddings; same as what we did for our corpus.\n\nchunks = []\nfor hyde_doc in hyde_docs:\n    chunks.extend(text_splitter.split_text(hyde_doc))\nq_emb = get_emb(chunks, prompt_name=\"query\", show_progress_bar=False)\nq_emb.shape\n\n(4, 1024)"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#search-in-index",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#search-in-index",
    "title": "Mastering RAG Pipelines",
    "section": "Search in index",
    "text": "Search in index\nWe can define how many matches to retrieve for a given query through k_per_variant\n\nk_per_variant = 5\nmatches = ds.get_nearest_examples_batch(\"embeddings\", q_emb, k_per_variant)\nnp.array(matches.total_scores)\n\narray([[0.7055741 , 0.6299919 , 0.62796915, 0.5997107 , 0.5608388 ],\n       [0.7040975 , 0.6866737 , 0.6717943 , 0.6013819 , 0.59106   ],\n       [0.53217894, 0.5187626 , 0.43610024, 0.42912632, 0.41111925],\n       [0.52601624, 0.4494145 , 0.446052  , 0.436696  , 0.3895734 ]],\n      dtype=float32)\n\n\nGreat! We got all the top k_per_variant matching chunks for each of the hyde in one go. The above array shows their corresponding similarity scores with each of the query embedding. We shall also now retrieve the unique ids of the corresponding chunks to rank them.\n\nindices = [x[\"id\"] for x in matches.total_examples]\nindices\n\n[[7, 4, 0, 3, 1], [7, 4, 0, 3, 9], [4, 9, 0, 13, 7], [4, 9, 13, 12, 10]]\n\n\nWe notice that ids 0, 4, 7 consistently came out on the top. Let‚Äôs see their rankings.\n\n\nReciprocal Rank Fusion\nWe define the RRF function to rank and retrieve most relevant matches.\n\ndef reciprocal_rank_fusion(indices, top_k=3, denom=50):\n    scores = defaultdict(int)\n    for row in indices:\n        for rank, idx in enumerate(row):\n            scores[idx] += 1 / (rank + denom)\n    results = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n    return [idx for idx, _ in results]\n\nLet‚Äôs now get the top_k chunk ids from all of the above that best match our original query.\n\ntop_k = 2\ntop_idx = reciprocal_rank_fusion(indices, top_k=top_k)\ntop_idx\n\n[4, 7]\n\n\n\nMarkdown(ds[top_idx[0]]['chunk'])\n\n\n3.2 AI models\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n\n\nLayout Analysis Model\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n\n\nTable Structure Recognition\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n\n\nOCR\n\n\n\nIndeed the top most document actually contains the answer to our query. We now wrap it all up in one function!\n\n\nCode\ndef retrieve(\n    tok, gen, query, n_variants=3, top_k_per_variant=5, top_k_retrieve=2, \n    model_name=\"\", **llm_kwargs,\n):\n    queries, tasks = aggregate_queries_and_tasks(\n        tok, gen, query.strip(), prompts[\"rewrite\"], prompts[\"variants\"], \n        n_variants, model_name, **llm_kwargs,\n    )\n    hyde_docs = generate_text(\n        tok, gen, queries, prompts[\"hyde\"], model_name, **llm_kwargs)\n    chunks = []\n    for hyde_doc in hyde_docs:\n        chunks.extend(text_splitter.split_text(hyde_doc))\n    q_emb = get_emb(chunks)\n    matches = ds.get_nearest_examples_batch(\"embeddings\", q_emb, top_k_per_variant)\n    indices = [match[\"id\"] for match in matches.total_examples]\n    top_idx = reciprocal_rank_fusion(indices, top_k_retrieve)\n    return top_idx, tasks\n\n\n\n%time i,t = retrieve(tok, gen, query, model_name=GEN_MODEL_NAME)\n\n\n\n\nCPU times: user 19.1 s, sys: 643 ms, total: 19.7 s\nWall time: 19.8 s\n\n\n\ni\n\n[4, 7]"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#ai-models",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#ai-models",
    "title": "Mastering RAG Pipelines",
    "section": "3.2 AI models",
    "text": "3.2 AI models\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#layout-analysis-model",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#layout-analysis-model",
    "title": "Mastering RAG Pipelines",
    "section": "Layout Analysis Model",
    "text": "Layout Analysis Model\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#table-structure-recognition",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#table-structure-recognition",
    "title": "Mastering RAG Pipelines",
    "section": "Table Structure Recognition",
    "text": "Table Structure Recognition\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#ocr",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#ocr",
    "title": "Mastering RAG Pipelines",
    "section": "OCR",
    "text": "OCR"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#ai-models-1",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#ai-models-1",
    "title": "Mastering RAG Pipelines",
    "section": "3.2 AI models",
    "text": "3.2 AI models\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#layout-analysis-model-1",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#layout-analysis-model-1",
    "title": "Mastering RAG Pipelines",
    "section": "Layout Analysis Model",
    "text": "Layout Analysis Model\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#table-structure-recognition-1",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#table-structure-recognition-1",
    "title": "Mastering RAG Pipelines",
    "section": "Table Structure Recognition",
    "text": "Table Structure Recognition\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#ocr-1",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#ocr-1",
    "title": "Mastering RAG Pipelines",
    "section": "OCR",
    "text": "OCR\n\n\nSource 2 :: docling_technical_report:7"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#applications",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#applications",
    "title": "Mastering RAG Pipelines",
    "section": "5 Applications",
    "text": "5 Applications\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling‚Äôs feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#future-work-and-contributions",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#future-work-and-contributions",
    "title": "Mastering RAG Pipelines",
    "section": "6 Future work and contributions",
    "text": "6 Future work and contributions\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#references",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#references",
    "title": "Mastering RAG Pipelines",
    "section": "References",
    "text": "References\n\n[1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#applications-1",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#applications-1",
    "title": "Mastering RAG Pipelines",
    "section": "5 Applications",
    "text": "5 Applications\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling‚Äôs feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#future-work-and-contributions-1",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#future-work-and-contributions-1",
    "title": "Mastering RAG Pipelines",
    "section": "6 Future work and contributions",
    "text": "6 Future work and contributions\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#references-1",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#references-1",
    "title": "Mastering RAG Pipelines",
    "section": "References",
    "text": "References\n\n[1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n\n\n\nSource 2 :: docling_technical_report:5"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#ocr-2",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#ocr-2",
    "title": "Mastering RAG Pipelines",
    "section": "OCR",
    "text": "OCR\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#assembly",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#assembly",
    "title": "Mastering RAG Pipelines",
    "section": "3.3 Assembly",
    "text": "3.3 Assembly\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#extensibility",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#extensibility",
    "title": "Mastering RAG Pipelines",
    "section": "3.4 Extensibility",
    "text": "3.4 Extensibility\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\nImplementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#performance",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#performance",
    "title": "Mastering RAG Pipelines",
    "section": "4 Performance",
    "text": "4 Performance"
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#tipsobservations",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#tipsobservations",
    "title": "Mastering RAG Pipelines",
    "section": "Tips/Observations:",
    "text": "Tips/Observations:\n\nExtracting text as a markdown greatly preserved the structure and continuity of the text. This resulted in better logical chunking which in turn led to better embeddings and as a consequence, better search results.\nReading the document via docling extracted more and correct text compared to pymupdf4llm but at a bit of an expense of speed. It is enabled by default for prioritising accuracy. This proved esp.¬†useful in extracting data containing lots of tables spread over multiple pages.\nYou can pass --fast-extract from CLI or tick a box via gradio UI to use pymupdf instead.\nIncreasing the model size (coupled with correct text extraction in markdown) greatly improved performance. The Qwen3 models very much adhered to instructions but the smaller variants instead of hallucinating simply fell back to saying ‚ÄòI don‚Äôt know‚Äô (as per instructions). The 4B variant understood the user intent which sometimes was vague and yet managed to give relevant results. The base variant is huge and it wouldn‚Äôt have been fit and run fast enough on a consumer grade laptop GPU. Loading the AWQ variant of it helped as it occupied substantially less memory compared to the original without much loss in performance.\nThis model also showed great multilingual capabilities. User can upload document in one language and ask questions in another. Or they could upload multilingual documents and ask multilingual queries. For the demo, I tested mostly in English & German.\nThe data is now stored in datasets format that allows for better storage & scaling (arrow) along with indexing (FAISS) for querying."
  },
  {
    "objectID": "posts/rag_with_hyde_rrf/rag_hyde.html#limitations-known-issues",
    "href": "posts/rag_with_hyde_rrf/rag_hyde.html#limitations-known-issues",
    "title": "Mastering RAG Pipelines",
    "section": "Limitations / Known Issues",
    "text": "Limitations / Known Issues\n\nEven though docling with mostly default options proved to be better than pymupdf4llm to extract text, it‚Äôs not perfect everytime. There‚Äôre instances where pymupdf extracted text from an embedded image inside a PDF better than docling. However, docling is highly configurable and allows for deep customization via ‚Äòpipelines‚Äô. And it also comes with a very permissive license for commercial use compared to PyMuPDF.\n\ndocling comes with easyocr by default for text OCR. It‚Äôs not powerful enough compared to tesseract or similar models. But since installing the latter and linking it with docling involves touching system config, it‚Äôs not pursued.\n\nWhen user uploads multiple PDFs, we can improve load times by reading them asynchronously. Attempts to do that with docling sometimes resulted in pages with ordering different than the original. So it‚Äôs dropped for the demo. More investigation is needed later."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMastering RAG Pipelines\n\n\n\nEDA\n\npytorch\n\nnlp\n\ngenai\n\nllms\n\nrag\n\nhyde\n\nrrf\n\nranking\n\ninformation retrieval\n\n\n\nHow HyDE‚Äôs simulated document representations combined with query rewrites and RRF enhance retrieval quality in RAG systems\n\n\n\n\n\nOct 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDocument Layout Similarity Detection\n\n\n\nEDA\n\npytorch\n\ncomputer vision\n\nembeddings\n\nDocLayout\n\nclassification\n\n\n\nAssessing image document similarity through structural and spatial composition analysis.\n\n\n\n\n\nOct 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCurate jobs with Agentic AI & Ollama\n\n\n\nagents\n\nnlp\n\nllms\n\nwebscraping\n\nollama\n\ncrewai\n\n\n\nGet a consolidated report of your favorite jobs\n\n\n\n\n\nSep 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSimple & Fast Local DL Setup with PyTorch, Pixi & Nvidia\n\n\n\nsetup\n\npixi\n\npytorch\n\nnvidia\n\n\n\nGet pytorch running locally with GPU support\n\n\n\n\n\nAug 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GIS with QGIS & Python - Part II\n\n\n\nEDA\n\ngis\n\nqgis\n\n\n\nhow to load data from CSV/Text files, do filtering and connect to data servers\n\n\n\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GIS with QGIS & Python - Part I\n\n\n\nEDA\n\ngis\n\nqgis\n\n\n\nExploring the fundamentals of geospatial data analysis\n\n\n\n\n\nMar 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDiving into Fastai‚Äôs mid-level API with MNIST\n\n\n\nEDA\n\ncomputer vision\n\nfastai\n\n\n\nhow to build custom pipelines & dataloaders\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFace mask detection with fastai\n\n\n\nEDA\n\ncomputer vision\n\nfastai\n\n\n\nmake your model COVID-19 aware‚Ä¶\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nfastai env setup\n\n\n\nsetup\n\nfastai\n\n\n\nget fastai running locally with GPU support\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating from Fastpages to Quarto\n\n\n\nmeta\n\n\n\nblogging setup‚Ä¶ revisited‚Ä¶\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge your data processing with DuckDB\n\n\n\nEDA\n\npandas\n\nsql\n\n\n\nEfficient & blazing fast SQL analytics in Pandas with DuckDB\n\n\n\n\n\nMay 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up your blog with Fastpages\n\n\n\nmeta\n\n\n\nStart your blogging journey in a few easy steps.\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\nNo matching items"
  }
]