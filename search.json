[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nDiving into Fastaiâ€™s mid-level API with MNIST\n\n\n\n\n\n\n\nEDA\n\n\ncomputer vision\n\n\nfastai\n\n\n\n\nhow to build custom pipelines & dataloaders\n\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nFace mask detection with fastai\n\n\n\n\n\n\n\nEDA\n\n\ncomputer vision\n\n\nfastai\n\n\n\n\nmake your model COVID-19 awareâ€¦\n\n\n\n\n\n\nDec 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nfastai env setup\n\n\n\n\n\n\n\nsetup\n\n\nfastai\n\n\n\n\nget your notebooks running locally with GPU support\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMigrating from Fastpages to Quarto\n\n\n\n\n\n\n\nmeta\n\n\n\n\nblogging setupâ€¦ revisitedâ€¦\n\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSupercharge your data processing with DuckDB\n\n\n\n\n\n\n\nEDA\n\n\npandas\n\n\nSQL\n\n\n\n\nEfficient & blazing fast SQL analytics in Pandas with DuckDB\n\n\n\n\n\n\nMay 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSetting up your blog with Fastpages\n\n\n\n\n\n\n\nmeta\n\n\n\n\nStart your blogging journey in a few easy steps.\n\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting my humble abode on the Internet. I am Vivek, a freelance Data Scientist by profession.\nSaá¹ƒsÄra means World in Sanskrit and I welcome you into mine where I blog about programming, among other things.\nI have over 6 years of experience in Python & Machine Learning ecosystems across a broad variety of sectors ranging from Fashion ðŸ›ï¸ to Mobility ðŸ›´ to Fintech ðŸ’±.\nAt Sezzle, a platform offering BNPL service, I worked on developing and deploying early machine learning based fraud detection models. Prior to that at FreeNow, a mobility solutions provider, I worked on GMV forecasting, churn prediction etc.\nApart from  & , I am also interested in ðŸ§‘â€ðŸŒ¾, ðŸšœ, ðŸŽ¶, ðŸ§‘â€ðŸ³ and aim to know a little bit of everything I come across.\nPlease feel free to reach me for any business, consulting or workshop opportunities. ðŸ˜ƒ"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "title": "Supercharge your data processing with DuckDB",
    "section": "",
    "text": "Do you have large datasets that you simply can not load into memory to analyse with Pandas? Or do you feel more comfortable expressing operations in SQL instead of python?\nFret not, for you have DuckDB now! âœ¨ðŸ¦†âœ¨"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Setup",
    "text": "Setup\nDuckDB is very lightweight and has no external dependencies and runs within the host process itself. Simply install it with:\npip install duckdb==0.3.4\nTo initialize it, run:\n\nimport duckdb\ndbcon = duckdb.connect()\n\nThatâ€™s it! Now you can test it by running:\n\ndbcon.execute('select 1, 2, 3').fetchall()\n\n[(1, 2, 3)]\n\n\nNext step is to run pip install pyarrow to add support for reading/writing parquet data.\n\nJupyter Notebook setup\nIf in case you wish to explore it in Jupyter Notebooks, install a few additional libraries for a better experience:\npip install ipython-sql SQLAlchemy duckdb-engine\nImport them once installed:\n\nimport pandas as pd\nimport sqlalchemy\n\n%load_ext sql\n\nSet a few config options to prettify the output and return it as Pandas DataFrame\n\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\nDuckDB is primarily designed to be an in-memory DB. You can however persist your data to disk.\n\n%sql duckdb:///:memory:\n# %sql duckdb:///path/to/file.db\n\n\n#collapse-output\n\n# we can also access the current config & settings of DuckDB by running the following:\n%sql SELECT * FROM duckdb_settings();\n\n\n\n\n\n  \n    \n      \n      name\n      value\n      description\n      input_type\n    \n  \n  \n    \n      0\n      access_mode\n      automatic\n      Access mode of the database (AUTOMATIC, READ_O...\n      VARCHAR\n    \n    \n      1\n      checkpoint_threshold\n      16.7MB\n      The WAL size threshold at which to automatical...\n      VARCHAR\n    \n    \n      2\n      debug_checkpoint_abort\n      NULL\n      DEBUG SETTING: trigger an abort while checkpoi...\n      VARCHAR\n    \n    \n      3\n      debug_force_external\n      False\n      DEBUG SETTING: force out-of-core computation f...\n      BOOLEAN\n    \n    \n      4\n      debug_many_free_list_blocks\n      False\n      DEBUG SETTING: add additional blocks to the fr...\n      BOOLEAN\n    \n    \n      5\n      debug_window_mode\n      NULL\n      DEBUG SETTING: switch window mode to use\n      VARCHAR\n    \n    \n      6\n      default_collation\n      \n      The collation setting used when none is specified\n      VARCHAR\n    \n    \n      7\n      default_order\n      asc\n      The order type used when none is specified (AS...\n      VARCHAR\n    \n    \n      8\n      default_null_order\n      nulls_first\n      Null ordering used when none is specified (NUL...\n      VARCHAR\n    \n    \n      9\n      disabled_optimizers\n      \n      DEBUG SETTING: disable a specific set of optim...\n      VARCHAR\n    \n    \n      10\n      enable_external_access\n      True\n      Allow the database to access external state (t...\n      BOOLEAN\n    \n    \n      11\n      enable_object_cache\n      False\n      Whether or not object cache is used to cache e...\n      BOOLEAN\n    \n    \n      12\n      enable_profiling\n      NULL\n      Enables profiling, and sets the output format ...\n      VARCHAR\n    \n    \n      13\n      enable_progress_bar\n      False\n      Enables the progress bar, printing progress to...\n      BOOLEAN\n    \n    \n      14\n      explain_output\n      physical_only\n      Output of EXPLAIN statements (ALL, OPTIMIZED_O...\n      VARCHAR\n    \n    \n      15\n      force_compression\n      NULL\n      DEBUG SETTING: forces a specific compression m...\n      VARCHAR\n    \n    \n      16\n      log_query_path\n      NULL\n      Specifies the path to which queries should be ...\n      VARCHAR\n    \n    \n      17\n      max_memory\n      26.9GB\n      The maximum memory of the system (e.g. 1GB)\n      VARCHAR\n    \n    \n      18\n      memory_limit\n      26.9GB\n      The maximum memory of the system (e.g. 1GB)\n      VARCHAR\n    \n    \n      19\n      null_order\n      nulls_first\n      Null ordering used when none is specified (NUL...\n      VARCHAR\n    \n    \n      20\n      perfect_ht_threshold\n      12\n      Threshold in bytes for when to use a perfect h...\n      BIGINT\n    \n    \n      21\n      preserve_identifier_case\n      True\n      Whether or not to preserve the identifier case...\n      BOOLEAN\n    \n    \n      22\n      profiler_history_size\n      NULL\n      Sets the profiler history size\n      BIGINT\n    \n    \n      23\n      profile_output\n      \n      The file to which profile output should be sav...\n      VARCHAR\n    \n    \n      24\n      profiling_mode\n      NULL\n      The profiling mode (STANDARD or DETAILED)\n      VARCHAR\n    \n    \n      25\n      profiling_output\n      \n      The file to which profile output should be sav...\n      VARCHAR\n    \n    \n      26\n      progress_bar_time\n      2000\n      Sets the time (in milliseconds) how long a que...\n      BIGINT\n    \n    \n      27\n      schema\n      \n      Sets the default search schema. Equivalent to ...\n      VARCHAR\n    \n    \n      28\n      search_path\n      \n      Sets the default search search path as a comma...\n      VARCHAR\n    \n    \n      29\n      temp_directory\n      .tmp\n      Set the directory to which to write temp files\n      VARCHAR\n    \n    \n      30\n      threads\n      12\n      The number of total threads used by the system.\n      BIGINT\n    \n    \n      31\n      wal_autocheckpoint\n      16.7MB\n      The WAL size threshold at which to automatical...\n      VARCHAR\n    \n    \n      32\n      worker_threads\n      12\n      The number of total threads used by the system.\n      BIGINT\n    \n    \n      33\n      binary_as_string\n      \n      In Parquet files, interpret binary data as a s...\n      BOOLEAN\n    \n    \n      34\n      Calendar\n      gregorian\n      The current calendar\n      VARCHAR\n    \n    \n      35\n      TimeZone\n      Europe/Berlin\n      The current time zone\n      VARCHAR\n    \n  \n\n\n\n\nFrom now on, you can run SQL directly by prefixing %sql (or %%sql for multiline statements) to your code cell and get your output returned as pandas dataframe :man_dancing:.\n\n%sql select 1 as a;\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      0\n      1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "title": "Supercharge your data processing with DuckDB",
    "section": "DuckDB vs traditional Databases",
    "text": "DuckDB vs traditional Databases\nWith pandas.read_sql command, one can already run SQL queries on an existing DB connection, read tables and load data as pandas DataFrames in memory for processing in python. While this is fine for lightweight operations, it is not optimized for heavy data processing. Traditional RDBMSs such as Postgres, MySQL, etc. process each row sequentially which apart from taking long time to execute, also induce a lot of overhead on CPU. DuckDB on the other hand is built with OLAP in mind and is Column-Vectorized. This helps massively parallelize disk I/O and query executions.\n\nDuckDB uses the Postgres SQL parser under the hood, and offers many of the same SQL features as Postgres 1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Projection & Filter Pushdowns",
    "text": "Projection & Filter Pushdowns\nNow letâ€™s do a simple filter operation on our dataset. Letâ€™s count the total number of rows that satisfy the condition TAXI_OUT > 10. Weâ€™ll try with both pandas & duckdb.\n\ndf[df['TAXI_OUT'] > 10].shape\n\n(45209245, 28)\n\n\n\n%%sql\n\nselect count(*) as count\nfrom df\nwhere TAXI_OUT > 10\n\n\n\n\n\n  \n    \n      \n      count\n    \n  \n  \n    \n      0\n      45209245\n    \n  \n\n\n\n\nWhile the earlier operation took ~9.5s, the latter just took ~250ms :zap:. Thereâ€™s just no comparison.\nThis is because duckdb automatically optimizes the query by selecting only the required column(s) (aka projection pushdown) and then applies the filtering to get a subset of data (aka filter pushdown). Pandas instead reads through all the columns. We can optimize this in pandas by doing these pushdowns ourselves.\n\nprojection_pushdown_df = df[['TAXI_OUT']]\nfilter_pushdown_df = projection_pushdown_df[projection_pushdown_df['TAXI_OUT'] > 10]\nfilter_pushdown_df.shape\n\n(45209245, 1)\n\n\nWe managed to bring this down from several seconds to almost a second. But using duckdb is still about 70-90% faster than this."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Groupby",
    "text": "Using Groupby\nNow letâ€™s calculate a few aggregates using groupby with projection & filter pushdowns combined.\nHere, we compute a few simple metrics with a certain airline carrier grouped by two origin & destination airports and finally sort the results by the origin airport.\n\nprojection_df = df[['ORIGIN', 'DEST', 'TAXI_OUT', \n                    'AIR_TIME', 'DISTANCE', 'OP_CARRIER']]\norigin_df = projection_df[\n    (projection_df['ORIGIN'].isin(('DCA', 'EWR'))) &\n    (projection_df['DEST'].isin(('DCA', 'EWR'))) &\n    (projection_df['OP_CARRIER'] == 'XE')]\n(origin_df\n     .groupby(['ORIGIN', 'DEST'])\n     .agg(\n         avg_taxi_out=('TAXI_OUT', 'mean'),\n         max_air_time=('AIR_TIME', 'max'),\n         total_distance=('DISTANCE', 'sum'))\n     .sort_index(level=0)\n)\n\n\n\n\n\n  \n    \n      \n      \n      avg_taxi_out\n      max_air_time\n      total_distance\n    \n    \n      ORIGIN\n      DEST\n      \n      \n      \n    \n  \n  \n    \n      DCA\n      EWR\n      22.116009\n      87.0\n      828835.0\n    \n    \n      EWR\n      DCA\n      23.675481\n      93.0\n      831024.0\n    \n  \n\n\n\n\nWe can make it a bit more concise by using .query for filtering pushdown.\n\n(df\n .query('OP_CARRIER == \"XE\" and ORIGIN in (\"DCA\", \"EWR\") and DEST in (\"DCA\", \"EWR\")')\n .groupby(['ORIGIN', 'DEST'])\n .agg(\n     avg_taxi_out=('TAXI_OUT', 'mean'),\n     max_air_time=('AIR_TIME', 'max'),\n     total_distance=('DISTANCE', 'sum'))\n)\n\n\n\n\n\n  \n    \n      \n      \n      avg_taxi_out\n      max_air_time\n      total_distance\n    \n    \n      ORIGIN\n      DEST\n      \n      \n      \n    \n  \n  \n    \n      DCA\n      EWR\n      22.116009\n      87.0\n      828835.0\n    \n    \n      EWR\n      DCA\n      23.675481\n      93.0\n      831024.0\n    \n  \n\n\n\n\nThis approach took only about half the time (~3s) compared to our earlier one becauseÂ .query uses a modified syntax of python and also indexing thus resulting in more efficient evaluation. We can now compare that to our SQL counterpartâ€¦\n\n%%sql\n\nselect\n    ORIGIN,\n    DEST,\n    AVG(TAXI_OUT) as avg_taxi_out,\n    MAX(AIR_TIME) as max_air_time,\n    SUM(DISTANCE) as total_distance\n\nfrom df\n\nwhere\n    OP_CARRIER = 'XE' and\n    ORIGIN in ('DCA', 'EWR') and\n    DEST in ('DCA', 'EWR')\n    \ngroup by ORIGIN, DEST\norder by ORIGIN\n\n\n\n\n\n  \n    \n      \n      ORIGIN\n      DEST\n      avg_taxi_out\n      max_air_time\n      total_distance\n    \n  \n  \n    \n      0\n      DCA\n      EWR\n      22.116009\n      87.0\n      828835.0\n    \n    \n      1\n      EWR\n      DCA\n      23.675481\n      93.0\n      831024.0\n    \n  \n\n\n\n\nThis ~400ms execution with duckdb above is around an order of magnitude faster and also a lot cleaner, Iâ€™d say. :wink:\nNotice that the data is already loaded under df and hence we donâ€™t need to read from the source parquet file.\n\nIn the same way, we can also improve the performance of our queries drastically when using joins across multiple tables. I leave this as an exercise to the reader.\n\nBut why actually load data into memory in the first place when we can process it more efficiently with it being just on disk? Often times, the data is too big to load into memory anyways.\nTo do that, we just need to create a VIEW to our data which lets us query the table directly without loading onto memory and update the source from the dataframe df to the newly created view instead.3"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using approximations",
    "text": "Using approximations\nAt times, it suffices just to get an estimate of certain data rather than a precise answer. Using approximations would help us to just that.\n\n%%sql\n\nselect\n    OP_CARRIER,\n    approx_count_distinct(DEST) as approx_num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n  \n    \n      \n      OP_CARRIER\n      approx_num_unique_destinations\n    \n  \n  \n    \n      0\n      9E\n      186\n    \n    \n      1\n      AA\n      116\n    \n    \n      2\n      AS\n      77\n    \n    \n      3\n      B6\n      73\n    \n    \n      4\n      CO\n      85\n    \n    \n      5\n      DL\n      171\n    \n    \n      6\n      EV\n      205\n    \n    \n      7\n      F9\n      130\n    \n    \n      8\n      FL\n      75\n    \n    \n      9\n      G4\n      126\n    \n  \n\n\n\n\n\n%%sql\n\nselect\n    OP_CARRIER,\n    -- takes more time to compute\n    count(distinct DEST) as num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n  \n    \n      \n      OP_CARRIER\n      num_unique_destinations\n    \n  \n  \n    \n      0\n      9E\n      185\n    \n    \n      1\n      AA\n      116\n    \n    \n      2\n      AS\n      77\n    \n    \n      3\n      B6\n      73\n    \n    \n      4\n      CO\n      85\n    \n    \n      5\n      DL\n      170\n    \n    \n      6\n      EV\n      205\n    \n    \n      7\n      F9\n      129\n    \n    \n      8\n      FL\n      75\n    \n    \n      9\n      G4\n      126\n    \n  \n\n\n\n\nOur approximation query earlier ran about 3-4 times faster than the precise one in this case. This is crucial when responsiveness is more important than precision (esp.Â for larger datasets)."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Window functions",
    "text": "Using Window functions\nFinally, letâ€™s wrap our analysis by showing off a bit more of what duckdb can do using some advanced SQL operations.\nWe create two CTEs (Common Table Expressions) to calculate a couple of features. We do filter & projection pushdowns in one CTE and compute our desired features in another. The first feature is a simple demo to showcase if-else support. The second feature is a bit advanced where we find out the last destination a given air carrier has flown to, sorted by flying date. And when it doesnâ€™t exist, replace it with NA. We then take a sample from the final resultant set.\n\n%%sql\n\nwith limited_data as (\n    select \n        FL_DATE,\n        ORIGIN, \n        DEST, \n        DISTANCE,\n        OP_CARRIER,\n    from airlinedata\n    where FL_DATE >= '2015-01-01'    \n),\n\nlast_destination_data as (\n    select *,\n        case\n            when DISTANCE*1.60934 > 500 then 'yes'\n            else 'no'\n        end as distance_more_than_500_km,\n\n        coalesce(last_value(DEST) over (\n            partition by OP_CARRIER\n            order by FL_DATE\n            rows between unbounded preceding and 1 preceding\n        ), 'NA') as last_destination_flown_with_this_carrier\n\n    from limited_data\n)\n\nselect *\nfrom last_destination_data\nusing sample 10;\n\n\n\n\n\n  \n    \n      \n      FL_DATE\n      ORIGIN\n      DEST\n      DISTANCE\n      OP_CARRIER\n      distance_more_than_500_km\n      last_destination_flown_with_this_carrier\n    \n  \n  \n    \n      0\n      2018-07-10\n      DCA\n      LGA\n      214.0\n      YX\n      no\n      DCA\n    \n    \n      1\n      2015-05-08\n      DAL\n      BWI\n      1209.0\n      WN\n      yes\n      BWI\n    \n    \n      2\n      2018-03-30\n      LAS\n      SJC\n      386.0\n      WN\n      yes\n      SJC\n    \n    \n      3\n      2015-07-10\n      BOS\n      MSP\n      1124.0\n      DL\n      yes\n      DTW\n    \n    \n      4\n      2016-06-01\n      DTW\n      BWI\n      409.0\n      DL\n      yes\n      DTW\n    \n    \n      5\n      2017-07-26\n      GEG\n      MSP\n      1175.0\n      DL\n      yes\n      SAN\n    \n    \n      6\n      2017-01-10\n      DFW\n      ACT\n      89.0\n      EV\n      no\n      DFW\n    \n    \n      7\n      2015-01-01\n      PHX\n      DRO\n      351.0\n      OO\n      yes\n      BFL\n    \n    \n      8\n      2018-05-06\n      DFW\n      MCO\n      985.0\n      AA\n      yes\n      IAH\n    \n    \n      9\n      2018-04-30\n      MSY\n      LAX\n      1670.0\n      WN\n      yes\n      LAX\n    \n  \n\n\n\n\nNice, isnâ€™t it?! The same operation is unimaginably complex (for me, at least) in pandas. ðŸ¤¯\nWith DuckDB, we can combine one or more of many of such complex operations and execute in one go without worrying much about manual optimizations."
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html",
    "href": "posts/face-mask-detection/facemask_detection.html",
    "title": "Face mask detection with fastai",
    "section": "",
    "text": "With COVID-19 mutating and still posing a threat globally, wearing a ðŸ˜· is still mandatory in many countries. In this post, we will see how to train a simple computer vision model to detect whether the person is wearing a facemask or not. Letâ€™s start by downloading the appropriate dataset1 from kaggle.\nYou can either download it manually from kaggle or use its free API. Letâ€™s do the latter.\nLetâ€™s create a directory called data to store and extract our desired dataset."
  },
  {
    "objectID": "posts/face-mask-detection/facemask_detection.html#verifying-results",
    "href": "posts/face-mask-detection/facemask_detection.html#verifying-results",
    "title": "Face mask detection with fastai",
    "section": "verifying results",
    "text": "verifying results\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\n\n\n\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\nLetâ€™s see what the model is getting wrong or is most unsure about by plotting its top losses!\n\ninterp.plot_top_losses(4, figsize=(8, 8))\n\n\n\n\n\n\n\n\n\n\n\nWe only got one incorrect image & the rest are correct but a bit inconfident (and even thatâ€™s low for the 2nd row of images).\n\nImage Shifting\nLetâ€™s do a bit of fun! Sometimes, itâ€™s known that some image recognition models predict completely different classes when even a few pixels are changedâ€¦ hence letâ€™s see how robust our model is by rotating and wrapping an image and then letting our model predict.\n\ndef rotate_and_wrap_image(image, percentage=.4):\n    im = tensor(image)\n    val = int(im.shape[1] * percentage)\n    return torch.cat((im[:, val:], im[:, :val]), dim=1)    \n\n\nim.resize((128, 128))\n\n\n\n\n\nshow_image(rotate_and_wrap_image(im));\n\n\n\n\n\nlearner.predict(rotate_and_wrap_image(im))\n\n\n\n\n\n\n\n\n('WithoutMask', TensorBase(1), TensorBase([3.2003e-07, 1.0000e+00]))\n\n\ninteresting! our model still predicts correct class and this worked on many other tested images as well. This might mean that the model has actually learnt to identify a ðŸ˜· and not â€˜rememberâ€™ the image itself."
  },
  {
    "objectID": "posts/migrate-to-quarto/index.html",
    "href": "posts/migrate-to-quarto/index.html",
    "title": "Migrating from Fastpages to Quarto",
    "section": "",
    "text": "Fastpages, based on which my original blog was setup has been archived and they now recommend using Quarto instead. If youâ€™re starting new, the latter is the recommended way to go but if you too have a fastpages blog setup initially and want to migrate, thereâ€™s a migration guide available. Itâ€™s not completely perfect as you have to tweak a few bits here & there before you see all the renderings correctly (because of slight syntactic variations amongst other things).\nHowever, I felt quarto to be much more intuitive and easy to setup. I also did the migration but found manually moving posts from my old blog repo to the new quarto repo to be a bit easier. (Migration worked but I was not happy with the directories it created as part of itâ€¦ I found it aesthetically less pleasing and hence moved them myself).\nFor simple blogging, fastpages offered more than enough features and quarto offers even more on top of that. Itâ€™s easy to get started with quarto. Head over to the start guide to learn more.\nI do miss the advanced styling options I setup in fastpages thoughâ€¦ will have to dig into Quarto to see how much I can reuse. Until then, have fun with my frugalistic looking blog! :D"
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "title": "Setting up your blog with Fastpages",
    "section": "",
    "text": "After going through a bit of an intensive setup, I thought of writing up my journey & difficulties I faced while setting up this blog so that it could help others who might start later."
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "title": "Setting up your blog with Fastpages",
    "section": "Motivation",
    "text": "Motivation\nI was looking for ways to have my tiny little place on the internet & therefore scouted for feasible options. As a Data Scientist, I interact with code regularly and (Jupyter) Notebooks are part of our DNA :stuck_out_tongue:. So the criteria that I had set for myself was that whatever platform I choose, it has to be (a) very code friendly, (b) easy to maintain, and (c) relatively affordable (if possible, even free).\n\nQuest for the right tool\nIâ€™m a big fan of Notion and use it for almost everything in my private life. So I initially considered setting up my blog with Super as you can create websites straight away from Notion pages. Though it looks great, the pricing is something that Iâ€™m still not comfortable with (yet).\nThen thereâ€™s Medium, which provides a nice writing experience but only as long as you donâ€™t have much code in your content. Though affordable, just like Super, it has mediocre support for code formatting natively & you have to optimize your content well ahead with a lot of GitHub gists. It also has no out-of-the-box support for mathematical symbols/equations. Read more about its shortcomings from a developer perspective in a great post from Prasanth Rao here.Though, I might still consider using this service to post once in a while to increase my outreach. Weâ€™ll see how it goes. \n\nThese first two options are great if you donâ€™t write code-heavy posts (that often) and are also very beginner friendly. But unfortunately, both of them are not free and also donâ€™t fit well for my use case. Besides, whereâ€™s the fun if you donâ€™t build stuff by tweaking stuff? :wink:\n\nI then decided to give GitHub Pages a try since itâ€™s a free static site generator service from GitHub. One need not know (much) about HTML/CSS and can simply write content in human readable Markdown format which gets rendered as a webpage. Besides, you get a nice revision history of your blog as everything would be version controlled with GitHub. In combination with Jekyll (that powers Github pages), thereâ€™re numerous themes & templates to choose from and so much customization that can be made via styles, CSS, etc. I can easily convert Jupyter notebooks into markdown scripts and have them rendered with Jekyll. Since one can display code snippets, markdown cells, and embed 3rd party content within Jupyter notebooks, I intended to go with this as it fulfilled most of my needsâ€¦ until I rediscovered Fastpages.\nFastpages, from fast.ai, turbocharges Jupyter notebooks by automating the process of creating blog posts from notebooks via Github Actions. We can write content in notebooks markdown files, or even Word documents and have them rendered as a web page. It offers so much more functionality on-top like code folding, interactive plots on a static web page via embeddings, comments & emoji support :heart_eyes_cat:, etc. For an excellent introduction, please refer to the official documentation.\nThat has convinced me & so now youâ€™re reading this site built with Fastpages. ðŸŽ‰"
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "title": "Setting up your blog with Fastpages",
    "section": "Setup",
    "text": "Setup\nFortunately, Fastpages is very well documented and it is highly recommended that you go through that first. However, you might still encounter some problems because of some outdated documentation, and if in case you want to test it locally on Linux systems, which is what I cover here. So, without further ado, letâ€™s dive in.\n\nThink of a name for your blog. It can just be blog like mine.\nGo through the setup instructions detailed here.\n\nIt might happen that once after you try to generate a copy as per the instructions above, a PR wonâ€™t be automatically generated. The debug instructions in the README are a bit outdated. In this case, go to the Settings -> Actions -> General section of your newly created blog repository and ensure that you have Read and write permissions enabled and the last checkbox is âœ”ï¸ like :point_down:. Hit Save. \nGo to the Actions tab and you might see a failed section. Ignore what it is for now and click that failed one. Most likely itâ€™d be a Setup workflow failure. On the top right, from the drop-down menu Re-run jobs, select Re-run failed jobs.\nOnce the above steps are all âœ…, go to the Pull Requests tab and wait for some time. Your first PR would soon be automatically created. You can also optionally check the progress under the Actions tab if desired.\nNow follow the instructions in the PR and merge it.\n\nCongratulations :confetti_ball:. Your blog would soon be up & running at {your-username}.github.io/{repo-name}. Now you can make changes directly on GitHub online or create notebook/markdown/Word files locally and simply upload them as per the instructions into the right folder. Your blog would be updated just like that (in a few minutes). ðŸª„\n\n\nTest Locally\nIn most of the cases, you might want to check how your post looks like before you publish to make sure it looks as you intend it to be, especially when it contains data from different sources or in different formats. This is when having the option of testing it locally comes in handy. With Fastpages, you can run your blog locally to see how it would look like so that you can fix any nitty gritty details before pushing it online.\nFastpages again provides a good documentation on how to test it locally with the help of Docker . It has worked fine on my Mac  but installing/upgrading Docker on Linux  is still nowhere as smooth as on Mac and hence I had to go through a bit of digging into forums to make it work on my Ubuntu  machine especially on the latest version 22.04 LTS. So, going forward Iâ€™d cover only this scenario.\n\n\n\n\n\n\nInfo\n\n\n\nFor Mac/Windows, all you need is Docker installed and simply run make server from the root level of the repo.\n\n\n\nDocker Desktop for Linux (DD4L) is still in beta and is only available for 21.XX versions of Ubuntu. So if you have that, go ahead with the setup below. If not, skip to the next step.\n\nFollow the Docker installation instructions from the official documentation.\nIf you had installed Docker via Snap or you had a previous preview version of Docker Desktop installed, itâ€™s recommended to uninstall them completely. See more here for help on how to do that.\n\nSince Ubuntu 22.04 LTS is not yet supported, I ended up installing  (not Docker Desktop) from here.\nCheck if you have docker-compose installed by doing which docker-compose or docker-compose -v. If not, install it as a standalone binary.\n\nIâ€™m not sure if it also installs Make but if it doesnâ€™t, please install it too following the instructions here.\n\nRun make server from the top level of your repository.\n\nYour Jupyter server would be available shortly at http://localhost:8080/tree and it would take a bit of time for your blog to be available under http://localhost:4000/{your-repo-name}. Be patient. :relieved:\n\n\n\n\n\n\n\n\nInfo\n\n\n\nif make server doesnâ€™t work because of permission issues, try sudo make server instead\n\n\n\nCongratulations once again! :tada: You now have a local version of your blog running. You can create new posts and have them rendered almost instantly :sparkles:. Once youâ€™re happy with the content & format, you can push it (preferably in a new branch and making a PR so that your main/master branch is unaffected). If you feel a bit adventurous, try customizing your blog more to your liking by changing fonts/styles, etc.\nIf you like this blogâ€™s customization, checkout its  repo esp.Â its custom-style.scss. May be Iâ€™ll write another post detailing it.\nGood Luck & Happy Blogging! â¤ï¸"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html",
    "title": "Diving into Fastaiâ€™s mid-level API with MNIST",
    "section": "",
    "text": "Welcome back! In this post, we will dive into Fastaiâ€™s mid-level API and learn how they help us build custom pipelines & dataloaders with the help of a simple computer vision (CV) example.\nThe dataset weâ€™ll use is the hello world equivalent of CV called MNIST. Now, thereâ€™re various ways to get this data and in fact, fastai provides it as a direct download from its URLs.MNIST attribute but I recently took part in a kaggle competition1 that provided data in an unusual wayâ€¦ one thatâ€™s not common enough to load via standard fastai methods. So I thought of making this post to show how that can be done! Letâ€™s go!"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#transforms",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#transforms",
    "title": "Diving into Fastaiâ€™s mid-level API with MNIST",
    "section": "Transforms",
    "text": "Transforms\nThe image data is in the form of an array and the labels as a column in a dataframe. I couldnâ€™t find a way to load that using the standard DataBlock API as shown in my face mask detection post right out of the way we have it (i.e., data not being stored on disk). Perhaps one can convert each row from a 784 array to a 28x28 matrix, store it as an image in a column and then load it using one of the ImageDataLoaders methods as shown above. But that sounds a bit more hacky to me compared to the elegant ways Transforms provides.\nTransform is a class that one can inherit from and it has 3 main methods to implement.\n\nencodes takes an item and transforms our data into the way we want (our custom transformation)\nsetups is an optional method that sets the inner state, if thereâ€™s any\ndecodes which is an optional step too that acts as (near) opposite to encodes. It tries to undo the operations performed in encodes, if and when possible.\n\nHereâ€™s a simple example from one of the lessons in fastai:\n\nclass NormalizeMean(Transform):\n    def setups(self, items): self.mean = sum(items)/len(items)\n    def encodes(self, x): return x-self.mean\n    def decodes(self, x): return x+self.mean\n\nHere, setups stores the inner state of the passed items i.e., the mean, encodes returns the data with the mean subtracted and decodes returns the data with the mean added (opposite of encodes).\n\n# initialize\nnm = NormalizeMean()\nnm\n\nNormalizeMean:\nencodes: (object,object) -> encodes\ndecodes: (object,object) -> decodes\n\n\n\nnm.setup([1, 2, 3, 4, 5])\n\n\nnm.mean\n\n3.0\n\n\n\nnm(2)\n\n-1.0\n\n\n\nnm.decode(2)\n\n5.0\n\n\nUsing this, letâ€™s see how an image can be obtained from an array of pixels! The idea is to pass one row of our training data and get an image & a label in returnâ€¦\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      label\n      pixel0\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      ...\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows Ã— 785 columns\n\n\n\n\nxx = train.loc[0][1:].values.reshape(-1, 28).astype(np.uint8)\nxx.shape\nshow_image(xx);\n\n\n\n\nLetâ€™s write the same as a transform that can run on all the datapoints.\n\nclass Convert_to_image(Transform):\n    def encodes(self, x):\n        mat = x[1:].reshape(-1, 28).astype(np.uint8)\n        return PILImage.create(mat)\n\nIn our case, we donâ€™t need to maintain any inner state, hence the setups method was skipped. Also thereâ€™s no need to revert to the original array state (although one can) and therefore the decodes method too was skipped.\nLetâ€™s test this by taking a sample row from our training dataset.\n\nrow = train.values[10]\nrow.shape\n\n(785,)\n\n\n\nc2i = Convert_to_image()\nc2i(row)\n\n\n\n\nYay! Now we have an image. Letâ€™s also extract the label out of the same data, which is the first value in the arrayâ€¦\n\nclass Extract_label(Transform):\n    def encodes(self, x):\n        return x[0]\n\n\nel = Extract_label()\nel(row)\n\n8\n\n\nSweet!"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#pipelines",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#pipelines",
    "title": "Diving into Fastaiâ€™s mid-level API with MNIST",
    "section": "Pipelines",
    "text": "Pipelines\nNow in order to construct our dataloaders, we still need to run a few more transformations on our independent & dependent variables such as converting the data to a tensor to take advantage of GPU etc. Pipeline helps us build a list of transformations to run on our data sequentially. Letâ€™s compose two pipelines: one that acts on our dependent data (i.e., images) and another on our independent data (i.e., labels).\n\nx_pipe = Pipeline([Convert_to_image, ToTensor])\ny_pipe = Pipeline([Extract_label, Categorize(vocab=train.label.unique())])\n\nFor our dependent data, first Convert_to_image was run, which takes a row from the dataframe, extract our pixel array, reshapes, converts that to a matrix and then to an image. Later it was converted to a tensor with a ToTensor built-in transformation.\nFor our independent data, the label was first extracted as defined in the Extract_label transform above and later converted to a Category that we want to predict using Categorize built-in transformation. The total possible labels that can be predicted was passed to the vocab (stands for vocabulary) parameter.\nNow letâ€™s run the pipeline to see what we get!\n\nshow_image(x_pipe(row));\n\n\n\n\n\ny_pipe(row)\n\nTensorCategory(8)\n\n\nNice! Weâ€™re now ready to construct our dataloaders."
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#custom-datasets-dataloaders",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#custom-datasets-dataloaders",
    "title": "Diving into Fastaiâ€™s mid-level API with MNIST",
    "section": "Custom Datasets & DataLoaders",
    "text": "Custom Datasets & DataLoaders\nTo construct dataloaders, a Datasets object needs to be created which takes raw data and can apply multiple pipelines in parallel. This wil be used to run our independent & dependent data piplines together. Optionally the parameter splits can be specified using one of the Splitter transforms, in this case a RandomSplitter which returns training & validation indices extracted from our raw dataset.\nAs we see, most of the functions thatâ€™re used regularly in fastai are actually transformations itself. :)\n\nsplits = RandomSplitter(valid_pct=.2)(np.arange(train.shape[0]))\n\n\ndsets = Datasets(train.values, [x_pipe, y_pipe], splits=splits)\n\nwith the Datasets object now obtained, we construct DataLoaders object by simply calling .dataloaders on it. Since we are not collecting data from disk, we donâ€™t have to specify the path and can optionally set a batch_size with bs. As these are just 28x28 images, we can set a bigger batch size.\n\ndls = dsets.dataloaders(bs=512)\n\n\nlen(dls.train.items), len(dls.valid.items)\n\n(33600, 8400)\n\n\n\ndls.train.show_batch()\n\n\n\n\n\ndls.valid.show_batch()\n\n\n\n\nFinally! We have come a long way extracting the raw data from a dataframe to be able to construct a dataloaders object with image as our dependent datablock and category as our indenpendent datablock!"
  },
  {
    "objectID": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#train",
    "href": "posts/mnist-transforms/starter-mnist-with-fast-ai.html#train",
    "title": "Diving into Fastaiâ€™s mid-level API with MNIST",
    "section": "Train",
    "text": "Train\nYou can now use this data to train using fastaiâ€™s vision_learner method!\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nlearner = vision_learner(dls, resnet18, metrics=accuracy).to_fp16()\n\n\nlearner.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0020892962347716093)\n\n\n\n\n\n\nlearner.fine_tune(5, 5e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.755852\n      0.339413\n      0.915119\n      00:06\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.119606\n      0.081445\n      0.981310\n      00:07\n    \n    \n      1\n      0.092199\n      0.080963\n      0.983571\n      00:07\n    \n    \n      2\n      0.054804\n      0.051717\n      0.986905\n      00:07\n    \n    \n      3\n      0.029896\n      0.037651\n      0.991667\n      00:07\n    \n    \n      4\n      0.015221\n      0.032806\n      0.992500\n      00:07\n    \n  \n\n\n\na near 100% accuracy on the validation set ðŸŽ‰\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\n\n\n\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\nThatâ€™s it for today! Fastaiâ€™s mid-level API offers much more functionality and we barely scratched the surface. Hope it inspires you to learn further and take advantage of this powerful feature. Head to the docs to learn more. Thanks for reading :)"
  },
  {
    "objectID": "posts/fastai-env-setup/index.html",
    "href": "posts/fastai-env-setup/index.html",
    "title": "fastai env setup",
    "section": "",
    "text": "Itâ€™s been quite a while since I last dabbled myself in deep learning and therefore decided to revisit from the basics. And what better way than to start doing that than learning from fastai? :D In this post, we will see how to quickly setup your dev environment for running notebooks locally to put your hard earned GPUs to use :p\nOf course, you can run your notebooks on cloud with free GPU support on platforms such as Google Colab, Paperspace Gradient or even kaggle notebooks but sometimes, it feels good to run things locally without worrying too much about quotas or network issues etc. If youâ€™re starting new in this field, itâ€™s highly recommended to try the above platforms first.\nFirstly, you need mamba. Use it wherever you use conda because itâ€™s much faster. Once you install it, run the following script:\n\n# create a conda environment\nmamba create -n fastai python=3.10\n\n# install suitable version of `pytorch-cuda` at your time of installation\nmamba install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n\n# install fastai\nmamba install -c fastchan fastai\n\nmamba install -c conda-forge jupyterlab ipywidgets\nMake sure you can use GPU with pytorch by running this in your python session:\nimport torch\nassert torch.cuda.is_available()\nThatâ€™s it. Now you can run notebooks with GPU support locally simply by doing mamba activate fastai and launching jupyter ! ðŸ’š"
  }
]