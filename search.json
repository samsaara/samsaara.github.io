[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating from Fastpages to Quarto\n\n\n\n\n\n\n\nmeta\n\n\n\n\nblogging setup‚Ä¶ again‚Ä¶\n\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSupercharge your data processing with DuckDB\n\n\n\n\n\n\n\nEDA\n\n\nSQL\n\n\n\n\nEfficient & blazing fast SQL analytics in Pandas with DuckDB\n\n\n\n\n\n\nMay 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSetting up your blog with Fastpages\n\n\n\n\n\n\n\nmeta\n\n\n\n\nStart your blogging journey in a few easy steps.\n\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for visiting my humble abode on the Internet. I am Vivek, a freelance Data Scientist by profession.\nSa·πÉsƒÅra means World in Sanskrit and I welcome you into mine where I blog about programming, among other things.\nI have over 6 years of experience in Python & Machine Learning ecosystems across a broad variety of sectors ranging from Fashion üõçÔ∏è to Mobility üõ¥ to Fintech üí±.\nAt Sezzle, a platform offering BNPL service, I worked on developing and deploying early machine learning based fraud detection models. Prior to that at FreeNow, a mobility solutions provider, I worked on GMV forecasting, churn prediction etc.\nApart from  & , I am also interested in üßë‚Äçüåæ, üöú, üé∂, üßë‚Äçüç≥ and aim to know a little bit of everything I come across.\nPlease feel free to reach me for any business, consulting or workshop opportunities. üòÉ"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html",
    "title": "Supercharge your data processing with DuckDB",
    "section": "",
    "text": "Do you have large datasets that you simply can not load into memory to analyse with Pandas? Or do you feel more comfortable expressing operations in SQL instead of python?\nFret not, for you have DuckDB now! ‚ú®ü¶Ü‚ú®"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#setup",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Setup",
    "text": "Setup\nDuckDB is very lightweight and has no external dependencies and runs within the host process itself. Simply install it with:\npip install duckdb==0.3.4\nTo initialize it, run:\n\nimport duckdb\ndbcon = duckdb.connect()\n\nThat‚Äôs it! Now you can test it by running:\n\ndbcon.execute('select 1, 2, 3').fetchall()\n\n[(1, 2, 3)]\n\n\nNext step is to run pip install pyarrow to add support for reading/writing parquet data.\n\nJupyter Notebook setup\nIf in case you wish to explore it in Jupyter Notebooks, install a few additional libraries for a better experience:\npip install ipython-sql SQLAlchemy duckdb-engine\nImport them once installed:\n\nimport pandas as pd\nimport sqlalchemy\n\n%load_ext sql\n\nSet a few config options to prettify the output and return it as Pandas DataFrame\n\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\nDuckDB is primarily designed to be an in-memory DB. You can however persist your data to disk.\n\n%sql duckdb:///:memory:\n# %sql duckdb:///path/to/file.db\n\n\n#collapse-output\n\n# we can also access the current config & settings of DuckDB by running the following:\n%sql SELECT * FROM duckdb_settings();\n\n\n\n\n\n  \n    \n      \n      name\n      value\n      description\n      input_type\n    \n  \n  \n    \n      0\n      access_mode\n      automatic\n      Access mode of the database (AUTOMATIC, READ_O...\n      VARCHAR\n    \n    \n      1\n      checkpoint_threshold\n      16.7MB\n      The WAL size threshold at which to automatical...\n      VARCHAR\n    \n    \n      2\n      debug_checkpoint_abort\n      NULL\n      DEBUG SETTING: trigger an abort while checkpoi...\n      VARCHAR\n    \n    \n      3\n      debug_force_external\n      False\n      DEBUG SETTING: force out-of-core computation f...\n      BOOLEAN\n    \n    \n      4\n      debug_many_free_list_blocks\n      False\n      DEBUG SETTING: add additional blocks to the fr...\n      BOOLEAN\n    \n    \n      5\n      debug_window_mode\n      NULL\n      DEBUG SETTING: switch window mode to use\n      VARCHAR\n    \n    \n      6\n      default_collation\n      \n      The collation setting used when none is specified\n      VARCHAR\n    \n    \n      7\n      default_order\n      asc\n      The order type used when none is specified (AS...\n      VARCHAR\n    \n    \n      8\n      default_null_order\n      nulls_first\n      Null ordering used when none is specified (NUL...\n      VARCHAR\n    \n    \n      9\n      disabled_optimizers\n      \n      DEBUG SETTING: disable a specific set of optim...\n      VARCHAR\n    \n    \n      10\n      enable_external_access\n      True\n      Allow the database to access external state (t...\n      BOOLEAN\n    \n    \n      11\n      enable_object_cache\n      False\n      Whether or not object cache is used to cache e...\n      BOOLEAN\n    \n    \n      12\n      enable_profiling\n      NULL\n      Enables profiling, and sets the output format ...\n      VARCHAR\n    \n    \n      13\n      enable_progress_bar\n      False\n      Enables the progress bar, printing progress to...\n      BOOLEAN\n    \n    \n      14\n      explain_output\n      physical_only\n      Output of EXPLAIN statements (ALL, OPTIMIZED_O...\n      VARCHAR\n    \n    \n      15\n      force_compression\n      NULL\n      DEBUG SETTING: forces a specific compression m...\n      VARCHAR\n    \n    \n      16\n      log_query_path\n      NULL\n      Specifies the path to which queries should be ...\n      VARCHAR\n    \n    \n      17\n      max_memory\n      26.9GB\n      The maximum memory of the system (e.g. 1GB)\n      VARCHAR\n    \n    \n      18\n      memory_limit\n      26.9GB\n      The maximum memory of the system (e.g. 1GB)\n      VARCHAR\n    \n    \n      19\n      null_order\n      nulls_first\n      Null ordering used when none is specified (NUL...\n      VARCHAR\n    \n    \n      20\n      perfect_ht_threshold\n      12\n      Threshold in bytes for when to use a perfect h...\n      BIGINT\n    \n    \n      21\n      preserve_identifier_case\n      True\n      Whether or not to preserve the identifier case...\n      BOOLEAN\n    \n    \n      22\n      profiler_history_size\n      NULL\n      Sets the profiler history size\n      BIGINT\n    \n    \n      23\n      profile_output\n      \n      The file to which profile output should be sav...\n      VARCHAR\n    \n    \n      24\n      profiling_mode\n      NULL\n      The profiling mode (STANDARD or DETAILED)\n      VARCHAR\n    \n    \n      25\n      profiling_output\n      \n      The file to which profile output should be sav...\n      VARCHAR\n    \n    \n      26\n      progress_bar_time\n      2000\n      Sets the time (in milliseconds) how long a que...\n      BIGINT\n    \n    \n      27\n      schema\n      \n      Sets the default search schema. Equivalent to ...\n      VARCHAR\n    \n    \n      28\n      search_path\n      \n      Sets the default search search path as a comma...\n      VARCHAR\n    \n    \n      29\n      temp_directory\n      .tmp\n      Set the directory to which to write temp files\n      VARCHAR\n    \n    \n      30\n      threads\n      12\n      The number of total threads used by the system.\n      BIGINT\n    \n    \n      31\n      wal_autocheckpoint\n      16.7MB\n      The WAL size threshold at which to automatical...\n      VARCHAR\n    \n    \n      32\n      worker_threads\n      12\n      The number of total threads used by the system.\n      BIGINT\n    \n    \n      33\n      binary_as_string\n      \n      In Parquet files, interpret binary data as a s...\n      BOOLEAN\n    \n    \n      34\n      Calendar\n      gregorian\n      The current calendar\n      VARCHAR\n    \n    \n      35\n      TimeZone\n      Europe/Berlin\n      The current time zone\n      VARCHAR\n    \n  \n\n\n\n\nFrom now on, you can run SQL directly by prefixing %sql (or %%sql for multiline statements) to your code cell and get your output returned as pandas dataframe :man_dancing:.\n\n%sql select 1 as a;\n\n\n\n\n\n  \n    \n      \n      a\n    \n  \n  \n    \n      0\n      1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#duckdb-vs-traditional-databases",
    "title": "Supercharge your data processing with DuckDB",
    "section": "DuckDB vs traditional Databases",
    "text": "DuckDB vs traditional Databases\nWith pandas.read_sql command, one can already run SQL queries on an existing DB connection, read tables and load data as pandas DataFrames in memory for processing in python. While this is fine for lightweight operations, it is not optimized for heavy data processing. Traditional RDBMSs such as Postgres, MySQL, etc. process each row sequentially which apart from taking long time to execute, also induce a lot of overhead on CPU. DuckDB on the other hand is built with OLAP in mind and is Column-Vectorized. This helps massively parallelize disk I/O and query executions.\n\nDuckDB uses the Postgres SQL parser under the hood, and offers many of the same SQL features as Postgres 1"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#projection-filter-pushdowns",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Projection & Filter Pushdowns",
    "text": "Projection & Filter Pushdowns\nNow let‚Äôs do a simple filter operation on our dataset. Let‚Äôs count the total number of rows that satisfy the condition TAXI_OUT > 10. We‚Äôll try with both pandas & duckdb.\n\ndf[df['TAXI_OUT'] > 10].shape\n\n(45209245, 28)\n\n\n\n%%sql\n\nselect count(*) as count\nfrom df\nwhere TAXI_OUT > 10\n\n\n\n\n\n  \n    \n      \n      count\n    \n  \n  \n    \n      0\n      45209245\n    \n  \n\n\n\n\nWhile the earlier operation took ~9.5s, the latter just took ~250ms :zap:. There‚Äôs just no comparison.\nThis is because duckdb automatically optimizes the query by selecting only the required column(s) (aka projection pushdown) and then applies the filtering to get a subset of data (aka filter pushdown). Pandas instead reads through all the columns. We can optimize this in pandas by doing these pushdowns ourselves.\n\nprojection_pushdown_df = df[['TAXI_OUT']]\nfilter_pushdown_df = projection_pushdown_df[projection_pushdown_df['TAXI_OUT'] > 10]\nfilter_pushdown_df.shape\n\n(45209245, 1)\n\n\nWe managed to bring this down from several seconds to almost a second. But using duckdb is still about 70-90% faster than this."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-groupby",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Groupby",
    "text": "Using Groupby\nNow let‚Äôs calculate a few aggregates using groupby with projection & filter pushdowns combined.\nHere, we compute a few simple metrics with a certain airline carrier grouped by two origin & destination airports and finally sort the results by the origin airport.\n\nprojection_df = df[['ORIGIN', 'DEST', 'TAXI_OUT', \n                    'AIR_TIME', 'DISTANCE', 'OP_CARRIER']]\norigin_df = projection_df[\n    (projection_df['ORIGIN'].isin(('DCA', 'EWR'))) &\n    (projection_df['DEST'].isin(('DCA', 'EWR'))) &\n    (projection_df['OP_CARRIER'] == 'XE')]\n(origin_df\n     .groupby(['ORIGIN', 'DEST'])\n     .agg(\n         avg_taxi_out=('TAXI_OUT', 'mean'),\n         max_air_time=('AIR_TIME', 'max'),\n         total_distance=('DISTANCE', 'sum'))\n     .sort_index(level=0)\n)\n\n\n\n\n\n  \n    \n      \n      \n      avg_taxi_out\n      max_air_time\n      total_distance\n    \n    \n      ORIGIN\n      DEST\n      \n      \n      \n    \n  \n  \n    \n      DCA\n      EWR\n      22.116009\n      87.0\n      828835.0\n    \n    \n      EWR\n      DCA\n      23.675481\n      93.0\n      831024.0\n    \n  \n\n\n\n\nWe can make it a bit more concise by using .query for filtering pushdown.\n\n(df\n .query('OP_CARRIER == \"XE\" and ORIGIN in (\"DCA\", \"EWR\") and DEST in (\"DCA\", \"EWR\")')\n .groupby(['ORIGIN', 'DEST'])\n .agg(\n     avg_taxi_out=('TAXI_OUT', 'mean'),\n     max_air_time=('AIR_TIME', 'max'),\n     total_distance=('DISTANCE', 'sum'))\n)\n\n\n\n\n\n  \n    \n      \n      \n      avg_taxi_out\n      max_air_time\n      total_distance\n    \n    \n      ORIGIN\n      DEST\n      \n      \n      \n    \n  \n  \n    \n      DCA\n      EWR\n      22.116009\n      87.0\n      828835.0\n    \n    \n      EWR\n      DCA\n      23.675481\n      93.0\n      831024.0\n    \n  \n\n\n\n\nThis approach took only about half the time (~3s) compared to our earlier one because¬†.query uses a modified syntax of python and also indexing thus resulting in more efficient evaluation. We can now compare that to our SQL counterpart‚Ä¶\n\n%%sql\n\nselect\n    ORIGIN,\n    DEST,\n    AVG(TAXI_OUT) as avg_taxi_out,\n    MAX(AIR_TIME) as max_air_time,\n    SUM(DISTANCE) as total_distance\n\nfrom df\n\nwhere\n    OP_CARRIER = 'XE' and\n    ORIGIN in ('DCA', 'EWR') and\n    DEST in ('DCA', 'EWR')\n    \ngroup by ORIGIN, DEST\norder by ORIGIN\n\n\n\n\n\n  \n    \n      \n      ORIGIN\n      DEST\n      avg_taxi_out\n      max_air_time\n      total_distance\n    \n  \n  \n    \n      0\n      DCA\n      EWR\n      22.116009\n      87.0\n      828835.0\n    \n    \n      1\n      EWR\n      DCA\n      23.675481\n      93.0\n      831024.0\n    \n  \n\n\n\n\nThis ~400ms execution with duckdb above is around an order of magnitude faster and also a lot cleaner, I‚Äôd say. :wink:\nNotice that the data is already loaded under df and hence we don‚Äôt need to read from the source parquet file.\n\nIn the same way, we can also improve the performance of our queries drastically when using joins across multiple tables. I leave this as an exercise to the reader.\n\nBut why actually load data into memory in the first place when we can process it more efficiently with it being just on disk? Often times, the data is too big to load into memory anyways.\nTo do that, we just need to create a VIEW to our data which lets us query the table directly without loading onto memory and update the source from the dataframe df to the newly created view instead.3"
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-approximations",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using approximations",
    "text": "Using approximations\nAt times, it suffices just to get an estimate of certain data rather than a precise answer. Using approximations would help us to just that.\n\n%%sql\n\nselect\n    OP_CARRIER,\n    approx_count_distinct(DEST) as approx_num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n  \n    \n      \n      OP_CARRIER\n      approx_num_unique_destinations\n    \n  \n  \n    \n      0\n      9E\n      186\n    \n    \n      1\n      AA\n      116\n    \n    \n      2\n      AS\n      77\n    \n    \n      3\n      B6\n      73\n    \n    \n      4\n      CO\n      85\n    \n    \n      5\n      DL\n      171\n    \n    \n      6\n      EV\n      205\n    \n    \n      7\n      F9\n      130\n    \n    \n      8\n      FL\n      75\n    \n    \n      9\n      G4\n      126\n    \n  \n\n\n\n\n\n%%sql\n\nselect\n    OP_CARRIER,\n    -- takes more time to compute\n    count(distinct DEST) as num_unique_destinations\n\nfrom airlinedata\n\ngroup by 1\norder by 1\n\nlimit 10\n\n\n\n\n\n  \n    \n      \n      OP_CARRIER\n      num_unique_destinations\n    \n  \n  \n    \n      0\n      9E\n      185\n    \n    \n      1\n      AA\n      116\n    \n    \n      2\n      AS\n      77\n    \n    \n      3\n      B6\n      73\n    \n    \n      4\n      CO\n      85\n    \n    \n      5\n      DL\n      170\n    \n    \n      6\n      EV\n      205\n    \n    \n      7\n      F9\n      129\n    \n    \n      8\n      FL\n      75\n    \n    \n      9\n      G4\n      126\n    \n  \n\n\n\n\nOur approximation query earlier ran about 3-4 times faster than the precise one in this case. This is crucial when responsiveness is more important than precision (esp.¬†for larger datasets)."
  },
  {
    "objectID": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "href": "posts/duckdb-tutorial/duckdb-tutorial.html#using-window-functions",
    "title": "Supercharge your data processing with DuckDB",
    "section": "Using Window functions",
    "text": "Using Window functions\nFinally, let‚Äôs wrap our analysis by showing off a bit more of what duckdb can do using some advanced SQL operations.\nWe create two CTEs (Common Table Expressions) to calculate a couple of features. We do filter & projection pushdowns in one CTE and compute our desired features in another. The first feature is a simple demo to showcase if-else support. The second feature is a bit advanced where we find out the last destination a given air carrier has flown to, sorted by flying date. And when it doesn‚Äôt exist, replace it with NA. We then take a sample from the final resultant set.\n\n%%sql\n\nwith limited_data as (\n    select \n        FL_DATE,\n        ORIGIN, \n        DEST, \n        DISTANCE,\n        OP_CARRIER,\n    from airlinedata\n    where FL_DATE >= '2015-01-01'    \n),\n\nlast_destination_data as (\n    select *,\n        case\n            when DISTANCE*1.60934 > 500 then 'yes'\n            else 'no'\n        end as distance_more_than_500_km,\n\n        coalesce(last_value(DEST) over (\n            partition by OP_CARRIER\n            order by FL_DATE\n            rows between unbounded preceding and 1 preceding\n        ), 'NA') as last_destination_flown_with_this_carrier\n\n    from limited_data\n)\n\nselect *\nfrom last_destination_data\nusing sample 10;\n\n\n\n\n\n  \n    \n      \n      FL_DATE\n      ORIGIN\n      DEST\n      DISTANCE\n      OP_CARRIER\n      distance_more_than_500_km\n      last_destination_flown_with_this_carrier\n    \n  \n  \n    \n      0\n      2018-07-10\n      DCA\n      LGA\n      214.0\n      YX\n      no\n      DCA\n    \n    \n      1\n      2015-05-08\n      DAL\n      BWI\n      1209.0\n      WN\n      yes\n      BWI\n    \n    \n      2\n      2018-03-30\n      LAS\n      SJC\n      386.0\n      WN\n      yes\n      SJC\n    \n    \n      3\n      2015-07-10\n      BOS\n      MSP\n      1124.0\n      DL\n      yes\n      DTW\n    \n    \n      4\n      2016-06-01\n      DTW\n      BWI\n      409.0\n      DL\n      yes\n      DTW\n    \n    \n      5\n      2017-07-26\n      GEG\n      MSP\n      1175.0\n      DL\n      yes\n      SAN\n    \n    \n      6\n      2017-01-10\n      DFW\n      ACT\n      89.0\n      EV\n      no\n      DFW\n    \n    \n      7\n      2015-01-01\n      PHX\n      DRO\n      351.0\n      OO\n      yes\n      BFL\n    \n    \n      8\n      2018-05-06\n      DFW\n      MCO\n      985.0\n      AA\n      yes\n      IAH\n    \n    \n      9\n      2018-04-30\n      MSY\n      LAX\n      1670.0\n      WN\n      yes\n      LAX\n    \n  \n\n\n\n\nNice, isn‚Äôt it?! The same operation is unimaginably complex (for me, at least) in pandas. ü§Ø\nWith DuckDB, we can combine one or more of many of such complex operations and execute in one go without worrying much about manual optimizations."
  },
  {
    "objectID": "posts/migrate-to-quarto/index.html",
    "href": "posts/migrate-to-quarto/index.html",
    "title": "Migrating from Fastpages to Quarto",
    "section": "",
    "text": "Fastpages, based on which my original blog was setup has been archived and they now recommend using Quarto instead. If you‚Äôre starting new, the latter is the recommended way to go but if you too have a fastpages blog setup initially and want to migrate, there‚Äôs a migration guide available. It‚Äôs not completely perfect as you have to tweak a few bits here & there before you see all the renderings correctly (because of slight syntactic variations amongst other things).\nHowever, I felt quarto to be much more intuitive and easy to setup. I also did the migration but found manually moving posts from my old blog repo to the new quarto repo to be a bit easier. (Migration worked but I was not happy with the directories it created as part of it‚Ä¶ I found it aesthetically less pleasing and hence moved them myself).\nFor simple blogging, fastpages offered more than enough features and quarto offers even more on top of that. It‚Äôs easy to get started with quarto. Head over to the start guide to learn more.\nI do miss the advanced styling options I setup in fastpages though‚Ä¶ will have to dig into Quarto to see how much I can reuse. Until then, have fun with my frugalistic looking blog! :D"
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html",
    "title": "Setting up your blog with Fastpages",
    "section": "",
    "text": "After going through a bit of an intensive setup, I thought of writing up my journey & difficulties I faced while setting up this blog so that it could help others who might start later."
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#motivation",
    "title": "Setting up your blog with Fastpages",
    "section": "Motivation",
    "text": "Motivation\nI was looking for ways to have my tiny little place on the internet & therefore scouted for feasible options. As a Data Scientist, I interact with code regularly and (Jupyter) Notebooks are part of our DNA :stuck_out_tongue:. So the criteria that I had set for myself was that whatever platform I choose, it has to be (a) very code friendly, (b) easy to maintain, and (c) relatively affordable (if possible, even free).\n\nQuest for the right tool\nI‚Äôm a big fan of Notion and use it for almost everything in my private life. So I initially considered setting up my blog with Super as you can create websites straight away from Notion pages. Though it looks great, the pricing is something that I‚Äôm still not comfortable with (yet).\nThen there‚Äôs Medium, which provides a nice writing experience but only as long as you don‚Äôt have much code in your content. Though affordable, just like Super, it has mediocre support for code formatting natively & you have to optimize your content well ahead with a lot of GitHub gists. It also has no out-of-the-box support for mathematical symbols/equations. Read more about its shortcomings from a developer perspective in a great post from Prasanth Rao here.Though, I might still consider using this service to post once in a while to increase my outreach. We‚Äôll see how it goes. \n\nThese first two options are great if you don‚Äôt write code-heavy posts (that often) and are also very beginner friendly. But unfortunately, both of them are not free and also don‚Äôt fit well for my use case. Besides, where‚Äôs the fun if you don‚Äôt build stuff by tweaking stuff? :wink:\n\nI then decided to give GitHub Pages a try since it‚Äôs a free static site generator service from GitHub. One need not know (much) about HTML/CSS and can simply write content in human readable Markdown format which gets rendered as a webpage. Besides, you get a nice revision history of your blog as everything would be version controlled with GitHub. In combination with Jekyll (that powers Github pages), there‚Äôre numerous themes & templates to choose from and so much customization that can be made via styles, CSS, etc. I can easily convert Jupyter notebooks into markdown scripts and have them rendered with Jekyll. Since one can display code snippets, markdown cells, and embed 3rd party content within Jupyter notebooks, I intended to go with this as it fulfilled most of my needs‚Ä¶ until I rediscovered Fastpages.\nFastpages, from fast.ai, turbocharges Jupyter notebooks by automating the process of creating blog posts from notebooks via Github Actions. We can write content in notebooks markdown files, or even Word documents and have them rendered as a web page. It offers so much more functionality on-top like code folding, interactive plots on a static web page via embeddings, comments & emoji support :heart_eyes_cat:, etc. For an excellent introduction, please refer to the official documentation.\nThat has convinced me & so now you‚Äôre reading this site built with Fastpages. üéâ"
  },
  {
    "objectID": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "href": "posts/fastpages-blog-setup/fastpages-blog-setup.html#setup",
    "title": "Setting up your blog with Fastpages",
    "section": "Setup",
    "text": "Setup\nFortunately, Fastpages is very well documented and it is highly recommended that you go through that first. However, you might still encounter some problems because of some outdated documentation, and if in case you want to test it locally on Linux systems, which is what I cover here. So, without further ado, let‚Äôs dive in.\n\nThink of a name for your blog. It can just be blog like mine.\nGo through the setup instructions detailed here.\n\nIt might happen that once after you try to generate a copy as per the instructions above, a PR won‚Äôt be automatically generated. The debug instructions in the README are a bit outdated. In this case, go to the Settings -> Actions -> General section of your newly created blog repository and ensure that you have Read and write permissions enabled and the last checkbox is ‚úîÔ∏è like :point_down:. Hit Save. \nGo to the Actions tab and you might see a failed section. Ignore what it is for now and click that failed one. Most likely it‚Äôd be a Setup workflow failure. On the top right, from the drop-down menu Re-run jobs, select Re-run failed jobs.\nOnce the above steps are all ‚úÖ, go to the Pull Requests tab and wait for some time. Your first PR would soon be automatically created. You can also optionally check the progress under the Actions tab if desired.\nNow follow the instructions in the PR and merge it.\n\nCongratulations :confetti_ball:. Your blog would soon be up & running at {your-username}.github.io/{repo-name}. Now you can make changes directly on GitHub online or create notebook/markdown/Word files locally and simply upload them as per the instructions into the right folder. Your blog would be updated just like that (in a few minutes). ü™Ñ\n\n\nTest Locally\nIn most of the cases, you might want to check how your post looks like before you publish to make sure it looks as you intend it to be, especially when it contains data from different sources or in different formats. This is when having the option of testing it locally comes in handy. With Fastpages, you can run your blog locally to see how it would look like so that you can fix any nitty gritty details before pushing it online.\nFastpages again provides a good documentation on how to test it locally with the help of Docker . It has worked fine on my Mac  but installing/upgrading Docker on Linux  is still nowhere as smooth as on Mac and hence I had to go through a bit of digging into forums to make it work on my Ubuntu  machine especially on the latest version 22.04 LTS. So, going forward I‚Äôd cover only this scenario.\n\n\n\n\n\n\nInfo\n\n\n\nFor Mac/Windows, all you need is Docker installed and simply run make server from the root level of the repo.\n\n\n\nDocker Desktop for Linux (DD4L) is still in beta and is only available for 21.XX versions of Ubuntu. So if you have that, go ahead with the setup below. If not, skip to the next step.\n\nFollow the Docker installation instructions from the official documentation.\nIf you had installed Docker via Snap or you had a previous preview version of Docker Desktop installed, it‚Äôs recommended to uninstall them completely. See more here for help on how to do that.\n\nSince Ubuntu 22.04 LTS is not yet supported, I ended up installing  (not Docker Desktop) from here.\nCheck if you have docker-compose installed by doing which docker-compose or docker-compose -v. If not, install it as a standalone binary.\n\nI‚Äôm not sure if it also installs Make but if it doesn‚Äôt, please install it too following the instructions here.\n\nRun make server from the top level of your repository.\n\nYour Jupyter server would be available shortly at http://localhost:8080/tree and it would take a bit of time for your blog to be available under http://localhost:4000/{your-repo-name}. Be patient. :relieved:\n\n\n\n\n\n\n\n\nInfo\n\n\n\nif make server doesn‚Äôt work because of permission issues, try sudo make server instead\n\n\n\nCongratulations once again! :tada: You now have a local version of your blog running. You can create new posts and have them rendered almost instantly :sparkles:. Once you‚Äôre happy with the content & format, you can push it (preferably in a new branch and making a PR so that your main/master branch is unaffected). If you feel a bit adventurous, try customizing your blog more to your liking by changing fonts/styles, etc.\nIf you like this blog‚Äôs customization, checkout its  repo esp.¬†its custom-style.scss. May be I‚Äôll write another post detailing it.\nGood Luck & Happy Blogging! ‚ù§Ô∏è"
  }
]